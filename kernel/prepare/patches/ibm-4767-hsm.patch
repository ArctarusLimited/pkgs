diff --git drivers/misc/Kconfig drivers/misc/Kconfig
index 0f5a49fc7c9e..5dc257848b2f 100644
--- drivers/misc/Kconfig
+++ drivers/misc/Kconfig
@@ -477,6 +477,7 @@ source "drivers/misc/ti-st/Kconfig"
 source "drivers/misc/lis3lv02d/Kconfig"
 source "drivers/misc/altera-stapl/Kconfig"
 source "drivers/misc/mei/Kconfig"
+source "drivers/misc/ibm4767/Kconfig"
 source "drivers/misc/vmw_vmci/Kconfig"
 source "drivers/misc/genwqe/Kconfig"
 source "drivers/misc/echo/Kconfig"
diff --git drivers/misc/Makefile drivers/misc/Makefile
index a086197af544..b14237c904b6 100644
--- drivers/misc/Makefile
+++ drivers/misc/Makefile
@@ -41,6 +41,7 @@ obj-y				+= ti-st/
 obj-y				+= lis3lv02d/
 obj-$(CONFIG_ALTERA_STAPL)	+=altera-stapl/
 obj-$(CONFIG_INTEL_MEI)		+= mei/
+obj-$(CONFIG_HSM_IBM_4767)  += ibm4767/
 obj-$(CONFIG_VMWARE_VMCI)	+= vmw_vmci/
 obj-$(CONFIG_LATTICE_ECP3_CONFIG)	+= lattice-ecp3-config.o
 obj-$(CONFIG_SRAM)		+= sram.o
diff --git drivers/misc/ibm4767/Kconfig drivers/misc/ibm4767/Kconfig
new file mode 100644
index 000000000000..cd6ab0969d48
--- /dev/null
+++ drivers/misc/ibm4767/Kconfig
@@ -0,0 +1,4 @@
+config HSM_IBM_4767
+	bool "IBM 4767 HSM Support"
+	help
+	  This option enables the driver for the IBM 4767 HSM
diff --git drivers/misc/ibm4767/Makefile drivers/misc/ibm4767/Makefile
new file mode 100644
index 000000000000..e102768065f1
--- /dev/null
+++ drivers/misc/ibm4767/Makefile
@@ -0,0 +1,17 @@
+obj-$(CONFIG_HSM_IBM_4767) += ibm4767.o
+
+ibm4767-objs := asym.o \
+		dma.o \
+		dt.o \
+		htb.o \
+		ioctl.o \
+		ints.o \
+		mailbox.o \
+		main.o \
+		procfs.o \
+		request.o \
+		special.o \
+		target.o \
+		timers.o \
+		util.o  \
+		workqueues.o
diff --git drivers/misc/ibm4767/asym.c drivers/misc/ibm4767/asym.c
new file mode 100755
index 000000000000..036b49481558
--- /dev/null
+++ drivers/misc/ibm4767/asym.c
@@ -0,0 +1,751 @@
+/*************************************************************************
+ *  Filename:asym.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:
+ *    Code to manage the asymmetric HRA pool
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_mailbox.h"
+#include "y_regs.h"
+#include "host_err.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+
+int
+asym_hra_pool_initialize(ibm4767_ctx_t *ctx)
+{
+	int i, rc;
+
+	PDEBUG(1, "Enter %s...\n", __FUNCTION__);
+
+	spin_lock_init(&ctx->asym_attr.mask_lock);
+
+	for (i=0; i < ASYM_HRA_POOL_SIZE; i++) {
+		init_scatterlist(&ctx->asym_attr.slist[i]);
+		INIT_LIST_HEAD(&ctx->asym_attr.DTs[i]);
+
+		rc = alloc_scatterlist(ctx, 
+				       NULL, 0, ASYM_HRA_SIZE, FALSE,
+				       &ctx->asym_attr.slist[i],
+				       DMA_FROM_DEVICE);
+
+		if (rc) {
+			PRINTKW("Device %d: alloc_scatterlist for HRA %d failed\n", ctx->dev_index, i);
+			goto error;
+		}
+
+		rc = dt_append_scatterlist(ctx, NULL, &ctx->asym_attr.DTs[i], &ctx->asym_attr.slist[i]);
+		if (rc) {
+			PRINTKW("Device %d: dt_append_scatterlist %d failed\n", ctx->dev_index, i);
+			goto error;
+		}
+
+		ctx->asym_attr.hra_update_vector |= (1LL << (63-i));
+	}
+
+	return HOST_DD_Good;
+
+error:
+	for (i=0; i < ASYM_HRA_POOL_SIZE; i++) {
+		free_scatterlist(&ctx->asym_attr.slist[i]);
+		dt_free_chain(ctx, &ctx->asym_attr.DTs[i]);
+	}
+	return rc;
+}
+
+
+void
+asym_hra_pool_unload(ibm4767_ctx_t *ctx)
+{
+	int i;
+
+	PDEBUG(1, "Enter %s...\n", __FUNCTION__);
+
+	for (i=0; i < ASYM_HRA_POOL_SIZE; i++) {
+		free_scatterlist(&ctx->asym_attr.slist[i]);
+		dt_free_chain(ctx, &ctx->asym_attr.DTs[i]);
+	}
+
+	if (ctx->hra_req) {
+		PDEBUG(1, "   Freeing HRA load packet...\n");
+		free_scatterlist(&ctx->hra_req->hdr_slist);
+		dt_free_chain(ctx, &ctx->hra_req->DT_req);
+		request_release(ctx, ctx->hra_req);
+
+		ctx->hra_req = NULL;
+	}
+}
+
+void
+asym_hra_pool_dump(ibm4767_ctx_t *ctx)
+{
+	int i;
+
+	PDEBUG(1, "Dumping ASYM HRA chains...\n");
+	for (i=0; i < ASYM_HRA_POOL_SIZE; i++) {
+		dump_chain(ctx, &ctx->asym_attr.DTs[i], TRUE);
+	}
+}
+
+#if defined(DEBUG)
+int
+asym_hra_reply_handler_analyze(ibm4767_ctx_t *ctx, uint64_t phys)
+{
+	struct dt_t           *dt = NULL;
+	mrm_event_log_header_t  *hdr = NULL;
+	asym_msg_t            *msg_info = NULL;
+	struct list_head      *pLH = NULL;
+	struct list_head      *chain = NULL;
+	int                    hra;
+	char *str;
+	char jrm[] = "ABCDEFGHIJKLMNOPQRSTUVWXYZ 0123456789 abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ 0123456789 abcdefghijklmnopqrstuvwxyz";
+	char *mark[3] = { "AAAAAAAAAAAAAAAAAAAAAAD", "BBBBBBBBBBBBBBBBBBBBBBD", "CCCCCCCCCCCCCCCCCCCCCCD" };
+
+	int expected_len, fail = FALSE;
+
+	for (hra=0; hra < ASYM_HRA_POOL_SIZE; hra++) {
+		if (list_empty(&ctx->asym_attr.DTs[hra]))
+			continue;
+
+		chain = &ctx->asym_attr.DTs[hra];
+		
+		dt = list_entry(chain->next, struct dt_t, list_head);
+		if (dt->this_dma == phys)
+			break;
+	}
+
+	if (!dt || (hra >= ASYM_HRA_POOL_SIZE)) 
+		return HDD_FAIL;
+
+	list_for_each(pLH, &ctx->asym_attr.slist[hra].frags) {
+		scatter_frag_t *sf = list_entry(pLH, scatter_frag_t, list_head);
+		dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+	}
+
+	// at this point, 'hra' is the index into asym_attr for this reply
+	//
+	hdr = (mrm_event_log_header_t *)dt->data_va;
+	msg_info = &hdr->log_header;
+
+	//hex_dump(ctx, 1, "LOG HEADER", msg_info, sizeof(mrm_event_log_header_t));
+	
+	be16_to_cpus(&msg_info->length);
+	be16_to_cpus(&msg_info->type);
+	be16_to_cpus(&msg_info->version);
+	be16_to_cpus(&msg_info->facility);
+	be16_to_cpus(&msg_info->severity);
+
+	if (msg_info->version == 1) {
+		str = jrm;
+	}
+	else if (msg_info->version == 2) {
+		str = mark[0];
+	}
+	else if (msg_info->version == 3) {
+		str = mark[1];
+	}
+	else if (msg_info->version == 4) {
+		str = mark[2];
+	}
+	else {
+		PRINTKW("BAD VERSION\n");
+		fail = TRUE;
+		goto done;
+	}
+	
+	if (msg_info->facility != 2) {
+		PRINTKW("BAD FACILITY\n");
+		fail = TRUE;
+		goto done;
+	}
+	
+	if (msg_info->severity != 3) {
+		PRINTKW("BAD SEVERITY\n");
+		fail = TRUE;
+		goto done;
+	}
+	
+	expected_len = sizeof(asym_msg_t) - sizeof(msg_info->length) + strlen(str) + 1;
+	if (msg_info->length != expected_len) {
+		PRINTKW("BAD LENGTH.  %d vs expected %d\n", msg_info->length, expected_len);
+		fail = TRUE;
+		goto done;
+	}
+
+	if (!fail) {
+		// for this test, we know that the log msg will fit in the first DT so we don't need
+		// to walk the chain...
+		char *p = (char *)hdr + sizeof(mrm_event_log_header_t);
+		if (memcmp(p, str, strlen(str)) != 0) {
+			PRINTKW("BAD STRING\n");
+			fail = TRUE;
+			goto done;
+		}
+	}
+
+done:
+	if (fail) {
+		//uint32_t bytes = cpu_to_be32((dt->fields >> 40) & 0x1FFFFF);
+		uint32_t bytes = 256;
+		hex_dump(NULL, 0, "LOG MSG CONTENTS", hdr, bytes);
+		mark_device_offline(ctx, HOSTAborted);
+		write32(0L, HIER(ctx));
+	}
+	else {
+		PDEBUG(2, "Log matched\n");
+	}
+
+	spin_lock_bh(&ctx->asym_attr.mask_lock);
+	ctx->asym_attr.hra_update_vector |= (1LL << (63-hra));
+	spin_unlock_bh(&ctx->asym_attr.mask_lock);
+
+	return HOST_DD_Good;
+}
+#endif
+
+
+int
+asym_hra_reply_handler(ibm4767_ctx_t *ctx, uint64_t phys)
+{
+	struct dt_t           *dt = NULL;
+	mrm_event_log_header_t  *hdr = NULL;
+	asym_msg_t            *msg_info = NULL;
+	struct list_head      *pLH = NULL;
+	struct list_head      *chain = NULL;
+	int                    i, hra, remain;
+
+#if defined(DEBUG)
+	if (ctx->analyze_logs)
+		return asym_hra_reply_handler_analyze(ctx, phys);
+#endif
+
+	for (hra=0; hra < ASYM_HRA_POOL_SIZE; hra++) {
+		if (list_empty(&ctx->asym_attr.DTs[hra]))
+			continue;
+
+		chain = &ctx->asym_attr.DTs[hra];
+		
+		dt = list_entry(chain->next, struct dt_t, list_head);
+		if (dt->this_dma == phys)
+			break;
+	}
+
+	if (!dt || (hra >= ASYM_HRA_POOL_SIZE)) 
+		return HDD_FAIL;
+
+	list_for_each(pLH, &ctx->asym_attr.slist[hra].frags) {
+		scatter_frag_t *sf = list_entry(pLH, scatter_frag_t, list_head);
+		dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+	}
+
+	// at this point, 'hra' is the index into asym HRA list for this reply and 'dt' is the
+	// first DT in its chain...
+	
+	hdr = (mrm_event_log_header_t *)dt->data_va;
+	msg_info = &hdr->log_header;
+
+	be16_to_cpus(&msg_info->length);
+	be16_to_cpus(&msg_info->type);
+	be16_to_cpus(&msg_info->version);
+	be16_to_cpus(&msg_info->facility);
+	be16_to_cpus(&msg_info->severity);
+
+#if defined(ENABLE_LOGCHECK)
+	// we've occasionally seen a left byte shift in some of the message logs.  hopefully this will help us
+	// determine who's at fault...
+	//
+	if ((msg_info->facility > 0x0100) || (msg_info->version > 0x0100)) {
+		PRINTKE("Device %d: *** POSSIBLY CORRUPT INCOMING LOG MESSAGE ***\n", ctx->dev_index);
+		PRINTKE("Device %d:   asym_msg_t: %04x %04x %04x %04x %04x\n", ctx->dev_index,
+				msg_info->length, msg_info->type, msg_info->version, msg_info->facility, msg_info->severity);
+		PRINTKE("Device %d:  dt: va %p  dma %016llx\n", ctx->dev_index, dt->data_va, dt->BE_data_dma);
+		hex_dump(NULL, 0, "First 64 bytes", hdr, 64);
+	}
+#endif
+
+	//hex_dump(ctx, 1, "LOG HEADER", msg_info, sizeof(mrm_event_log_header_t));
+	//PDEBUG(1, "asym handler: len=%d, type=%d, version=%d, fac=%d, sev=%d\n",
+	//		msg_info->length, msg_info->type, msg_info->version, 
+	//		msg_info->facility, msg_info->severity);
+
+	ctx->lastlog_timestamp = jiffies;
+
+	// if an FFDC with no log occurred, we'll get lots of log messages.  The final
+	// message will have severity KERN_EMERG/LOG_EMERG.  at this point, presumably 
+	// the card will need to be reset.  it's unclear at this point whether we should
+	// do this automatically or wait for the user to do it manually...for now we'll
+	// do it automatically.
+	//
+	// at richard's request, we need to treat sev0 thru sev2 log messages the same
+	// (since zSeries treats sev0, sev1 and sev2 as fatal).
+	//
+	if ((msg_info->severity <= 2) && !sev0_ignore) {
+		spin_lock_bh(&ctx->counter_lock);
+		if (!sev0_append(ctx, jiffies)) {
+			spin_unlock_bh(&ctx->counter_lock);
+			PRINTKW("Device %d has sent %d sev0-sev2 log msgs within %d minutes.  Marking MCPU offline...\n", ctx->dev_index, sev0_limit, sev0_timeframe);
+
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce |= TIMER_QUIESCE_DEFERRED_MAIN_RESET;
+			ctx->timer_quiesce |= TIMER_QUIESCE_DEFERRED_FORCED_EDUMP;
+			ctx->timer_quiesce |= TIMER_QUIESCE_DEFERRED_BUF_UPDATE;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			del_timer_sync(&ctx->deferred_forced_edump_timer);
+			del_timer_sync(&ctx->deferred_buf_update_timer);
+			del_timer_sync(&ctx->agentid_list_timer);
+			cancel_deferred_main_reset(ctx);
+			mark_mcpu_offline(ctx, HOSTAborted);
+
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce &= ~TIMER_QUIESCE_DEFERRED_MAIN_RESET;
+			ctx->timer_quiesce &= ~TIMER_QUIESCE_DEFERRED_FORCED_EDUMP;
+			ctx->timer_quiesce &= ~TIMER_QUIESCE_DEFERRED_BUF_UPDATE;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+		}
+		else {
+			spin_unlock_bh(&ctx->counter_lock);
+			PRINTKW("Device %d: Sev %d log msg rcvd.  Initiating main reset in 2 seconds...\n", ctx->dev_index, msg_info->severity);
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce |= TIMER_QUIESCE_DEFERRED_FORCED_EDUMP;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			del_timer_sync(&ctx->deferred_forced_edump_timer);
+
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce &= ~TIMER_QUIESCE_DEFERRED_FORCED_EDUMP;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			defer_main_reset(ctx, 2, FALSE);
+		}
+	}
+
+	if (!msg_info->length)
+		return HOST_DD_Good;
+
+	// see Fence chapter 8 (Asymmetric Card to Host Data flow) for a diagram
+	remain = msg_info->length - (sizeof(asym_msg_t) - sizeof(msg_info->length));
+	if (remain < 0) {
+		PRINTKW("Device %d sent a log message with a negative length!?\n", ctx->dev_index);
+		return HDD_FAIL;
+	}
+
+	i = 0;
+	list_for_each(pLH, chain) {
+		uint32_t len;
+		uint32_t bytes;
+		dt = (struct dt_t *)list_entry(pLH, struct dt_t, list_head);
+
+		bytes = cpu_to_be32((dt->fields >> 40) & 0x1FFFFF);
+		if (i++ == 0) {
+			len = MIN(remain, bytes - sizeof(mrm_event_log_header_t));
+			log_event(ctx, msg_info, (char *)(hdr) + sizeof(mrm_event_log_header_t), len);
+		}
+		else {
+			len = MIN(remain, bytes);
+			log_event(ctx, msg_info, dt->data_va, len);
+		}
+
+		remain -= len;
+		if (remain == 0)
+			break;
+	}
+
+	spin_lock_bh(&ctx->asym_attr.mask_lock);
+	ctx->asym_attr.hra_update_vector |= (1LL << (63-hra));
+	spin_unlock_bh(&ctx->asym_attr.mask_lock);
+
+	return HOST_DD_Good;
+}
+
+
+void
+asym_hra_pool_update(ibm4767_ctx_t *ctx)
+{
+        uint32_t hi32;
+
+        spin_lock_bh(&ctx->asym_attr.mask_lock);
+
+        if (ctx->asym_attr.hra_update_vector == 0x0) {
+                PDEBUG(1, "Device %d: %s...no available buffers. deferring update.\n", ctx->dev_index, __FUNCTION__);
+		atomic_set(&ctx->asym_attr.req_buf_update, 1);
+                spin_unlock_bh(&ctx->asym_attr.mask_lock);
+
+                mod_timer(&ctx->deferred_buf_update_timer, jiffies + HZ/5);
+                return;
+        }
+
+        // update vector has at least one unused entry.  we only maintain 8 HRA entries so these
+        // will always be in the upper 32bits of the update vector...
+        //
+        hi32 = (uint32_t)((ctx->asym_attr.hra_update_vector >> 32) & 0xFFFFFFFF);
+
+	PDEBUG(1, "Device %d: Sending buffer update:  0x%08x\n", ctx->dev_index, hi32);
+
+        ctx->asym_attr.hra_update_vector = 0;
+
+        write32(cpu_to_be32(hi32), H2M_MBX_L(ctx));
+        write32(cpu_to_be32(MBX_BUFUPDATEHIGH32), H2M_MBX_H(ctx));
+
+        spin_unlock_bh(&ctx->asym_attr.mask_lock);
+}
+
+
+int
+asym_hra_pool_load_packet_init(ibm4767_ctx_t *ctx)
+{
+	request_t          *pReq    = NULL;
+	dma_header16_t     *pDMAhdr = NULL;
+	hrb_type3_header_t *pHdr1   = NULL;
+	hrb_header_part2_t *pHdr2   = NULL;
+	hra_pool_packet_t  *pPkt    = NULL;
+	struct dt_t        *pDT     = NULL;
+	char               *pBuf    = NULL;
+	uint32_t            tmp;
+	uint32_t            pad1;
+	uint32_t            hra1_len, hdr_len, len1, len2, len3;
+	int                 i, rc;
+	request_wait_t      rwt;
+
+
+	PDEBUG(1, "Enter %s...\n", __FUNCTION__);
+
+	rc = request_allocate(ctx, REQUEST_HRA, &rwt);
+	if (rc != HOST_DD_Good)
+		return HOST_DD_DeviceBusy;
+	pReq = rwt.pReq;
+
+	// For 4767, the HRA POOL PACKET will receive an ACK response so there's one HRA...
+	//
+	hra1_len = (sizeof(mrm_type1_header_t) + 7) & ~7;
+
+	rc = alloc_scatterlist(ctx,
+			       NULL, 0,
+			       hra1_len,
+			       FALSE,
+			       &pReq->hra1_slist,
+			       DMA_FROM_DEVICE);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist hra1 failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_hra1, &pReq->hra1_slist);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist hra1 failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	atomic_inc(&pReq->num_hras);
+
+
+	// see Fence 10.2 for HRA POOL PACKET layout
+	//
+	hdr_len = sizeof(hrb_type3_header_t) + sizeof(hrb_header_part2_t);
+	tmp = (hdr_len + sizeof(hra_pool_packet_t) + 7) & ~7;
+	pad1 = tmp - (hdr_len + sizeof(hra_pool_packet_t));
+	hdr_len += pad1;
+
+	pBuf   = pReq->header;
+	pHdr1  = (hrb_type3_header_t *)pBuf;  pBuf += sizeof(hrb_type3_header_t);
+	pBuf  += pad1;
+	pHdr2  = (hrb_header_part2_t *)pBuf;  pBuf += sizeof(hrb_header_part2_t);
+	pPkt   = (hra_pool_packet_t *)pBuf;
+
+	pDMAhdr = (dma_header16_t *)pHdr1;
+
+	len3 = sizeof(hrb_type3_header_t) 
+		- sizeof(pHdr1->len3)
+		- sizeof(pHdr1->len2)
+		- sizeof(dma_header16_t)
+		+ pad1 + sizeof(hrb_header_part2_t);
+
+	len2 = len3 + sizeof(pHdr1->len3)
+		+ sizeof(hra_pool_packet_t);
+
+	len1 = (len2
+		+ sizeof(pHdr1->len2)
+		+ sizeof(dma_header8_t)) & 0xFFFFF;
+
+	pDMAhdr->hdr8.bytes[0] = DMA_HTYPE_H2M;
+	pDMAhdr->hdr8.bytes[1] = DMA_RQSTR_HOST | DMA_CW_NORM;
+	pDMAhdr->hdr8.bytes[2] = DMA_CW_MRB0 | DMA_CW_TX | DMA_CW_INT_NOIV;
+	pDMAhdr->hdr8.bytes[3] = 0;  // UF (reserved)
+	pDMAhdr->hdr8.bytes[4] = ctx->vfid;
+	// for host->card DMA, the DMA header length is ignored by hardware
+	//pDMAhdr->hdr8.bytes[5] = (len1 & 0x0F0000) >> 16;
+	//pDMAhdr->hdr8.bytes[6] = (len1 & 0x00FF00) >>  8;
+	//pDMAhdr->hdr8.bytes[7] = (len1 & 0x0000FF);
+	pDMAhdr->sig           = cpu_to_be16(DMA_SIG_H2M);
+
+ 	pHdr1->len2          = cpu_to_be16(len2);
+ 	pHdr1->len3          = cpu_to_be16(len3);
+	pHdr1->ep_len        = 0;
+	pHdr1->flags         = HRB_FLAGS_HRA_POOL;
+	pHdr1->agent_id      = 0;
+	pHdr1->num_hras      = cpu_to_be16(1);
+	pHdr1->tag_len_hra.tag    = 0x1;
+	pHdr1->tag_len_hra.len[0] = (hra1_len & 0x0F0000) >> 16;
+	pHdr1->tag_len_hra.len[1] = (hra1_len & 0x00FF00) >>  8;
+	pHdr1->tag_len_hra.len[2] = (hra1_len & 0x0000FF);
+#if defined(WINDOWS)
+	pDT = CONTAINING_RECORD(pReq->DT_hra1.Flink, struct dt_t, list_entry);
+	pHdr1->tag_len_hra.hra = cpu_to_be64(pDT->this_LA);
+#endif
+#if defined(LINUX)
+	pDT = list_entry(pReq->DT_hra1.next, struct dt_t, list_head);
+	pHdr1->tag_len_hra.hra = cpu_to_be64(pDT->this_dma);
+#endif
+	pHdr2->frag_idx    = 0;
+	pHdr2->frag_total  = 0;
+	pHdr2->request_id  = 0;
+	pHdr2->user_def    = 0;
+	pHdr2->status      = 0;
+
+
+	ctx->asym_attr.hra_update_vector = 0;
+	for (i=0; i < ASYM_HRA_POOL_SIZE; i++) 
+		ctx->asym_attr.hra_update_vector |= (1LL << (63-i));
+
+	pPkt->update_vector = cpu_to_be64(ctx->asym_attr.hra_update_vector);
+
+	PDEBUG(1, "Device %d: initial HRA update vector: %016llx\n", ctx->dev_index, ctx->asym_attr.hra_update_vector);
+
+	memset(pPkt->hra_table, 0, sizeof(pPkt->hra_table));
+
+	// each HRA table entry is a DT chain.  extract the addresses of the first DT in each chain
+	//
+	for (i=0; i < ASYM_HRA_POOL_SIZE; i++) {
+		struct list_head *pLH = ctx->asym_attr.DTs[i].next;
+		struct dt_t      *pDT = NULL;
+
+		if (list_empty(&ctx->asym_attr.DTs[i]))
+			continue;
+
+		pDT = list_entry(pLH, struct dt_t, list_head);
+		pPkt->hra_table[i] = cpu_to_be64(pDT->this_dma);
+	}
+
+	rc = alloc_scatterlist(ctx, 
+			       pReq->header, hdr_len + sizeof(hra_pool_packet_t), 
+			       hdr_len + sizeof(hra_pool_packet_t), 
+			       TRUE, 
+			       &pReq->hdr_slist, 
+			       DMA_TO_DEVICE);
+	if (rc != HOST_DD_Good) {
+		PDEBUG(1, "Device %d: alloc_scatterlist hdr failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->hdr_slist);
+	if (rc != HOST_DD_Good) {
+		PDEBUG(1, "Device %d: dt_append_scatterlist hdr failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	dt_set_hrb_len(&pReq->DT_req, len1);
+
+	// consider all HRA entries 'in-use' until we start receiving asym replies
+	//
+	ctx->asym_attr.hra_update_vector = 0;  
+
+	ctx->hra_req = pReq;
+
+	return HOST_DD_Good;
+
+cleanup:
+	if (pReq) {
+		free_scatterlist(&pReq->hdr_slist);
+		free_scatterlist(&pReq->hra1_slist);
+
+		dt_free_chain(ctx, &pReq->DT_req);
+		dt_free_chain(ctx, &pReq->DT_hra1);
+	
+		request_release(ctx, pReq);
+	}
+
+	return rc;
+}
+
+	
+int
+asym_hra_pool_load_packet_send(ibm4767_ctx_t *ctx)
+{
+	request_t          *pReq  = NULL;
+	hra_pool_packet_t  *pPkt  = NULL;
+	hrb_type3_header_t *pHdr1 = NULL;
+	hrb_header_part2_t *pHdr2 = NULL;
+	char               *pBuf  = NULL;
+	struct list_head   *pLH   = NULL;
+	uint32_t            tmp, pad1, hdr_len;
+	int                 rc;
+
+
+	PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+
+	if (!ctx->hra_req) {
+		rc = asym_hra_pool_load_packet_init(ctx);
+		if (rc != HOST_DD_Good)
+			return rc;
+	}
+		
+	// this routine gets called each time the card requests a buffer update.
+	// the HRA table remains unchanged but the update_vector will vary depending
+	// on usage.  since in the 4767 universe, the HRA POOL PACKET will generate
+	// an ACK response, it's tempting to simply treat this like any other request,
+	// allocating and freeing the request on the fly.  that's problematic in the
+	// Windows universe since there are restrictions on when we can allocate
+	// scatterlists and DT chains.  so we pre-allocate the pool packet when we 
+	// know it's safe to do so and simply re-send the same one over and over
+
+	// we need to locate the update_vector in the pre-constructed request...
+	//
+	hdr_len = sizeof(hrb_type3_header_t) + sizeof(hrb_header_part2_t);
+	tmp = (hdr_len + sizeof(hra_pool_packet_t) + 7) & ~7;
+	pad1 = tmp - (hdr_len + sizeof(hra_pool_packet_t));
+
+	pReq = ctx->hra_req;
+	pBuf   = pReq->header;
+	pHdr1  = (hrb_type3_header_t *)pBuf;  pBuf += sizeof(hrb_type3_header_t);
+	pBuf  += pad1;
+	pHdr2  = (hrb_header_part2_t *)pBuf;  pBuf += sizeof(hrb_header_part2_t);
+	pPkt   = (hra_pool_packet_t *)pBuf;
+
+	// it's possible that the update_vector won't have any available slots.  in that case
+	// the comm mgr should send a followup REQBUFUPDATE message...
+	//
+	pPkt->update_vector = cpu_to_be64(ctx->asym_attr.hra_update_vector);
+
+	list_for_each(pLH, &pReq->hdr_slist.frags) {
+		scatter_frag_t *sf = list_entry(pLH, scatter_frag_t, list_head);
+		dma_sync_single_for_device(ctx->dev, sf->vp.dma, sf->vp.len, DMA_TO_DEVICE);
+	}
+
+	// the first request we send to the MCPU following the HRA load packet poses a problem: the refetch
+	// code will remove the last (only) DT from the HRA load packet request and prepend it to the subsequent chain.
+	// normally that's okay but we want the HRA load packet request to persist indefinitely so we'll need to
+	// rebuild the DT chain.  the underlying scatterlists are unaffected.
+	//
+	// update: now that the load packet is created and sent from a timer, it no longer needs to persist so
+	// this step should be a NO-OP now.  i'll leave it as deprecated for the time being...eventually we'll want to
+	// integrate the load_packet_init() and load_packet_send() functions but probably not until I verify that
+	// this design works in the Win2k12 universe...
+	//
+	if (list_empty(&pReq->DT_req)) {
+		PDEBUG(1, "Device %d: re-creating DT chain for HRA load packet\n", ctx->dev_index);
+
+		rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->hdr_slist);
+		if (rc != HOST_DD_Good) {
+			PRINTKE("Device %d: couldn't re-create DT chain for HRA load packet!\n", ctx->dev_index);
+			mark_device_offline(ctx, HOSTNoMemory);
+		}
+	}
+
+
+	// some requests are async and don't sleep waiting for a reply.  HRA load packet is one of them.
+	// explicitly set waitEvent to '1' so flush_request() doesn't get confused if an error occurs.
+	//
+	atomic_set(&pReq->waitEvent, 1);
+	rc = dma_send_request_MCPU(ctx, pReq);
+	if (rc) {
+		PDEBUG(1, "Device %d: Send HRA load packet failed\n", ctx->dev_index);
+	}
+	//else { 
+	//	write32(cpu_to_be32(MBX_SYMPROTOCOL), H2M_MBX_H(ctx));
+	//}
+	
+	return HOST_DD_Good;
+}
+
+
+int
+asym_hra_pool_load_packet_reply(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	mrm_type1_header_t *mrm_hdr = NULL;
+	scatter_frag_t     *sf      = NULL;
+	uint32_t          len;   
+
+
+	// HRA load packet replies consist only of an mrm_type1_header_t so
+	// they're small enough to fit in a single scatter frag...
+	//
+	sf = list_entry(pReq->hra1_slist.frags.next, scatter_frag_t, list_head);
+	mrm_hdr = (mrm_type1_header_t *)sf->vp.va;
+	len     = sf->len;
+
+	if (mrm_hdr->flags != MRM_FLAGS_HRA_ACK) {
+		PRINTKW("Device %d: bad HRA load packet reply\n", ctx->dev_index); 
+		mark_mcpu_offline(ctx, HOSTFirmwareError);
+		return HOST_DD_BadAddress;
+	}
+
+	// The Fence defines about a dozen possible error codes.  Most are bad errors that imply something
+	// bizarre happened that corrupted the packet data so just treat them as showstoppers and be done with it
+	//
+	if (mrm_hdr->status) {
+		PRINTKW("Device %d:  HRA load packet reply error (0x%08x)\n", ctx->dev_index, mrm_hdr->status);
+		mark_mcpu_offline(ctx, HOSTFirmwareError);
+	}
+	else {
+		PDEBUG(1, "Device %d:  HRA load packet reply success...\n", ctx->dev_index);
+		// don't set MCPU_ACTIVE until we receive at least one AGENTID_ATTACHED notice
+		//ctx->status_mcpu = MCPU_ACTIVE;
+	}
+
+	if (ctx->hra_req) {
+		free_scatterlist(&ctx->hra_req->hdr_slist);
+		free_scatterlist(&ctx->hra_req->hra1_slist);
+
+		dt_free_chain(ctx, &ctx->hra_req->DT_req);
+		dt_free_chain(ctx, &ctx->hra_req->DT_hra1);
+
+		request_release(ctx, ctx->hra_req);
+		ctx->hra_req = NULL;
+	}
+
+	return HOST_DD_Good;
+}
diff --git drivers/misc/ibm4767/common-ras.h drivers/misc/ibm4767/common-ras.h
new file mode 100755
index 000000000000..bf846576878a
--- /dev/null
+++ drivers/misc/ibm4767/common-ras.h
@@ -0,0 +1,215 @@
+ /************************************************************************
+ *  Filename:common-ras.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Visegrady, Tamas  IBM Poughkeepsie  <tamas@us.ibm.com>
+ *           Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common sanity checks for transport-related files  
+ *                    (dt, dma, hra, htb, mailbox)                      
+ *                                                                      
+ * Change History:                                                      
+ * Author:           J.P.Andruzzi <jandruzz@us.ibm.com>                 
+ * Date:             2004.10.28                                         
+ * Description       Changed Copyright notice for General Availability  
+ ************************************************************************/
+
+// $Date$
+
+#ifndef  _COMMON_RAS_H_
+#define  _COMMON_RAS_H_
+
+//#include <asm/io.h>
+
+#define  RAS				2	/* controls assert()s */
+#define  LINELENGTH			150	/* used by _info formatters */
+
+#define  LOG_PREFIX  "ibm4767: "
+
+/* Mainly used by sanity checks on function inputs... */
+
+
+/* Kernel modules are gcc only, obviously, but who knows... */
+
+#ifdef __GNUC__
+#  define  UNLIKELY(expression)  (__builtin_expect((expression), 0))
+#  define  LIKELY(expression)    (__builtin_expect((expression), 1))
+#else
+#  define  UNLIKELY(expression)  (expression)
+#  define  LIKELY(expression)    (expression)
+#endif
+
+
+#define  return_rc_if(expr, rc)						\
+		if UNLIKELY(expr) { assertfail(expr); return (rc); }
+#define  return_void_if(expr)						\
+		if UNLIKELY(expr) { assertfail(expr); return; }
+
+#define  return_fatal_if(expr)						\
+		if UNLIKELY(expr) { assertfail(expr); return -EFAULT; }
+
+#define  return_zero_if(expr)						\
+		return_rc_if((expr), 0);
+#define  return_if(expr)						\
+		return_rc_if((expr), -EINVAL);
+
+#define  return_sg_if_nullptr(ptr, rc)					\
+		return_rc_if(NULL == (ptr), (rc));
+
+#define  return_if_nullptr(ptr)						\
+		return_sg_if_nullptr((ptr), -EINVAL)
+#define  return_zero_if_nullptr(ptr)					\
+		return_sg_if_nullptr((ptr), 0)
+#define  return_null_if_nullptr(ptr)					\
+		return_sg_if_nullptr((ptr), NULL)
+#define  return_void_if_nullptr(ptr)					\
+		if UNLIKELY(NULL == (ptr)) { assert(NULL != (ptr)); return; }
+
+#define  return_sg_if_nullbuffer(len, rc)				\
+		return_rc_if(0 == (len), (rc));
+#define  return_if_nullbuffer(len)					\
+		return_sg_if_nullbuffer((len), -EINVAL)
+
+
+#define  is_valid_card(card)						\
+		UNLIKELY((card < MAX_DEV_COUNT) &&			\
+			 (NULL != Xanchor.device_addresses[(card)]))
+
+#define  return_sg_if_invalid_card(card, rc)				\
+		if UNLIKELY(! is_valid_card(card)) {			\
+			assert((card) < MAX_DEV_COUNT);			\
+			assert(NULL != Xanchor.device_addresses[(card)]); \
+			PDEBUG(2, "Invalid card\n");			\
+			return (rc);					\
+		}
+#define  return_if_invalid_card(card)					\
+		return_sg_if_invalid_card(card, -EINVAL)
+#define  return_null_if_invalid_card(card)				\
+		return_sg_if_invalid_card(card, NULL)
+#define  return_zero_if_invalid_card(card)				\
+		return_sg_if_invalid_card(card, 0)
+#define  return_void_if_invalid_card(card)				\
+		return_void_if((MAX_DEV_COUNT <= card) ||		\
+			(NULL == Xanchor.device_addresses[(card)]));
+
+
+#if defined(DEBUG)
+#  define memset_if_debug(addr, byte, len)    { memset(addr, byte, len); }
+#else
+#  define memset_if_debug(addr, byte, len)    do { } while (0)
+#endif
+
+
+/* READL is awkward, but more importantly it logs. The unused         */
+/* parameter in READL_NOLOG is intentional, it enables                */
+/* quick-and-dirty transformations between READL and READL_NOLOG.     */
+
+#  define  WRITEL(what, where) {					\
+		PDEBUG(2, "writel(%08X->%p)\n",				\
+			(u32) (what), (void *)(where));			\
+		writel((what), (void *)(where));			\
+		}
+
+#  define  READL(where, whereto, default)				\
+		{ u32 READL_tmp = readl((void *)(where));		\
+									\
+		PDEBUG(2, "readl(%p)==%08X\n",				\
+			(void *) (where), READL_tmp);			\
+		(whereto) = READL_tmp;					\
+		}
+
+#  define  READL_NOLOG(where, whereto, default)				\
+		{ (whereto) = readl((void *)(where)); }
+
+
+
+#if  defined(RAS)  && RAS
+#  define assertfail(expr) {						\
+		printk(KERN_ERR LOG_PREFIX "Assertion failed: (! %s),%s,%s,l" \
+		"=%d\n", #expr,__FILE__,__FUNCTION__,__LINE__);		\
+	}
+#  define assert(expr)		{ if UNLIKELY(!(expr)) { assertfail(expr) } }
+#else
+#  define assertfail(expr)	do { } while (0)
+#  define assert(expr)		do { } while (0)
+#endif
+
+
+/* Paranoid asserts protect against a module clobbering its own structures,  */
+/* i.e., checks not related to interfaces.  These should be somewhat safer   */
+/* to disable than other assertions (neither of them are recommended).       */
+/* For consistency, the usage of _paranoid_ macros is recommended only for   */
+/* sanity checking conditions limited to values in the same module/file.     */
+
+#if  defined(RAS)  && (2 <= RAS)
+#  define  assert_paranoid(expr)					\
+		{ if UNLIKELY(!(expr)) { assertfail(expr) } }
+#  define  return_if_paranoid(expr)					\
+		return_if(expr)
+#  define  return_if_nullptr_paranoid(ptr)				\
+		return_sg_if_nullptr((ptr), -EINVAL)
+#  define  return_zero_if_nullptr_paranoid(ptr)				\
+		return_sg_if_nullptr((ptr), 0)
+#  define  return_void_if_nullptr_paranoid(ptr)				\
+		return_void_if_nullptr(ptr)
+#  define  return_null_if_nullptr_paranoid(ptr)				\
+		return_null_if_nullptr(ptr)
+#else
+#  define  assert_paranoid(expr)		 do { } while (0)
+#  define  return_if_paranoid(ptr)		 do { } while (0)
+#  define  return_if_nullptr_paranoid(ptr)	 do { } while (0)
+#  define  return_zero_if_nullptr_paranoid(ptr)  do { } while (0)
+#  define  return_void_if_nullptr_paranoid(ptr)	 do { } while (0)
+#  define  return_null_if_nullptr_paranoid(ptr)	 do { } while (0)
+#endif
+
+
+/* Coercing storage access to u32 or u8 from a given offset is very common,  */
+/* so it's worth macroizing globally.  It's not very nice visually, but it   */
+/* is correct C, and works.  (The offset version with a zero offset gets     */
+/* optimized away to the base one, BTW.)                                     */
+
+#define  u8__at_(var)		(           *((u8*) &( var )))
+#define  u8__off(var, offset)	(*((offset) + (u8*) &( var )))
+
+#define  u16_at_(var)		(           *((u16*) &( var )))
+#define  u16_off(var, offset)	(*((offset) + (u16*) &( var )))
+
+#define  u32_at_(var)		(           *((u32*) &( var )))
+#define  u32_off(var, offset)	(*((offset) + (u32*) &( var )))
+
+#define  u64_at_(var)		(           *((u64*) &( var )))
+#define  u64_off(var, offset)	(*((offset) + (u64*) &( var )))
+
+
+/* These special cases are used several times in debugging output.          */
+
+#define  be16_at_(var)		__be16_to_cpu(u16_at_(var))
+#define  be16_off(var, offset)	__be16_to_cpu(u16_off((var), (offset)))
+
+#define  be32_at_(var)		__be32_to_cpu(u32_at_(var))
+#define  be32_off(var, offset)	__be32_to_cpu(u32_off((var), (offset)))
+
+#define  be64_at_(var)		__be64_to_cpu(u64_at_(var))
+#define  be64_off(var, offset)	__be64_to_cpu(u64_off((var), (offset)))
+
+#endif
+
diff --git drivers/misc/ibm4767/dma.c drivers/misc/ibm4767/dma.c
new file mode 100755
index 000000000000..f79a67b382a6
--- /dev/null
+++ drivers/misc/ibm4767/dma.c
@@ -0,0 +1,784 @@
+/*************************************************************************
+ *  Filename:dma.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_mailbox.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+
+int
+dma_initialize(ibm4767_ctx_t *ctx)
+{ return HOST_DD_Good; }
+
+
+void
+dma_unload(ibm4767_ctx_t *ctx)
+{ }
+
+
+
+void 
+dma_enable(ibm4767_ctx_t *ctx, uint32_t mask)
+{
+	u16 tmp16;
+	u32 tmp32;
+
+	PDEBUG(1, "Device %d: dma_enable (mask = 0x%08x)\n", ctx->dev_index, mask);
+
+	PDEBUG(3, "Device %d: HBMCR [%p]: %08x\n", 
+			ctx->dev_index,
+			(void *)HBMCR(ctx), cpu_to_be32(read32(HBMCR(ctx))));
+
+	pci_read_config_word(ctx->pci_dev, PCI_COMMAND, &tmp16);
+	tmp16 |= (PCI_COMMAND_MASTER | PCI_COMMAND_IO | PCI_COMMAND_MEMORY);
+	pci_write_config_word(ctx->pci_dev, PCI_COMMAND, tmp16);
+
+	PDEBUG(3, "Device %d: setting PCI_COMMAND register to 0x%04x\n", ctx->dev_index, tmp16);
+
+	// attempt to disable PCIe relaxed ordering...
+	//
+	tmp32 = pci_find_capability(ctx->pci_dev, PCI_CAP_ID_EXP);
+	if (!tmp32) {
+		PRINTKW("Device %d: Couldn't find PCIe capability!\n", ctx->dev_index);
+	}
+	else {
+		pci_read_config_word(ctx->pci_dev, tmp32+PCI_EXP_DEVCTL, &tmp16);
+		tmp16 = tmp16 & ~PCI_EXP_DEVCTL_RELAX_EN;
+		PDEBUG(3, "Device %d: setting PCI_DEV_CTL to 0x%04x\n", ctx->dev_index, tmp16);
+
+		pci_write_config_word(ctx->pci_dev, tmp32+PCI_EXP_DEVCTL, tmp16);
+	}
+
+	// enable PCI-E ECRC if directed to...
+	//
+	// if enabled, ECRC needs to be enabled on the bridge, too.  caveat: I'm pretty sure 
+	// that nobody has stepped-up to support PCIe AER on x86 kernels so this will likely 
+	// be a no-op until then...
+	//
+	// Also note that mucking with the bridge like this probably won't work in the Windows 
+	// universe..
+	//
+	if (enable_ecrc) {
+		struct pci_dev *bridge = ctx->bridge_dev;
+
+		if (!bridge) {
+			PRINTKW("Device %d: No PCIe bridge (running virtualized maybe?).  Skipping PCI ECRC\n", ctx->dev_index);
+			goto ecrc_done;
+		}
+
+		do {
+			// locate the root bridge.  see section 7.8.2 of PCIe spec
+			// we're interested in bits 4:7.  alternatively, we can assume that
+			// Linux PCI code is sane and just walk up the parent chain until
+			// bus->self == NULL...
+			//
+			tmp32 = pci_find_capability(bridge, PCI_CAP_ID_EXP);
+			if (!tmp32) {
+				PRINTKW("Device %d: Failed to find bridge PCI capability\n", ctx->dev_index);
+				goto ecrc_done;
+			}
+
+			pci_read_config_word(bridge, tmp32+PCI_CAP_FLAGS, &tmp16);
+			if (((tmp16 & PCI_EXP_FLAGS_TYPE) >> 4) == PCI_EXP_TYPE_ROOT_PORT)
+				break;
+			bridge = bridge->bus->self;
+		} while (bridge);
+
+		if (!bridge) {
+			PRINTKW("Device %d: Failed to locate PCIe bridge!?\n", ctx->dev_index);
+			goto ecrc_done;
+		}
+
+		// see section PCIe spec section 7.10...
+		//
+		tmp32 = pci_find_capability(bridge, PCI_EXT_CAP_ID_ERR);
+		if (!tmp32) {
+			PRINTKW("Device %d: Can't find PCI EXT capability on bridge\n", ctx->dev_index);
+			goto ecrc_done;
+		}
+		else {
+			uint32_t val32;
+			pci_read_config_dword(bridge, tmp32+PCI_ERR_CAP, &val32);
+			if (!(val32 & (PCI_ERR_CAP_ECRC_GENC | PCI_ERR_CAP_ECRC_CHKC))) {
+				PRINTKW("Device %d: Warning - bridge doesn't support ECRC_GENC or ECRC_CHKC.\n", ctx->dev_index);
+				goto ecrc_done;
+			}
+			val32 |= (PCI_ERR_CAP_ECRC_GENE | PCI_ERR_CAP_ECRC_CHKE);
+			pci_write_config_dword(bridge, tmp32+PCI_ERR_CAP, val32);
+
+			PDEBUG(2, "Device %d: setting bridge PCI_ERR_CAP to 0x%08x\n", ctx->dev_index, val32);
+		}
+
+		// now, turn on ECRC on the device...
+		//
+		tmp32 = pci_find_capability(ctx->pci_dev, PCI_EXT_CAP_ID_ERR);
+		if (!tmp32) {
+			PRINTKW("Device %d: Can't find PCI EXT capability\n", ctx->dev_index);
+			goto ecrc_done;
+		}
+		else {
+			uint32_t val32;
+			pci_read_config_dword(ctx->pci_dev, tmp32+PCI_ERR_CAP, &val32);
+			if (!(val32 & (PCI_ERR_CAP_ECRC_GENC | PCI_ERR_CAP_ECRC_CHKC))) {
+				PRINTKW("Device %d: Warning - device doesn't support ECRC_GENC or ECRC_CHKC\n", ctx->dev_index);
+				goto ecrc_done;
+			}
+			val32 |= (PCI_ERR_CAP_ECRC_GENE | PCI_ERR_CAP_ECRC_CHKE);
+			pci_write_config_dword(ctx->pci_dev, tmp32+PCI_ERR_CAP, val32);
+
+			PDEBUG(2, "Device %d: setting PCI_ERR_CAP to 0x%08x\n", ctx->dev_index, val32);
+		}
+	}
+
+ecrc_done:
+
+	// set the max burst length in the HBMCR...only valid for PF...
+	//
+	tmp32 = read32(HBMCR(ctx));
+	write32(tmp32 | cpu_to_be32(mask), HBMCR(ctx));
+	mdelay(100);
+
+	PDEBUG(3, "Device %d: New HBMCR [%p]: %08x\n", 
+			ctx->dev_index,  (void *)HBMCR(ctx), cpu_to_be32(read32(HBMCR(ctx))));
+
+	ctx->dma_mask |= mask;
+
+}
+
+
+void 
+dma_disable(ibm4767_ctx_t *ctx, uint32_t mask)
+{
+	uint32_t tmp;
+
+	PDEBUG(1, "Device %d: dma_disable (mask = 0x%08x)\n", ctx->dev_index, mask);
+
+	tmp = read32(HBMCR(ctx));
+	
+	PDEBUG(3, "Device %d: Old HBMCR [%p]: %08x\n", ctx->dev_index, (void *)HBMCR(ctx), cpu_to_be32(tmp));
+
+	write32(tmp & ~(cpu_to_be32(mask)), HBMCR(ctx));
+
+	wmb();
+		
+	PDEBUG(3, "Device %d: New HBMCR [%p]: %08x\n", 
+			ctx->dev_index, (void *)HBMCR(ctx), cpu_to_be32(read32(HBMCR(ctx))));
+
+	ctx->dma_mask = ctx->dma_mask & ~mask;
+}
+
+
+int
+dma_check_fp_enable(ibm4767_ctx_t *ctx, uint64_t hbmsr_mask, uint32_t hbmcr_mask)
+{
+	uint64_t hbmsr;
+	uint32_t hbmcr;
+
+	if (ctx->dma_mask & hbmcr_mask)
+		return HDD_SUCCESS;
+
+
+	hbmsr = cpu_to_be64(read64(HBMSR(ctx)));
+	if (hbmsr & hbmsr_mask) {
+		hbmcr = read32(HBMCR(ctx));
+		write32(hbmcr | cpu_to_be32(hbmcr_mask), HBMCR(ctx));
+
+		hbmcr = cpu_to_be32(read32(HBMCR(ctx)));
+		if ((hbmcr & hbmcr_mask) == 0) {
+			PRINTKW("Device %d: HBMSR says mask is enabled but unable to enable via HBMCR\n", ctx->dev_index);
+			return HDD_FAIL;
+		}
+		ctx->dma_mask |= hbmcr_mask;
+	}
+	else {
+		PRINTKW("Device %d: FP disabled by HBMSR\n", ctx->dev_index);
+		return HDD_FAIL;
+	}
+
+	return HDD_SUCCESS;
+}
+
+
+int
+dma_send_request_MCPU(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	struct list_head *pLH;
+	struct dt_t *pLastDT;
+	int    refetch_needed = FALSE;
+
+
+	PDEBUG(1, "Enter %s...\n", __FUNCTION__);
+
+	if (list_empty(&pReq->DT_req)) {
+		PDEBUG(1, "Error...empty request chain!?\n");
+		return HDD_FAIL;
+	}
+
+	spin_lock_bh(&ctx->fetch_lock_mcpu);
+	if (ctx->last_DT_MCPU) 
+		refetch_needed = TRUE;
+
+	// a brief discussion about DTs:  DTs belong to two linked-lists.  
+	// one is linked by DMA addrs and is used by the HW to read the
+	// request and the other is linked by virtual addrs.  since
+	// a refetch causes the HW to re-read the last DT in the previous
+	// chain, we can't free it with the other DTs when the request
+	// completes (we can, however, free the request *data* that the
+	// DT points to).  instead, we have to remove the last DT from 
+	// the current virtual chain and hold it in limbo until the next 
+	// request is sent to the card.  we can then free the DT along
+	// with all but the last DTs belonging to the new request...
+	//
+	if (refetch_needed) {
+		PDEBUG(1, "Device %d: REFETCH MCPU\n", ctx->dev_index);
+		// must dump chain before prepending last DT to chain since its data pointer
+		// might no longer be valid
+		//
+		//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_req, TRUE);
+		//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+
+		dt_prepend(&pReq->DT_req, ctx->last_DT_MCPU); 
+
+		pLH = list_remove_tail(&pReq->DT_req);
+		pLastDT = list_entry(pLH, struct dt_t, list_head);
+		ctx->last_DT_MCPU = pLastDT;
+		
+		spin_unlock_bh(&ctx->fetch_lock_mcpu);
+		
+		write32(cpu_to_be32(HCR_REFETCH_H2M), HCR(ctx));
+		return HOST_DD_Good;
+	}
+	else {
+		uint64_t  hbmsr;
+
+		PDEBUG(1, "Device %d: FETCH MCPU\n", ctx->dev_index);
+		hbmsr = cpu_to_be64(read64(HBMSR(ctx)));
+		if (!(hbmsr & HBMSR_H2M_TCPR)) {
+			PDEBUG(1, "Device %d: card busy at fetch!\n", ctx->dev_index);
+			spin_unlock_bh(&ctx->fetch_lock_mcpu);
+			return HDD_FAIL;
+		}
+		else {
+			struct dt_t  *pDT;
+			dma_addr_t BE_dma;
+
+			//PDEBUG(3, "Device %d: HRA load chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_req, TRUE);
+			
+			pDT = list_entry(pReq->DT_req.next, struct dt_t, list_head);
+			BE_dma = cpu_to_be64(pDT->this_dma);
+
+			pLH = list_remove_tail(&pReq->DT_req);
+			pLastDT = list_entry(pLH, struct dt_t, list_head); 
+			ctx->last_DT_MCPU = pLastDT;
+			
+			PDEBUG(2, "Device %d: Fetch DT:  VA: %p, BE_LA: %016llx\n", ctx->dev_index, pDT, BE_dma);
+
+			write64(BE_dma, H2M_TCP(ctx));
+			write32(cpu_to_be32(HCR_FETCH_H2M), HCR(ctx));
+			
+			spin_unlock_bh(&ctx->fetch_lock_mcpu);
+		}
+	}
+
+	return HOST_DD_Good;
+
+}
+
+
+int
+dma_send_request_SSP(ibm4767_ctx_t  *ctx, request_t *pReq)
+{
+	struct list_head *pLH;
+	struct dt_t       *pLastDT;
+	int         refetch_needed = FALSE;
+
+
+	PDEBUG(2, "Enter %s...\n", __FUNCTION__);
+
+	if (list_empty(&pReq->DT_req)) {
+		PDEBUG(1, "Error...empty request chain!?\n");
+		return HDD_FAIL;
+	}
+
+	spin_lock_bh(&ctx->fetch_lock_ssp);
+	if (ctx->last_DT_SSP) 
+		refetch_needed = TRUE;
+
+	// a brief discussion about DTs:  DTs belong to two linked-lists.  
+	// one is linked by DMA addrs and is used by the HW to read the
+	// request and the other is linked by virtual addrs.  since
+	// a refetch causes the HW to re-read the last DT in the previous
+	// chain, we can't free it with the other DTs when the request
+	// completes (we can, however, free the request *data* that the
+	// DT points to).  instead, we have to remove the last DT from 
+	// the current virtual chain and hold it in limbo until the next 
+	// request is sent to the card.  we can then free the DT along
+	// with all but the last DTs belonging to the new request...
+	//
+	if (refetch_needed) {
+		PDEBUG(2, "Device %d: REFETCH SSP\n", ctx->dev_index);
+		// must dump chain before prepending last DT to chain since its data pointer
+		// might no longer be valid
+		//
+		//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_req, TRUE);
+		//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+
+		dt_prepend(&pReq->DT_req, ctx->last_DT_SSP); 
+
+		pLH = list_remove_tail(&pReq->DT_req);
+		pLastDT = list_entry(pLH, struct dt_t, list_head);
+		ctx->last_DT_SSP = pLastDT;
+	
+		spin_unlock_bh(&ctx->fetch_lock_ssp);
+		
+		write32(cpu_to_be32(HCR_REFETCH_H2S), HCR(ctx));
+		return HOST_DD_Good;
+	}
+	else {
+		uint64_t  hbmsr;
+
+		PDEBUG(1, "Device %d: FETCH SSP\n", ctx->dev_index);
+		hbmsr = cpu_to_be64(read64(HBMSR(ctx)));
+		if (!(hbmsr & HBMSR_H2S_TCPR)) {
+			PDEBUG(1, "Device %d: SSP busy at fetch!\n", ctx->dev_index);
+			spin_unlock_bh(&ctx->fetch_lock_ssp);
+			return HDD_FAIL;
+		}
+		else {
+			struct dt_t  *pDT;
+			dma_addr_t BE_dma;
+
+			//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_req, TRUE);
+			//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+
+			pDT = list_entry(pReq->DT_req.next, struct dt_t, list_head);
+			BE_dma = cpu_to_be64(pDT->this_dma);
+
+			pLH = list_remove_tail(&pReq->DT_req);
+			pLastDT = list_entry(pLH, struct dt_t, list_head); 
+			ctx->last_DT_SSP = pLastDT;
+
+			PDEBUG(2, "Device %d: Fetch DT:  VA: %p, BE_LA: %016llx\n", ctx->dev_index, pDT, BE_dma);
+
+			write64(BE_dma, H2S_TCP(ctx));
+			write32(cpu_to_be32(HCR_FETCH_H2S), HCR(ctx));
+			
+			spin_unlock_bh(&ctx->fetch_lock_ssp);
+		}
+	}
+
+	return HOST_DD_Good;
+}
+
+
+int
+dma_send_request_SKCH(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	struct list_head *pLH;
+	struct dt_t *pLastDT;
+	int    refetch_needed = FALSE;
+
+
+	PDEBUG(2, "Enter %s...\n", __FUNCTION__);
+
+	if (list_empty(&pReq->DT_req)) {
+		PDEBUG(1, "Error...empty request chain!?\n");
+		return HDD_FAIL;
+	}
+
+	spin_lock_bh(&ctx->fetch_lock_skch);
+	if (ctx->last_DT_SKCH) 
+		refetch_needed = TRUE;
+
+	// a brief discussion about DTs:  DTs belong to two linked-lists.  
+	// one is linked by DMA addrs and is used by the HW to read the
+	// request and the other is linked by virtual addrs.  since
+	// a refetch causes the HW to re-read the last DT in the previous
+	// chain, we can't free it with the other DTs when the request
+	// completes (we can, however, free the request *data* that the
+	// DT points to).  instead, we have to remove the last DT from 
+	// the current virtual chain and hold it in limbo until the next 
+	// request is sent to the card.  we can then free the DT along
+	// with all but the last DTs belonging to the new request...
+	//
+	if (refetch_needed) {
+		PDEBUG(2, "Device %d: REFETCH SKCH\n", ctx->dev_index);
+		// must dump chain before prepending last DT to chain since its data pointer
+		// might no longer be valid
+		//
+		//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_req, TRUE);
+		//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+
+		dt_prepend(&pReq->DT_req, ctx->last_DT_SKCH); 
+
+		pLH = list_remove_tail(&pReq->DT_req);
+		pLastDT = list_entry(pLH, struct dt_t, list_head);
+		ctx->last_DT_SKCH = pLastDT;
+		
+		write32(cpu_to_be32(HCR_REFETCH_SK), HCR(ctx));
+		spin_unlock_bh(&ctx->fetch_lock_skch);
+
+		return HOST_DD_Good;
+	}
+	else {
+		uint64_t  hbmsr;
+
+		PDEBUG(2, "Device %d: FETCH SKCH\n", ctx->dev_index);
+		hbmsr = cpu_to_be64(read64(HBMSR(ctx)));
+		if (!(hbmsr & HBMSR_H2SK_TCPR)) {
+			PDEBUG(1, "Device %d: card busy at fetch!\n", ctx->dev_index);
+			spin_unlock_bh(&ctx->fetch_lock_skch);
+			return HDD_FAIL;
+		}
+		else {
+			struct dt_t  *pDT;
+			dma_addr_t BE_dma;
+
+			//PDEBUG(3, "Device %d: SKCH request chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_req, TRUE);
+			
+			pDT = list_entry(pReq->DT_req.next, struct dt_t, list_head);
+			BE_dma = cpu_to_be64(pDT->this_dma);
+
+			pLH = list_remove_tail(&pReq->DT_req);
+			pLastDT = list_entry(pLH, struct dt_t, list_head); 
+			ctx->last_DT_SKCH = pLastDT;
+			
+			PDEBUG(2, "Device %d: Fetch DT:  VA: %p, BE_LA: %016llx\n", ctx->dev_index, pDT, BE_dma);
+
+			write64(BE_dma, H2SK_TCP(ctx));
+			write32(cpu_to_be32(HCR_FETCH_SK), HCR(ctx));
+			
+			spin_unlock_bh(&ctx->fetch_lock_skch);
+		}
+	}
+
+	return HOST_DD_Good;
+
+}
+
+
+int
+dma_send_request_PKA(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	struct list_head *pLH;
+	struct dt_t *pLastDT;
+	int    refetch_needed = FALSE;
+
+
+	PDEBUG(1, "Enter %s...\n", __FUNCTION__);
+
+	if (list_empty(&pReq->DT_req)) {
+		PDEBUG(1, "Error...empty request chain!?\n");
+		return HDD_FAIL;
+	}
+
+	spin_lock_bh(&ctx->fetch_lock_pka);
+	if (ctx->last_DT_PKA) 
+		refetch_needed = TRUE;
+
+	// a brief discussion about DTs:  DTs belong to two linked-lists.  
+	// one is linked by DMA addrs and is used by the HW to read the
+	// request and the other is linked by virtual addrs.  since
+	// a refetch causes the HW to re-read the last DT in the previous
+	// chain, we can't free it with the other DTs when the request
+	// completes (we can, however, free the request *data* that the
+	// DT points to).  instead, we have to remove the last DT from 
+	// the current virtual chain and hold it in limbo until the next 
+	// request is sent to the card.  we can then free the DT along
+	// with all but the last DTs belonging to the new request...
+	//
+	if (refetch_needed) {
+		PDEBUG(1, "Device %d: REFETCH PKA\n", ctx->dev_index);
+		// must dump chain before prepending last DT to chain since its data pointer
+		// might no longer be valid
+		//
+		//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_req, TRUE);
+		//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+		dt_prepend(&pReq->DT_req, ctx->last_DT_PKA); 
+
+		pLH = list_remove_tail(&pReq->DT_req);
+		pLastDT = list_entry(pLH, struct dt_t, list_head);
+		ctx->last_DT_PKA = pLastDT;
+		
+		spin_unlock_bh(&ctx->fetch_lock_pka);
+		
+		write32(cpu_to_be32(HCR_REFETCH_MM), HCR(ctx));
+		return HOST_DD_Good;
+	}
+	else {
+		uint64_t  hbmsr;
+
+		PDEBUG(1, "Device %d: FETCH PKA\n", ctx->dev_index);
+		hbmsr = cpu_to_be64(read64(HBMSR(ctx)));
+		if (!(hbmsr & HBMSR_H2MM_TCPR)) {
+			PDEBUG(1, "Device %d: card busy at fetch!\n", ctx->dev_index);
+			spin_unlock_bh(&ctx->fetch_lock_pka);
+			return HDD_FAIL;
+		}
+		else {
+			struct dt_t  *pDT;
+			dma_addr_t BE_dma;
+			
+			//PDEBUG(3, "Device %d: HRA load chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_req, TRUE);
+			
+			pDT = list_entry(pReq->DT_req.next, struct dt_t, list_head);
+			BE_dma = cpu_to_be64(pDT->this_dma);
+
+			pLH = list_remove_tail(&pReq->DT_req);
+			pLastDT = list_entry(pLH, struct dt_t, list_head); 
+			ctx->last_DT_PKA = pLastDT;
+			
+			PDEBUG(2, "Device %d: Fetch DT:  VA: %p, BE_LA: %016llx\n", ctx->dev_index, pDT, BE_dma);
+
+			write64(BE_dma, H2MM_TCP(ctx));
+			write32(cpu_to_be32(HCR_FETCH_MM), HCR(ctx));
+			
+			spin_unlock_bh(&ctx->fetch_lock_pka);
+		}
+	}
+
+	return HOST_DD_Good;
+
+}
+
+	
+int
+dma_send_request_FPGA_A(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	struct list_head *pLH;
+	struct dt_t *pLastDT;
+	int    refetch_needed = FALSE;
+
+
+	PDEBUG(1, "Enter %s...\n", __FUNCTION__);
+
+	if (list_empty(&pReq->DT_req)) {
+		PDEBUG(1, "Error...empty request chain!?\n");
+		return HDD_FAIL;
+	}
+
+	spin_lock_bh(&ctx->fetch_lock_fpgaA);
+	if (ctx->last_DT_FPGA_A) 
+		refetch_needed = TRUE;
+
+	// a brief discussion about DTs:  DTs belong to two linked-lists.  
+	// one is linked by DMA addrs and is used by the HW to read the
+	// request and the other is linked by virtual addrs.  since
+	// a refetch causes the HW to re-read the last DT in the previous
+	// chain, we can't free it with the other DTs when the request
+	// completes (we can, however, free the request *data* that the
+	// DT points to).  instead, we have to remove the last DT from 
+	// the current virtual chain and hold it in limbo until the next 
+	// request is sent to the card.  we can then free the DT along
+	// with all but the last DTs belonging to the new request...
+	//
+	if (refetch_needed) {
+		PDEBUG(2, "Device %d: REFETCH FPGA-A\n", ctx->dev_index);
+		// must dump chain before prepending last DT to chain since its data pointer
+		// might no longer be valid
+		//
+		//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_req, TRUE);
+		//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+
+		dt_prepend(&pReq->DT_req, ctx->last_DT_FPGA_A); 
+
+		pLH = list_remove_tail(&pReq->DT_req);
+		pLastDT = list_entry(pLH, struct dt_t, list_head);
+		ctx->last_DT_FPGA_A = pLastDT;
+		
+		spin_unlock_bh(&ctx->fetch_lock_fpgaA);
+		
+		write32(cpu_to_be32(HCR_REFETCH_H2FA), HCR(ctx));
+		return HOST_DD_Good;
+	}
+	else {
+		uint64_t  hbmsr;
+
+		PDEBUG(1, "Device %d: FETCH FPGA-A\n", ctx->dev_index);
+		hbmsr = cpu_to_be64(read64(HBMSR(ctx)));
+		if (!(hbmsr & HBMSR_H2FA_TCPR)) {
+			PDEBUG(1, "Device %d: card busy at fetch!\n", ctx->dev_index);
+			spin_unlock_bh(&ctx->fetch_lock_fpgaA);
+			return HDD_FAIL;
+		}
+		else {
+			struct dt_t  *pDT;
+			dma_addr_t BE_dma;
+			
+			//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_req, TRUE);
+			//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+			
+			pDT = list_entry(pReq->DT_req.next, struct dt_t, list_head);
+			BE_dma = cpu_to_be64(pDT->this_dma);
+
+			pLH = list_remove_tail(&pReq->DT_req);
+			pLastDT = list_entry(pLH, struct dt_t, list_head); 
+			ctx->last_DT_FPGA_A = pLastDT;
+			
+			PDEBUG(2, "Device %d: Fetch DT:  VA: %p, BE_LA: %016llx\n", ctx->dev_index, pDT, BE_dma);
+
+			write64(BE_dma, H2FA_TCP(ctx));
+			write32(cpu_to_be32(HCR_FETCH_H2FA), HCR(ctx));
+			
+			spin_unlock_bh(&ctx->fetch_lock_fpgaA);
+		}
+	}
+
+	return HOST_DD_Good;
+
+}
+
+	
+int
+dma_send_request_FPGA_B(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	struct list_head *pLH;
+	struct dt_t *pLastDT;
+	int    refetch_needed = FALSE;
+
+
+	PDEBUG(1, "Enter %s...\n", __FUNCTION__);
+
+	if (list_empty(&pReq->DT_req)) {
+		PDEBUG(1, "Error...empty request chain!?\n");
+		return HDD_FAIL;
+	}
+
+	spin_lock_bh(&ctx->fetch_lock_fpgaB);
+	if (ctx->last_DT_FPGA_B) 
+		refetch_needed = TRUE;
+
+	// a brief discussion about DTs:  DTs belong to two linked-lists.  
+	// one is linked by DMA addrs and is used by the HW to read the
+	// request and the other is linked by virtual addrs.  since
+	// a refetch causes the HW to re-read the last DT in the previous
+	// chain, we can't free it with the other DTs when the request
+	// completes (we can, however, free the request *data* that the
+	// DT points to).  instead, we have to remove the last DT from 
+	// the current virtual chain and hold it in limbo until the next 
+	// request is sent to the card.  we can then free the DT along
+	// with all but the last DTs belonging to the new request...
+	//
+	if (refetch_needed) {
+		PDEBUG(1, "Device %d: REFETCH FPGA-B\n", ctx->dev_index);
+		// must dump chain before prepending last DT to chain since its data pointer
+		// might no longer be valid
+		//
+		//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_req, TRUE);
+		//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+
+		dt_prepend(&pReq->DT_req, ctx->last_DT_FPGA_B); 
+
+		pLH = list_remove_tail(&pReq->DT_req);
+		pLastDT = list_entry(pLH, struct dt_t, list_head);
+		ctx->last_DT_FPGA_B = pLastDT;
+		
+		spin_unlock_bh(&ctx->fetch_lock_fpgaB);
+		
+		write32(cpu_to_be32(HCR_REFETCH_H2FB), HCR(ctx));
+		return HOST_DD_Good;
+	}
+	else {
+		uint64_t  hbmsr;
+
+		PDEBUG(1, "Device %d: FETCH FPGA-B\n", ctx->dev_index);
+		hbmsr = cpu_to_be64(read64(HBMSR(ctx)));
+		if (!(hbmsr & HBMSR_H2FB_TCPR)) {
+			PDEBUG(1, "Device %d: card busy at fetch!\n", ctx->dev_index);
+			spin_unlock_bh(&ctx->fetch_lock_fpgaB);
+			return HDD_FAIL;
+		}
+		else {
+			struct dt_t  *pDT;
+			dma_addr_t BE_dma;
+			
+			//PDEBUG(3, "Device %d: Req chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_req, TRUE);
+			//PDEBUG(3, "Device %d: HRA1 chain contents:\n", ctx->dev_index);
+			//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+			
+			pDT = list_entry(pReq->DT_req.next, struct dt_t, list_head);
+			BE_dma = cpu_to_be64(pDT->this_dma);
+
+			pLH = list_remove_tail(&pReq->DT_req);
+			pLastDT = list_entry(pLH, struct dt_t, list_head); 
+			ctx->last_DT_FPGA_B = pLastDT;
+			
+			PDEBUG(2, "Device %d: Fetch DT:  VA: %p, BE_LA: %016llx\n", ctx->dev_index, pDT, BE_dma);
+
+			write64(BE_dma, H2FB_TCP(ctx));
+			write32(cpu_to_be32(HCR_FETCH_H2FB), HCR(ctx));
+			
+			spin_unlock_bh(&ctx->fetch_lock_fpgaB);
+		}
+	}
+
+	return HOST_DD_Good;
+
+}
diff --git drivers/misc/ibm4767/driver.h drivers/misc/ibm4767/driver.h
new file mode 100755
index 000000000000..f7e14978cffa
--- /dev/null
+++ drivers/misc/ibm4767/driver.h
@@ -0,0 +1,1144 @@
+/*************************************************************************
+ *  Filename:driver.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  Main header file.                                          
+ *                                         
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+
+#ifndef _DRIVER_H_
+#define _DRIVER_H_
+
+#include <linux/version.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/kobject.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+
+#include "y_config.h"
+#include "host_err.h"
+
+
+#if defined(TRUE)
+ #undef #TRUE
+#endif
+
+#if defined(FALSE)
+ #undef FALSE
+#endif
+
+#define TRUE  1
+#define FALSE 0
+
+
+// Version/Release/Variant
+//
+// VARIANT is used to distinguish the various sandbox builds from official tree
+//    VARIANT  = 1 --> official library version
+//    VARIANT != 1 --> custom sandbox version
+//
+//    In the case of a sandbox version, BUILD represents the version of the 
+//    official source tree used as the base
+//
+#define DRIVER_VERSION 5
+#define DRIVER_RELEASE 1
+#define DRIVER_VARIANT 1
+#define DRIVER_BUILD   209
+
+#define DEV_NAME             "ibm4767"     // Device name
+#define IBM4767_VENDOR_ID    0x1014        // Vendor identifier
+#define IBM4767_DEVID_4767   0x0466        // Device identifier for ibm4767
+#define IBM4767_DEVID_4768   0x0610        // Device identifier for ibm4768
+
+#if defined(WINDOWS)
+#define write8 (val, addr)  WRITE_REGISTER_UCHAR  ((PUCHAR)(addr),   (val))
+#define write16(val, addr)  WRITE_REGISTER_USHORT ((PUSHORT)(addr),  (val))
+#define write32(val, addr)  WRITE_REGISTER_ULONG  ((PULONG)(addr),   (val))
+#define write64(val, addr)  WRITE_REGISTER_ULONG64((PULONG64)(addr), (val))
+
+#define read8(addr)   READ_REGISTER_UCHAR  ((PUCHAR)(addr))
+#define read16(addr)  READ_REGISTER_USHORT ((PUSHOT)(addr))
+#define read32(addr)  READ_REGISTER_ULONG  ((PULONG)(addr))
+#define read64(addr)  READ_REGISTER_ULONG64((PULONG64)(addr))
+#endif
+
+#if defined(LINUX)
+#define write8(val, addr)   writeb((val), (addr))
+#define write16(val, addr)  writew((val), (addr))
+#define write32(val, addr)  writel((val), (addr))
+
+#if defined(BROKEN_64BIT_READS)
+#define write64(val, addr)  { writel(((val) & 0xFFFFFFFF), (addr)); writel(((val) >> 32), (addr+4)); }
+#else
+#define write64(val, addr)  writeq((val), (addr))
+#endif
+
+#define read8(addr)   readb((addr))
+#define read16(addr)  readw((addr))
+#define read32(addr)  readl((addr))
+
+#if defined(BROKEN_64BIT_READS)
+#define read64(addr) (((uint64_t)readl(addr) & 0xFFFFFFFF) | (((uint64_t)readl(addr+4) & 0xFFFFFFFF) << 32))
+#else
+#define read64(addr)  readq((addr))
+#endif
+
+#endif // LINUX
+
+
+#define MIN(a, b)  (((a) < (b)) ? (a) : (b))
+
+#define ADC2TEMP(t) ((((int)(t)) < 128) ? ((int)(t)) : ((int)(t) - 256))
+#define ADC2MV(v)   (((uint64_t)(v) * 5500) / 256)
+
+
+// Print Messages Definitions
+//
+#define PRINTK(fmt,  args...)  printk(KERN_INFO    DEV_NAME ": " fmt, ##args)
+#define PRINTKW(fmt, args...)  printk(KERN_WARNING DEV_NAME ": " fmt, ##args)
+#define PRINTKE(fmt, args...)  printk(KERN_ERR     DEV_NAME ": " fmt, ##args)
+#define PRINTKC(fmt, args...)  printk(KERN_CRIT    DEV_NAME ": " fmt, ##args)
+
+#ifdef DEBUG
+  #define PDEBUG(lvl, fmt, args...) \
+      { if (dbg_level >= lvl) printk(DEV_NAME ": " fmt, ##args); }
+  #define PDEBUGC(lvl, fmt, args...) \
+      { if (dbg_level >= lvl) printk(": " fmt, ##args); }
+#else
+  #define PDEBUG(lvl, fmt, args...) {}
+  #define PDEBUGC(lvl, fmt, args...) {}
+#endif
+
+#define CMD_MB0_CONTINUE           0x00
+#define CMD_MB0_IBMBURN            0x04
+#define CMD_MB0_RQ_STATUS          0x80
+#define CMD_MB0_RQ_STATUS_NORESET  0x81
+#define CMD_MB0_RQ_S0_HASH         0x84
+
+#define CMD_MB1_IBM_INITIALIZE     0x10
+#define CMD_MB1_RECERTIFY          0x12
+#define CMD_MB1_REMBURN1           0x15
+#define CMD_MB1_ESTOWN2            0x17
+#define CMD_MB1_REMBURN2           0x18
+#define CMD_MB1_EMBURN2            0x19
+#define CMD_MB1_SUROWN2            0x1B
+#define CMD_MB1_ESTOWN3            0x1C
+#define CMD_MB1_REMBURN3           0x1D
+#define CMD_MB1_EMBURN3            0x1E
+#define CMD_MB1_SUROWN3            0x20
+#define CMD_MB1_SELF_TAMPER        0x22
+#define CMD_MB1_S1Q_HEALTH         0x88
+#define CMD_MB1_S1Q_CERTS          0x89
+#define CMD_MB1_S1Q_FIRMWARE       0x8A
+#define CMD_MB1_S1Q_FIPS_TEST      0xA5
+
+// ASIC revisions (DD1.5 cards have the 2.0 ASIC)...
+//
+#define ASIC_DD_10  0x10
+#define ASIC_DD_20  0x20
+
+
+#define CDU_DAEMON_AGENTID  0xFFFF
+
+#define MAX_DEV_COUNT     8             // maximum number of devices supported
+
+#define DEFAULT_HIGH_TEMP_THRESH  83
+#define DEFAULT_LOW_TEMP_THRESH   75
+
+#define TIMEOUT_SSP_MB      60          // timeout to reach MB0 or MB1
+#define TIMEOUT_MINIBOOT    1200 	// timeout value for miniboot commands
+#define TIMEOUT_CDU         600 	// timeout value for CDU operations
+
+// NOTE: the following MCPU timeout windows match the S390 default timeouts.
+// Do not change the default values.  Any change to the default timeouts need
+// to be agreed-upon by all driver platforms.
+#define TIMEOUT_NORMAL      30 		// timeout value for normal MCPU requests
+#define TIMEOUT_NORMAL_SLOW 900 	// timeout value for "slow" MCPU requests
+
+#define MIN_TIMEOUT       1             // absolute minimum time for timeout
+#define MAX_RETRIES_SLOW      30
+#define MAX_RETRIES_LOW_PRI   8
+#define MAX_POST_RETRIES      4
+#define MAX_MB0_RETRIES       4
+
+#define SLOW_OP    1
+#define NORMAL_OP  2
+
+#define POST_EMERGENCY_DUMP_SIZE  4096
+#define TAS_EMERGENCY_DUMP_SIZE   65536
+#define SEG2_EMERGENCY_DUMP_SIZE  163840 
+
+#define SSP_EMERGENCY_DUMP_BUFSIZE   65536
+#define MCPU_EMERGENCY_DUMP_BUFSIZE  196608
+
+// for request size limits, see "Maximum Lengths" discussion (Fence page 24-26)
+//
+#define MAX_REQUEST_SIZE     0xFFC8
+#define MAX_REPLY_CPRB_SIZE  0xFFC8
+#define MAX_REPLY_DATA_SIZE  0xFFD8
+
+#define MAX_FP_REQUEST_SIZE  0xFFFFF
+
+
+#define MAX_INT_STATES       256 
+#define NUM_SAVED_INT_STATES 20
+#define NUM_PRI_WINDOWS      2 
+
+
+// define our request queue size.  to accomodate the FTE guys, manufacturing mode
+// drivers will have a deeper queue than production drivers...
+//
+#if defined(ENABLE_MFG)
+#define MAX_NORM_REQUESTS 64
+#else
+#define MAX_NORM_REQUESTS 32
+#endif
+#define REQ_POOL_COUNT    (MAX_NORM_REQUESTS + 1)
+
+// 8 event observers per card
+#define OBS_POOL_COUNT   8
+
+// pool enough DTs for a 20MB SSP request and MAX_NORMAL_REQUESTS full-sized MCPU requests
+// assume worst case kernel only gives PAGE_SIZE chunks...
+//
+// worst SSP request:  20MB request data, two 64KB HRAs.  special protocol says a 20MB request will have
+// 320 64KB special fragments.  each fragment is a standalone request with its own DMA header.
+#define DT_POOL_COUNT_SSP  (((20*1024*1024)+(2*64*1024))/PAGE_SIZE + 320 + 1)
+//
+// worst MCPU request:  64KB request data, two 64KB HRAs
+#define DT_POOL_COUNT_MCPU (((64*3*1024)/PAGE_SIZE + 1)*MAX_NORM_REQUESTS)
+//
+#define DT_POOL_COUNT (DT_POOL_COUNT_SSP + DT_POOL_COUNT_MCPU)
+
+
+#define HDD_SUCCESS    HOST_DD_Good
+#define HDD_FAIL      -1
+
+
+#define AUDIT_CTX_ALLOC          0x00000001
+#define AUDIT_PCI_ENABLE         0x00000002
+#define AUDIT_MEM_REGION         0x00000004
+#define AUDIT_IO_REMAP           0x00000008
+#define AUDIT_REQUEST_POOL       0x00000010
+#define AUDIT_HTB_ALLOC          0x00000020
+#define AUDIT_DT_INIT            0x00000040
+#define AUDIT_HRA_INIT           0x00000080
+#define AUDIT_CDEV_ADD           0x00000100
+#define AUDIT_IRQ                0x00000200
+#define AUDIT_ENABLE_MSI         0x00000400
+#define AUDIT_REGISTER_DRIVER    0x00000800
+#define AUDIT_REGISTER_IOCTL32   0x00001000
+#define AUDIT_REGISTER_CHRDEV    0x00002000
+#define AUDIT_PROCFS             0x00004000
+
+
+#define FLAG_LOCKED              0x00000001
+#define FLAG_QUIESCE             0x00000002
+#define FLAG_MAIN_RESET_PENDING  0x00000004
+#define FLAG_MAIN_RESET_ACTIVE   0x00000008
+#define FLAG_SSP_RESET_PENDING   0x00000010
+#define FLAG_SSP_WUR_PENDING     0x00000020
+#define FLAG_MANUAL_DISABLED     0x00000040
+#define FLAG_MBFAIL_EXPECTED     0x00000080
+
+
+#define EDUMP_FLAG_BYPASS_DE_SSP     0x01
+#define EDUMP_FLAG_BYPASS_DE_MCPU    0x02
+#define EDUMP_FLAG_MARK_SSP_OFFLINE  0x04
+#define EDUMP_FLAG_MARK_MCPU_OFFLINE 0x08
+
+typedef enum { 
+	MCPU_UNINITIALIZED = 0,    // On bus, nothing else known 
+	MCPU_IN_RESET,             // MCPU placed in RESET/DEBUG by Miniboot
+	MCPU_BOOTING,              // Device is resetting/booting 
+	MCPU_POST2_START,
+	MCPU_POST2_HELLO,
+	MCPU_POST2_END,
+#if defined(ENABLE_MFG)
+	MCPU_POST2_SERIAL_DBG,
+	MCPU_IBM_TAS_START,
+	MCPU_IBM_TAS_ACTIVE,
+	MCPU_IBM_TAPP_START,
+	MCPU_IBM_TAPP_ACTIVE,
+#endif
+	MCPU_INITIALIZED,      // GOOD MORNING received from Comm Mgr
+	MCPU_ACTIVE,           // Seg2/Seg3 ready for 
+	MCPU_OFFLINE,          // Device unusable
+	MCPU_TAMPER,           // Device tampered
+	MCPU_SOFT_TAMPER,      // Device soft-tampered (recoverable)
+	MCPU_TEMP_SHUTDOWN     // Device shutdown due to temperature
+} MCPUSTAT;
+
+
+typedef enum {
+	SSP_UNINITIALIZED = 0,
+	SSP_WAIT_FOR_SSPWUR,
+	SSP_WAIT_FOR_MBHALT,
+	SSP_WAIT_FOR_POST0,
+	SSP_WAIT_FOR_POST0_END,
+	SSP_WAIT_FOR_MB0_START,
+	SSP_WAIT_FOR_MB0_HELLO,
+	SSP_MB0,
+	SSP_WAIT_FOR_MB0_END,
+	SSP_WAIT_FOR_POST1_START,
+	SSP_WAIT_FOR_POST1_HELLO,
+	SSP_POST1_HELLO,
+	SSP_WAIT_FOR_POST1_END,
+#if defined(ENABLE_MFG)
+	SSP_POST1_SERIAL_DBG,
+	SSP_IBM_TAS_START,
+	SSP_IBM_TAS_ACTIVE,
+	SSP_IBM_TAPP_START,
+	SSP_IBM_TAPP_ACTIVE,
+#endif
+	SSP_WAIT_FOR_MB1_START,
+	SSP_WAIT_FOR_MB1_HELLO,
+	SSP_MB1,
+	SSP_WAIT_FOR_MB1_END,
+	SSP_MB_DONE,
+	SSP_OFFLINE,
+	SSP_TAMPER,
+	SSP_SOFT_TAMPER,
+	SSP_TEMP_SHUTDOWN
+} SSPSTAT;
+
+typedef enum {
+	CDU_INACTIVE = 0,
+	CDU_WAIT_FOR_AGENTID_LIST,
+	CDU_REMBURN3_NOCDU_NEED_USER_RESET,
+	CDU_REMBURN3_NOCDU_AUTORESET,
+	CDU_REMBURN3_CDU,
+	CDU_WAIT_FOR_READY_TO_PROCEED,
+	CDU_WAIT_FOR_PROCEED_ACK
+} CDUSTAT;
+
+typedef enum {
+	FP_OFFLINE,
+	FP_READY
+} FPSTAT;
+
+typedef enum {
+	TAMPER_NONE,
+	TAMPER_PERM,
+	TAMPER_UNKNOWN,
+	TAMPER_SOFT_TEMP,
+	TAMPER_SOFT_VOLT,
+	TAMPER_SOFT_INJ,
+	TAMPER_SOFT_OTHER
+} TAMPERSTAT;
+
+typedef enum {
+	NO_LOCKINGNEEDED = 0,
+	LOCKING_NEEDED   = 1
+} LOCKING;
+
+
+// ibm4767's user API doesn't support asym requests but the card can still generate
+// log messages which are delivered to the driver.
+//
+// we arbitrarily allocate 8 buffers for these asym log events.  
+//
+#define ASYM_HRA_POOL_SIZE 8 
+#define ASYM_HRA_SIZE (64*1024)
+
+
+//
+// DMA HEADERS
+//
+// see the following FS sections 
+//    6.2.2 - generic 8-byte DMA header
+//    6.3.2 - 16-byte HRB header 
+//    6.4.2 - 16-byte SRB header
+//    6.5.2 - Generic DMA request header/block (SKCH fastpath, etc)
+//
+
+// DMA channels (see AIB shim layer section 3.2)
+//
+#define DMA_CH_RD_MM   0x0
+#define DMA_CH_RD_SKA  0x1
+#define DMA_CH_RD_SKB  0x2
+#define DMA_CH_RD_MCPU 0x3
+#define DMA_CH_RD_SSP  0x4
+#define DMA_CH_RD_FA   0x5
+#define DMA_CH_RD_FB   0x6
+#define DMA_CH_RD_RSVD 0x7
+#define DMA_CH_WR_MM   0x8
+#define DMA_CH_WR_SKA  0x9
+#define DMA_CH_WR_SKB  0xA
+#define DMA_CH_WR_MCPU 0xB
+#define DMA_CH_WR_SSP  0xC
+#define DMA_CH_WR_FA   0xD
+#define DMA_CH_WR_FB   0xE
+#define DMA_CH_WR_RSVD 0xF
+
+
+// see table in section 6.2.4 (p245) "Header Type and Sign" for the following H_Type 
+// values.
+//
+// note: the table indicates that M2H IV has htype=0x42 while the diagram in 
+// section 6.3.6.1 (p253) "MRM Interrupt Vector" shows H_Type=0x52.  the reason for 
+// the difference is these guys are referring to different IVs.  The M2H IV on p245 
+// is sent to the MCPU when the MRM has been sent to the host.  The M2H IV on p254
+// is sent to the host's HTB...
+//
+// header byte 0
+#define DMA_HTYPE_H2PKA   0x00
+#define DMA_HTYPE_PKA2H   0x10
+#define DMA_HTYPE_PKB2H   0x18
+#define DMA_HTYPE_H2SKA   0x20
+#define DMA_HTYPE_H2SKB   0x28
+#define DMA_HTYPE_SKA2H   0x30
+#define DMA_HTYPE_SKB2H   0x38
+#define DMA_HTYPE_H2M     0x40  // see note above...
+#define DMA_HTYPE_M2H     0x50  // see note above...
+#define DMA_HTYPE_HTB     0x70
+#define DMA_HTYPE_H2S     0x80
+#define DMA_HTYPE_S2H     0x90
+#define DMA_HTYPE_H2FA    0xE0
+#define DMA_HTYPE_H2FB    0xE8
+#define DMA_HTYPE_FA2H    0xF0
+#define DMA_HTYPE_FB2H    0xF8
+
+#define DMA_HTYPE_IV      0x02  // bit 6 (ignore page 77 where is says bit 5)
+
+#define IV_HTYPE_PKA2H    (DMA_HTYPE_PKA2H | DMA_HTYPE_IV)
+#define IV_HTYPE_PKB2H    (DMA_HTYPE_PKB2H | DMA_HTYPE_IV)
+#define IV_HTYPE_H2SKA    (DMA_HTYPE_H2SKA | DMA_HTYPE_IV)
+#define IV_HTYPE_H2SKB    (DMA_HTYPE_H2SKB | DMA_HTYPE_IV)
+#define IV_HTYPE_SKA2H    (DMA_HTYPE_SKA2H | DMA_HTYPE_IV)
+#define IV_HTYPE_SKB2H    (DMA_HTYPE_SKB2H | DMA_HTYPE_IV)
+#define IV_HTYPE_H2M      (DMA_HTYPE_H2M   | DMA_HTYPE_IV)
+#define IV_HTYPE_M2H      (DMA_HTYPE_M2H   | DMA_HTYPE_IV)
+#define IV_HTYPE_H2S      (DMA_HTYPE_H2S   | DMA_HTYPE_IV)
+#define IV_HTYPE_S2H      (DMA_HTYPE_S2H   | DMA_HTYPE_IV)
+#define IV_HTYPE_FA2H     (DMA_HTYPE_FA2H  | DMA_HTYPE_IV)
+#define IV_HTYPE_FB2H     (DMA_HTYPE_FB2H  | DMA_HTYPE_IV)
+
+// header byte 1
+#define DMA_RQSTR_HOST     0x10
+#define DMA_CW_NORM        0x00
+#define DMA_CW_EXT         0x01
+#define DMA_CW_ABORT       0x02
+
+// header byte 2
+#define DMA_CW_NO_TX       0x80
+#define DMA_CW_TX          0x00
+#define DMA_CW_MRB0        0x00
+#define DMA_CW_MRB1        0x04
+#define DMA_CW_NOINT_NOIV  0x00
+#define DMA_CW_NOINT_IV    0x01
+#define DMA_CW_INT_NOIV    0x02
+#define DMA_CW_INT_IV      0x03
+
+#pragma pack(1)
+typedef struct {
+	uint8_t  bytes[8];
+} dma_header8_t;
+
+
+typedef struct {
+	dma_header8_t hdr8;
+#define DMA_SIG_H2PKA  0x1234
+#define DMA_SIG_H2SKCH 0xA5A5
+#define DMA_SIG_H2FPGA 0xE2F2
+#define DMA_SIG_H2M    0x1234
+#define DMA_SIG_H2S    0x5678
+	uint16_t    sig;
+	uint8_t     HDF[6];
+} dma_header16_t;
+
+
+#define DMA_MAX_REQLEN      (0xFFFFF - 16)
+
+// For fast-path operations, see the following
+//    6.5.2   - General DMA request header/block (SKCH fast path, etc)
+//   12.1.2.3 - SKCH payload block
+//   12.1.2.5 - SKCH opcodes
+//   12.1.2.8 - SKCH I/O Descriptor layout
+//   12.4     - SKCH req/repl blocks for various operations
+//
+// see...
+//    8.2 - PKA  request block
+//   10.2 - SKCH request block
+typedef struct {
+	dma_header8_t  hdr8;
+	uint16_t     sig;
+	uint16_t     reserved;
+	uint32_t     hdf;
+	uint64_t     request_id;
+	uint64_t     dra;
+} fp_request_header_t;
+
+// see...
+//    8.3 - PKA  reply block
+//   10.3 - SKCH reply block
+typedef struct {
+	dma_header8_t  hdr8;
+	uint64_t     dra;
+	uint64_t     request_id;
+} fp_reply_header_t;
+
+
+
+typedef struct {
+	uint8_t  tag;
+	uint8_t  len[3];
+	uint32_t reserved;
+	uint64_t hra;
+} tag_len_hra_t;
+
+// the following structures are for communication with Seg2/Seg3 apps
+//
+// HRB headers will consist of
+//    hrb_type1_header_t or hrb_type3_header_t
+//    0-7 bytes of padding
+//    hrb_header_part2_t
+//
+// see Fence section 6.0 figures 1 and 2 for headers with 2 HRAs
+typedef struct { 
+	dma_header16_t  dma_hdr;
+	uint16_t      len2; 
+	uint16_t      len3; 
+	uint16_t      num_hras;
+	uint8_t       ep_len;
+	uint8_t       flags;
+#define HRB_FLAGS_NORMAL   0x00
+#define HRB_FLAGS_HRA_POOL 0x01
+	tag_len_hra_t   tag_len_hra[2];
+	uint16_t      agent_id; 
+	// 0-7 bytes of padding appended here
+} hrb_type1_header_t;
+
+// see Fence section 6.0 figures 3 and 4 for headers with 1 HRA
+typedef struct { 
+	dma_header16_t  dma_hdr;
+	uint16_t      len2; 
+	uint16_t      len3; 
+	uint16_t      num_hras;
+	uint8_t       ep_len;
+	uint8_t       flags;
+	tag_len_hra_t   tag_len_hra;
+	uint16_t      agent_id; 
+	// 0-7 bytes of padding appended here
+} hrb_type3_header_t;
+
+typedef struct {
+	uint8_t     frag_idx;
+	uint8_t     frag_total;
+	uint16_t    request_id; 
+	uint32_t    user_def; 
+	uint32_t    status; 
+	// request control block appended here
+} hrb_header_part2_t;
+
+
+// MRM headers: see Fence section 7.0
+//
+// type 1 MRM header is the Reply Control Block
+typedef struct {
+	uint64_t    header[3];
+	uint16_t    len2;
+	uint8_t     reserved[3];
+	uint8_t     flags;
+#define MRM_FLAGS_NORMAL  0x00
+#define MRM_FLAGS_HRA_ACK 0x01
+	uint16_t    request_id;
+	uint32_t    user_def;
+	uint32_t    status;
+} mrm_type1_header_t;
+
+
+// type 2 MRM header is the Reply Data Block
+typedef struct {
+	uint64_t    header[3];
+} mrm_type2_header_t;
+
+
+typedef struct { 
+	uint16_t  length; 
+	uint16_t  type; 
+	uint16_t  version; 
+	uint16_t  facility; 
+	uint16_t  severity;
+} asym_msg_t;
+#define TYPE_ERRLOG     0xFFFF
+
+typedef struct {
+	uint64_t  header[3];
+	asym_msg_t  log_header;
+} mrm_event_log_header_t;
+
+
+// HRA load packet errors - returned in 'status'
+//
+#define HPA_NO_MASK     0x00000004
+#define HPA_LEN_BAD     0x00000005 
+#define HPA_NO_NOHRAS   0x00000006
+#define HPA_MASKBITS    0x00000007
+#define HPA_ILL_TIMED   0x00000008
+
+
+// the following structures are for special communications (miniboot, POST, FTE, etc)
+// this is superficially similar to the hrb_type1_header_t with some subtle changes...
+//
+// note: 
+typedef struct {
+	dma_header16_t  dma_hdr;
+	uint16_t      len2; 
+	uint16_t      len3; 
+	uint16_t      num_hras;
+	uint16_t      reserved1;
+	tag_len_hra_t tag_len_hra[2];
+	uint16_t      agent_id;
+	uint16_t      reserved2;
+	uint16_t      frag_total;
+	uint16_t      req_id;
+	uint32_t      user_def;
+	uint32_t      status;
+	uint32_t      msg_len;
+#define XC_MIN_FRAG_LEN (64*1024)
+	uint32_t      frag_len;
+	uint64_t      reserved3;
+} pilot_msg_t;
+
+typedef struct {
+	uint64_t header[3];
+	uint32_t len;
+	uint32_t status;
+	uint16_t req_id;
+	uint8_t  padding[6];
+} special_repl_header_t;
+
+typedef struct {
+	uint64_t update_vector;
+	uint64_t hra_table[ASYM_HRA_POOL_SIZE];
+} hra_pool_packet_t;
+#pragma pack()
+
+
+struct virt_phys {
+#if defined(LINUX)
+	dma_addr_t dma;
+#endif
+#if defined(WINDOWS)
+	WDFCOMMONBUFFER  cb;
+	PHYSICAL_ADDRESS dma;
+#endif
+	void *      va; 
+	uint32_t    len;
+};
+
+
+typedef struct {
+	struct virt_phys vp;
+#if defined(LINUX)
+	struct list_head list_head;
+	int              order;
+	uint32_t         len;
+#endif
+#if defined(WINDOWS)
+	LIST_ENTRY       list_entry;
+#endif
+} scatter_frag_t;
+
+
+typedef struct {
+	struct _ibm4767_ctx_t  *ctx;
+	uint32_t                num;
+#if defined(LINUX)
+	struct list_head  frags;
+	int               direction;
+#endif
+#if defined(WINDOWS)
+	LIST_ENTRY  frags;
+#endif
+} y_scatterlist_t;
+
+
+// DTs cannot straddle a 4K boundary so force alignment accordingly...
+// fields named BE_* are stored as big-endian
+//
+// alignment: Andretta requires 64bit alignment.  DT cannot cross a 512B boundary
+//
+// from FS section 3.5.2: // 'fields' looks like:
+//    [ 0: 3] ctrl bits      ( 4 bits)  ( << 60)
+//    [ 4:23] byte count     (20 bits)  ( << 40)
+//    [24:27] reserved       ( 4 bits)  ( << 36)
+//    [28:47] HRB len        (20 bits)  ( << 16)
+//    [48:63] sign (0xD64D)  (16 bits)  ( <<  0)
+// can't use bitfields for these for portability reasons
+//
+struct dt_t {
+	// the first 6 items are required by the hardware...do not change the order!
+	//
+	dma_addr_t  BE_data_dma;  // dma addr of buffer this DT points to
+	uint64_t    fields;       // see above diagram
+	dma_addr_t  BE_next_dma;  // dma addr of next DT in the chain
+
+	// the following items are for our bookkeeping...
+	//
+	dma_addr_t  this_dma;     // dma addr of the DT itself
+	void       *data_va; 
+
+	struct list_head list_head;
+
+	// the only DTs that we allocate that aren't associated with a request are
+	// used by the HRA pool.  so pReq is almost always non-NULL.
+	void *pReq;
+} __attribute__ ((__aligned__(64), __packed__));
+
+struct dt_pool_page {
+	dma_addr_t  dma;
+	void       *va;
+};
+
+
+#define  DT_SIGNATURE       0xD64D
+
+//
+// the following are pre-adjusted for endianness to reduce the 
+// amount of byte-swapping that we have to do...
+//
+#if defined(BIG_ENDIAN)
+#define  DT_CTRL_ENDOFCHAIN 0x8000000000000000LL
+// reserved                 0x4000000000000000LL
+#define  DT_CTRL_NOTIFYRX   0x2000000000000000LL
+#define  DT_CTRL_RMRI       0x1000000000000000LL
+#define  DT_CTRL_MASK       0xF000000000000000LL
+#define  DT_BYTE_COUNT_MASK 0x0FFFFF0000000000LL
+// reserved                 0x000000F000000000LL
+#define  DT_HRB_LEN_MASK    0x0000000FFFFF0000LL
+#define  DT_SIGN_MASK       0x000000000000FFFFLL
+#else
+#define  DT_CTRL_ENDOFCHAIN 0x0000000000000080LL
+// reserved                 0x0000000000000040LL
+#define  DT_CTRL_NOTIFYRX   0x0000000000000020LL
+#define  DT_CTRL_RMRI       0x0000000000000010LL
+#define  DT_CTRL_MASK       0x00000000000000F0LL
+#define  DT_BYTE_COUNT_MASK 0x0000000000FFFF0FLL
+// reserved                 0x00000000F0000000LL
+#define  DT_HRB_LEN_MASK    0x0000FFFF0F000000LL
+#define  DT_SIGN_MASK       0xFFFF000000000000LL
+#endif
+
+//
+// end of endian-pre-adjusted constants
+//
+
+// number of HTB entries needed: 
+//    if request tracking disabled: (2 * REQ_POOL_COUNT) + (ASYM_HRA_POOL_SIZE)
+//    if request tracking  enabled: (3 * REQ_POOL_COUNT) + (ASYM_HRA_POOL_SIZE) 
+//
+//    256 entries gives us nice page-boundaries...
+//
+#define HTB_ENTRIES      ((3*REQ_POOL_COUNT + ASYM_HRA_POOL_SIZE + 256) & 0xFFFFFF00)
+//#define HTB_ENTRIES      (MIN(256, (3*REQ_POOL_COUNT + ASYM_HRA_POOL_SIZE)))
+#define HTB_VECTOR_SIZE  16
+#define HTB_SIZE         (HTB_ENTRIES * HTB_VECTOR_SIZE)
+
+
+#pragma pack(1)
+// see FS 6.2.3 for IV structure description
+typedef struct {
+	uint8_t   type;
+	uint16_t  ctrl;  
+	uint8_t   UF;
+	uint8_t   VF;
+	uint8_t   rc;  // bits 0-3: reserved, 4-7: rc
+	uint8_t   reserved;
+	uint8_t   len;
+	uint64_t  msg;
+} htb_vector_t;
+#pragma pack()
+
+typedef struct {
+	uint32_t	hisr;
+	uint64_t        htb_start;
+	uint64_t        htb_end;
+	uint32_t        ssp_mbx_hi;
+	uint32_t        ssp_mbx_lo;
+	uint32_t        mcpu_mbx_hi;
+	uint32_t        mcpu_mbx_lo;
+} interrupt_state_t;
+
+#define REQUEST_INACTIVE 0
+#define REQUEST_MCPU     1
+#define REQUEST_SSP      2
+#define REQUEST_HRA      3
+#define REQUEST_OTHER    4
+#define REQUEST_FP       5
+
+#define REQ_STATUS_HRA1_RECD  0x01
+#define REQ_STATUS_HRA2_RECD  0x02
+#define REQ_STATUS_XFERRED    0x04
+
+
+typedef struct {
+	struct list_head list_head;
+	struct list_head DT_req;
+	struct list_head DT_hra1;
+	struct list_head DT_hra2;
+
+	y_scatterlist_t hdr_slist;
+	y_scatterlist_t req_slist;
+	y_scatterlist_t req_data_slist;
+	y_scatterlist_t hra1_slist;
+	y_scatterlist_t hra2_slist;
+
+	char         *header;
+	atomic_t      num_hras;
+	uint32_t      timeout;
+	uint8_t       status;  
+
+	wait_queue_head_t  waitQ;
+	atomic_t           waitEvent;
+
+	volatile char      active;
+	volatile char      reqd_state;
+	uint32_t           retcode;
+	uint16_t           request_id;
+} request_t;
+
+
+typedef struct {
+	struct list_head   list_head;
+	wait_queue_head_t  waitQ;
+	atomic_t           waitEvent;
+
+	uint32_t           rc;
+	int                type;
+	request_t         *pReq;
+} request_wait_t;
+
+#define OBSERVER_INACTIVE 0
+#define OBSERVER_INUSE    1
+
+typedef struct {
+	wait_queue_head_t  waitQ;
+	atomic_t           waitEvent;
+	volatile char      active;
+	uint32_t           event_code;
+} observer_t;
+
+typedef struct {
+	atomic_t          req_buf_update;
+	y_scatterlist_t   slist[ASYM_HRA_POOL_SIZE];
+	struct list_head  DTs[ASYM_HRA_POOL_SIZE];
+
+	spinlock_t        mask_lock;
+	uint64_t          hra_update_vector;
+} asym_attr_t;
+
+typedef struct {
+	unsigned long     timestamp;
+	struct list_head  list_head;
+} sev0_entry_t;
+
+
+#define CTX_TYPE_PF   1
+#define CTX_TYPE_VF   2
+
+#define MAX_MSIX 16
+
+#define ASIC_REV_DD1  0x10
+#define ASIC_REV_DD2  0x20
+
+
+typedef struct _ibm4767_ctx_t {
+	uint8_t                 ctx_type;
+	struct _ibm4767_ctx_t  *pf;  // only valid for CTX_TYPE_VF
+
+	// hardware resources
+	struct device  *  dev;
+	struct pci_dev *  pci_dev;
+	struct pci_dev *  bridge_dev;
+	unsigned char  *  io_va;
+	struct cdev       cdev;
+	int               dev_index;
+
+	uint8_t           msix_enabled;
+	struct msix_entry msix_entries[MAX_MSIX];
+	uint8_t           msi_enabled;
+	uint8_t           vfid;
+
+	// VPD and hardware/firmware info
+	xcHWInfo_t  hwinfo;
+
+	struct tasklet_struct interrupt_dpc;
+
+	struct work_struct  hra_pool_work;
+	struct work_struct  seg2_edump_work;
+	struct work_struct  post0_edump_work;
+	struct work_struct  post2_edump_work;
+#if defined(ENABLE_MFG)
+	struct work_struct  ssp_tas_edump_work;
+	struct work_struct  mcpu_tas_edump_work;
+#endif
+	struct work_struct  main_reset_work;
+
+#define WORK_QUIESCE_HRA_POOL        0x00000001L
+#define WORK_QUIESCE_SEG2_EDUMP      0x00000002L
+#define WORK_QUIESCE_POST0_EDUMP     0x00000004L
+#define WORK_QUIESCE_POST2_EDUMP     0x00000008L
+#define WORK_QUIESCE_SSP_TAS_EDUMP   0x00000010L
+#define WORK_QUIESCE_MCPU_TAS_EDUMP  0x00000020L
+#define WORK_QUIESCE_MAIN_RESET      0x00000040L
+#define WORK_QUIESCE_ALL             0xFFFFFFFFL
+	uint32_t work_quiesce;
+	spinlock_t  work_quiesce_lock;
+
+	// timers
+	struct timer_list deferred_buf_update_timer;
+	struct timer_list deferred_forced_edump_timer;
+	struct timer_list deferred_main_reset_timer;
+	struct timer_list deferred_ssp_wakeup_timer;
+	struct timer_list cdu_timeout_timer;
+	struct timer_list agentid_list_timer;
+	struct timer_list ssp_timer;
+	struct timer_list temp_throttle_timer;
+	struct timer_list seg3_started_timer;
+
+// these definitions are timers that reschedule themselves
+// or schedule other timers...
+#define TIMER_QUIESCE_DEFERRED_BUF_UPDATE    0x00000001L
+#define TIMER_QUIESCE_DEFERRED_FORCED_EDUMP  0x00000002L
+#define TIMER_QUIESCE_DEFERRED_MAIN_RESET    0x00000004L
+#define TIMER_QUIESCE_DEFERRED_SSP_WUR       0x00000008L
+#define TIMER_QUIESCE_CDU                    0x00000010L
+#define TIMER_QUIESCE_TEMP_THROTTLE          0x00000020L
+#define TIMER_QUIESCE_SEG3_STARTED           0x00000040L
+#define TIMER_QUIESCE_ALL                    0xFFFFFFFFL
+	uint32_t    timer_quiesce;
+	spinlock_t  timer_quiesce_lock;
+
+	// DT pool...
+	spinlock_t           dt_pool_lock;
+	struct dt_pool_page *dt_pool_pages;
+	int                  dt_pool_page_count;
+	struct list_head     dt_pool;
+
+	// HTB stuff...
+	struct virt_phys htb[2];
+	uint32_t         htb_bytes;
+	uint8_t          htb_active;
+	dma_addr_t       htb_next_dma;
+	dma_addr_t       htb_last_wa;
+
+	// request pool
+	spinlock_t       req_pool_lock;
+	request_t        req_array[REQ_POOL_COUNT];
+	struct list_head req_pool;
+	struct list_head req_wait_list;
+	request_t       *hra_req;
+	request_t       *mb_req;
+#if defined(ENABLE_MFG)
+	request_t       *p1_req;
+	request_t       *p2_req;
+	request_t       *mcpu_tas_req;
+	request_t       *ssp_tas_req;
+	request_t       *mcpu_tapp_req;
+	request_t       *ssp_tapp_req;
+#endif
+
+	// event observers
+	spinlock_t      observe_pool_lock;
+	observer_t      observe_pool[OBS_POOL_COUNT];
+
+	// state machine values
+	MCPUSTAT        status_mcpu;
+	SSPSTAT         status_ssp;
+	CDUSTAT         status_cdu;
+	FPSTAT          status_fp;
+	TAMPERSTAT      status_tamper;
+
+	// status_flags is protected by 'counter_lock' below
+	uint32_t        status_flags;
+
+	// internal bookkeeping
+	spinlock_t      fetch_lock_mcpu;
+	spinlock_t      fetch_lock_ssp;
+	spinlock_t      fetch_lock_skch;
+	spinlock_t      fetch_lock_pka;
+	spinlock_t      fetch_lock_fpgaA;
+	spinlock_t      fetch_lock_fpgaB;
+	struct dt_t    *last_DT_MCPU;
+	struct dt_t    *last_DT_SSP;
+	struct dt_t    *last_DT_SKCH;
+	struct dt_t    *last_DT_PKA;
+	struct dt_t    *last_DT_FPGA_A;
+	struct dt_t    *last_DT_FPGA_B;
+
+	// used for special requests
+	uint32_t        reqd_ssp_state;
+	uint32_t        reqd_mcpu_state;
+
+	uint32_t        dma_mask;
+
+	interrupt_state_t int_states[MAX_INT_STATES];
+	atomic_t   int_state_rd_next;
+	atomic_t   int_state_wr_next;
+
+	// reading the following registers has side-effects so user-invoked
+	// register dump cannot read them on-the-fly unless we're post-mortem.
+	// so we save their last-known values here..
+	//
+	uint32_t  last_hisr;       // last-known value of HISR
+	uint32_t  last_s2h_l;      // last-known mailboxes...
+	uint32_t  last_s2h_h;
+	uint32_t  last_m2h_l;
+	uint32_t  last_m2h_h;
+
+	uint16_t  last_mbx_agentid_ssp;
+
+	// the following counters show total # requests for each type
+	atomic_t  ebusy_counter;
+	atomic_t  mcpu_counter;
+	atomic_t  mrb0_counter, mrb1_counter;
+	atomic_t  ssp_counter;
+	atomic_t  fp_counter;
+	atomic_t  ioctl_counter;
+
+	// this counter reflects the number of "REQUEST failed.  Main reset pending/active"
+	// messages we've spat out since the last reset completed.  we set a limit of
+	// 5 messages so as to not clutter the logs...
+#define REQFAIL_MSG_COUNT  5
+	int       ebusy_resetmsg_count;
+	int       reqfail_msg_count;
+
+	// the following counters show # active requests for each type
+	// since decisions are made based on their values, we need it to
+	// be accurate so grab the spinlock when updating
+	spinlock_t counter_lock;
+	int active_mb;
+	int active_p1;
+	int active_p2;
+	int active_ssp_tas;
+	int active_ssp_tapp;
+	int active_mcpu_tas;
+	int active_mcpu_tapp;
+	int active_norm;
+	int active_fp;
+	int active_opens;
+
+	spinlock_t timeout_lock;
+	uint32_t  timeout_sum;
+	uint32_t  timeout_sum_skch;
+	uint32_t  timeout_sum_pka;
+	uint32_t  timeout_sum_fpgaA;
+	uint32_t  timeout_sum_fpgaB;
+
+	// these have to be the same datatype as 'jiffies'
+	// sev0_counter is protected by the 'counter_lock' spinlock
+	struct list_head  sev0_list;
+
+	unsigned long lastlog_timestamp;
+	
+	int       mb_round;
+
+	uint32_t  throttle_lvl;
+
+	// access to indirect registers must be serialized...
+	spinlock_t indirect_lock;
+
+
+	spinlock_t edump_lock;
+	uint8_t    edump_flag;
+	char  *edump_mcpu;
+	char  *edump_ssp;
+
+#if defined(ENABLE_MFG)
+	char  *edump_ssp_tas;
+	// TAS MCPU edump can the edump_mcpu buffer 
+
+	// the following are for TAS/TAPP mailbox monitoring
+	// _LEN must be a power of 2
+#define MAILBOX_HISTORY_LEN  1024
+	spinlock_t ssp_mbxhist_lock;
+	uint64_t  *ssp_mbxhist_list;
+	uint16_t   ssp_mbxhist_r_idx;
+	uint16_t   ssp_mbxhist_w_idx;
+
+	spinlock_t mcpu_mbxhist_lock;
+	uint64_t  *mcpu_mbxhist_list;
+	uint16_t   mcpu_mbxhist_r_idx;
+	uint16_t   mcpu_mbxhist_w_idx;
+#endif
+
+	asym_attr_t  asym_attr;
+	uint8_t  analyze_logs;
+
+} ibm4767_ctx_t;
+
+
+struct priv_data_t {
+	ibm4767_ctx_t *ctx;
+	uint8_t        is_open;
+	uint8_t        need_ssp_reset;
+	uint8_t        req_sent;
+};
+
+
+
+// module parameters that are used throughout the driver
+extern int         allow_indirect;
+extern int         auto_reset;
+extern int         dbg_level;
+extern int         enable_ecrc;
+extern uint32_t    enable_fuzzer;
+extern int         ignore_tamper;
+extern int         num_msix;
+extern int         pcie_link;
+extern int         post1_jump;
+extern int         postde_echo;
+extern int         promote_ssp_reset;
+extern int         reset_on_panic;
+extern int         sev0_ignore;
+extern uint32_t    sev0_limit;
+extern uint32_t    sev0_timeframe;
+extern uint32_t    timeout_cdu;
+extern uint32_t    timeout_fpgaA;
+extern uint32_t    timeout_fpgaB;
+extern uint32_t    timeout_mb;
+extern uint32_t    timeout_mcpu[NUM_PRI_WINDOWS];
+extern uint32_t    timeout_pka;
+extern uint32_t    timeout_skch;
+extern uint32_t    timeout_ssp;
+extern int         use_msix;
+extern int         use_msi;
+// end module parameters
+
+// ibm4767_lock gates access to ibm4767_list and ibm4767_count
+extern rwlock_t       ibm4767_lock;
+extern ibm4767_ctx_t *ibm4767_list[MAX_DEV_COUNT];
+extern uint8_t        tasklet_shutdown[MAX_DEV_COUNT];
+extern int            ibm4767_count;
+
+extern atomic_t     temp_soft_tamper_detected;
+
+#endif
diff --git drivers/misc/ibm4767/dt.c drivers/misc/ibm4767/dt.c
new file mode 100755
index 000000000000..b435631a0f85
--- /dev/null
+++ drivers/misc/ibm4767/dt.c
@@ -0,0 +1,459 @@
+/*************************************************************************
+ *  Filename:dt.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Visegrady, Tamas  IBM Poughkeepsie  <tamas@us.ibm.com>
+ *           Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/vmalloc.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+int
+dt_initialize(ibm4767_ctx_t *ctx)
+{
+	struct dt_t *pDT = NULL;
+	dma_addr_t   dma;
+	void        *va;
+	int          i, j, pages, dts_per_page;
+
+	PDEBUG(1, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	// our DT pool is ~550KB.  might not be able to allocate a chunk of coherent memory that large
+	// so we break it into PAGE_SIZE chunks.
+	//
+	dts_per_page = (PAGE_SIZE) / sizeof(struct dt_t);
+	pages        = (DT_POOL_COUNT / dts_per_page) + 1;
+
+	ctx->dt_pool_pages = (struct dt_pool_page *)vmalloc(pages * sizeof(struct dt_pool_page));
+	if (!ctx->dt_pool_pages) {
+		PRINTKW("Device %d: cannot allocate dt_pool_pages\n", ctx->dev_index);
+		return HOST_DD_NoMemory;
+	}
+	
+	for (i=0; i < pages; i++) {
+		va = dma_alloc_coherent(ctx->dev, PAGE_SIZE, &dma, GFP_ATOMIC);
+		if (!va) {
+			PRINTKW("Device %d: dma_alloc_coherent failed for DT pool idx %d\n", ctx->dev_index, i);
+			return HOST_DD_NoMemory;
+		}
+		ctx->dt_pool_pages[i].va  = va;
+		ctx->dt_pool_pages[i].dma = dma;
+		memset(va, 0x0, PAGE_SIZE);
+	}
+
+	INIT_LIST_HEAD(&ctx->dt_pool);
+
+	for (i=0; i < pages; i++) {
+		pDT = (struct dt_t *)ctx->dt_pool_pages[i].va;
+		dma = ctx->dt_pool_pages[i].dma;
+	
+		for (j=0; j < dts_per_page; j++) {
+			INIT_LIST_HEAD(&pDT->list_head);
+			pDT->this_dma = dma;
+
+			list_add(&pDT->list_head, &ctx->dt_pool);
+
+			pDT++;
+			dma += sizeof(struct dt_t);
+		}
+	}
+
+	ctx->dt_pool_page_count = pages;
+	PDEBUG(1, "Device %d: dt_pool_page_count: %d\n", ctx->dev_index, ctx->dt_pool_page_count);
+
+#if 0
+	PDEBUG(1, "Device %d: DT pool after initialization:\n", ctx->dev_index);
+	dump_chain(ctx, &ctx->dt_pool, FALSE);
+#endif
+
+	spin_lock_init(&ctx->dt_pool_lock);
+
+	return HOST_DD_Good;
+}
+
+
+void
+dt_unload(ibm4767_ctx_t *ctx)
+{ 
+	int i;
+	
+	PDEBUG(1, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	for (i=0; i < ctx->dt_pool_page_count; i++) {
+		if (ctx->dt_pool_pages[i].va)
+			dma_free_coherent(ctx->dev, PAGE_SIZE, ctx->dt_pool_pages[i].va, ctx->dt_pool_pages[i].dma);
+	}
+
+	if (ctx->dt_pool_pages)
+		vfree(ctx->dt_pool_pages);
+
+	ctx->dt_pool_pages = NULL;
+}
+
+
+// append 'list2' onto 'list1'.  list_splice can accomplish something similar
+// on Linux but still need to adjust the 'cb' and 'next_dma' fields
+//
+void
+dt_merge(struct list_head  *list1, 
+	 struct list_head  *list2)
+{ 
+	struct list_head *last1;
+	struct list_head *last2, *next2;
+
+	struct dt_t  *last_dt1, *last_dt2;
+	struct dt_t  *first_dt2;
+
+	if (!list1 || !list2)
+		return;
+
+	if (list_empty(list2))
+		return;
+
+	last1 = list1->prev;
+	last2 = list2->prev;
+	next2 = list2->next;
+
+	last_dt1  = list_entry(last1, struct dt_t, list_head);
+	last_dt2  = list_entry(last2, struct dt_t, list_head);
+	first_dt2 = list_entry(next2, struct dt_t, list_head);
+
+	// even though this routine is (currently) only called when returning
+	// a chain to the unused pool, we maintain a fully-formed chain in case
+	// future code tries to reuse this routine for something clever...
+	//
+	last_dt1->fields &= ~DT_CTRL_ENDOFCHAIN;
+	last_dt1->BE_next_dma = cpu_to_be64(first_dt2->this_dma);
+
+	last_dt2->fields |= DT_CTRL_ENDOFCHAIN;
+
+	list1->prev  = last2;
+	last2->next  = list1;
+
+	last1->next  = next2;
+	next2->prev  = last1;
+}
+
+
+// append a DT onto an existing list
+//
+int
+dt_append(struct list_head *list,
+	  struct dt_t      *pDT)
+{
+	struct dt_t  *lastDT;
+
+	if (!list || !pDT)
+		return HDD_FAIL;
+
+	if (list_empty(list))
+		return HDD_FAIL;
+
+	lastDT = list_entry(list->prev, struct dt_t, list_head);
+	lastDT->fields &= ~DT_CTRL_ENDOFCHAIN;
+	lastDT->BE_next_dma = cpu_to_be64(pDT->this_dma);
+
+	pDT->fields |= DT_CTRL_ENDOFCHAIN;
+
+	list_add_tail(&pDT->list_head, list);
+
+	return HOST_DD_Good;
+}
+
+
+// prepend a DT onto an existing list
+//
+int
+dt_prepend(struct list_head  *list,
+	   struct dt_t       *pDT)
+{
+	struct dt_t       *pFirst;
+
+	if (!list || !pDT)
+		return HDD_FAIL;
+
+	if (!list_empty(list)) {
+		pFirst = list_entry(list->next, struct dt_t, list_head);
+		pDT->BE_next_dma = cpu_to_be64(pFirst->this_dma);
+	}
+
+	list_add(&pDT->list_head, list);
+	
+	pDT->fields &= ~DT_CTRL_ENDOFCHAIN;
+
+	return HOST_DD_Good;
+}
+
+
+struct dt_t *
+dt_allocate(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	struct dt_t *pDT = NULL;
+	struct list_head *pLH;
+
+	spin_lock_bh(&ctx->dt_pool_lock);
+
+	if (list_empty(&ctx->dt_pool)) 
+		goto done;
+
+	pLH = list_remove_head(&ctx->dt_pool);
+
+	pDT = list_entry(pLH, struct dt_t, list_head);
+	pDT->BE_data_dma = 0;
+	pDT->fields      = 0;
+	pDT->BE_next_dma = 0;
+	pDT->pReq        = pReq;
+
+done:
+	spin_unlock_bh(&ctx->dt_pool_lock);
+	return pDT;
+}
+
+
+void
+dt_free(ibm4767_ctx_t *ctx, struct dt_t *pDT)
+{
+	if (!pDT)
+		return;
+
+	spin_lock_bh(&ctx->dt_pool_lock);
+	// this routine assumes that pDT is not currently a member of a list!
+	//
+	list_add_tail(&pDT->list_head, &ctx->dt_pool);
+	spin_unlock_bh(&ctx->dt_pool_lock);
+}
+
+
+void
+dt_free_chain(ibm4767_ctx_t *ctx, struct list_head *chain_head)
+{
+	struct dt_t *pDT;
+
+	if (!chain_head)
+		return;
+
+	pDT = list_entry(chain_head->next, struct dt_t, list_head);
+
+	spin_lock_bh(&ctx->dt_pool_lock);
+	dt_merge(&ctx->dt_pool, chain_head);
+	spin_unlock_bh(&ctx->dt_pool_lock);
+}
+
+
+int
+dt_append_scatterlist(ibm4767_ctx_t      *ctx,
+		      request_t          *pReq,
+		      struct list_head   *dt_list,
+		      y_scatterlist_t    *slist)
+{
+	int rc = HOST_DD_Good;
+	struct list_head *pSL;
+	struct dt_t      *pDT, *prevDT = NULL;
+
+	//
+	// assumes 'dt_list' has already been initialized
+	//
+
+	if (list_empty(&slist->frags)) {
+		PDEBUG(1, "Device %d: BUG?  dt_append_scatterlist for empty scatterlist\n", ctx->dev_index);
+		return HOST_DD_Good;
+	}
+
+
+	// this routine might be called multiple times to append multiple scatterlists
+	// into a DT chain.  this means that prevDT isn't necessarily NULL...
+	//
+	if (!list_empty(dt_list)) {
+		struct list_head *pLH = dt_list->prev;
+		prevDT = list_entry(pLH, struct dt_t, list_head);
+		prevDT->fields &= ~DT_CTRL_ENDOFCHAIN;
+	}
+
+	pSL = slist->frags.next;
+	do {
+		scatter_frag_t *frag = list_entry(pSL, scatter_frag_t, list_head);
+
+		pDT = dt_allocate(ctx, pReq);
+		if (!pDT) {
+			PDEBUG(1, "Device %d: No more free DTs...\n", ctx->dev_index);
+			rc = HOST_DD_NoMemory;
+			goto error;
+		}
+
+		if (frag->vp.len == 0) {
+			PDEBUG(1, "Device %d: 0-length scatter fragment!\n", ctx->dev_index);
+			rc = HDD_FAIL;
+			goto error;
+		}
+
+		pDT->BE_data_dma = cpu_to_be64(frag->vp.dma);
+
+		pDT->fields = ((uint64_t)frag->len << 40) | DT_SIGNATURE;
+		pDT->fields = cpu_to_be64(pDT->fields);
+		
+		pDT->BE_next_dma = 0;
+		pDT->data_va = frag->vp.va;
+
+#if 0
+		PDEBUG(1," Device %d: DT @ VA: %p  (DMA: %p)\n", ctx->dev_index, pDT, (char *)pDT->this_dma);
+		PDEBUG(1,"  BE_data_dma: x%016llx  frag: x%016llx\n", pDT->BE_data_dma, frag->vp.dma);
+		PDEBUG(1,"  fields: x%016llx\n", pDT->fields);
+		PDEBUG(1,"  BE_next_dma: x%016llx\n", pDT->BE_next_dma);
+		PDEBUG(1,"  this DT's DMA addr: x%016llx\n", pDT->this_dma);
+#endif
+		if (prevDT) 
+			prevDT->BE_next_dma = cpu_to_be64(pDT->this_dma);
+		
+		prevDT = pDT;
+		list_add_tail(&pDT->list_head, dt_list);
+		
+		pSL = pSL->next;
+	} while (pSL != &slist->frags);
+
+	prevDT->fields |= DT_CTRL_ENDOFCHAIN;
+
+	return HOST_DD_Good;
+
+error:
+	// deallocate any DTs that we had allocated...
+	//
+	dt_free_chain(ctx, dt_list);
+	return rc;
+}
+
+
+void
+dump_chain(ibm4767_ctx_t *ctx, struct list_head *list, int verbose)
+{ 
+#if defined(DEBUG)
+	struct list_head *pLH = NULL;
+	int i = 0;
+
+	if (!list)
+		return;
+
+	if (list_empty(list))
+		return;
+
+	list_for_each(pLH, list) {
+		struct dt_t *pDT = list_entry(pLH, struct dt_t, list_head);
+
+		PDEBUG(3, "%d: VA: %p (LA: %016llx):  data: %016llx  fields: %016llx  next: %016llx\n",
+				i++, 
+				pDT, pDT->this_dma, pDT->BE_data_dma, pDT->fields, pDT->BE_next_dma);
+
+		if (verbose) {
+			//uint32_t bytes = cpu_to_be32((pDT->fields >> 40) & 0x1FFFFF);
+			uint32_t bytes = (cpu_to_be64(pDT->fields) >> 40) & 0x1FFFFF;
+			PDEBUG(5, "DATA @ VA = %p, LA = %016llx, len = %d\n", pDT->data_va, cpu_to_be64(pDT->BE_data_dma), bytes);
+			hex_dump(ctx, 5, "contents:", pDT->data_va, MIN(256, bytes));
+		}
+	}
+#endif
+}
+
+
+void
+dump_dt(ibm4767_ctx_t *ctx, struct dt_t *pDT)
+{ 
+	PDEBUG(3, "%016llx %08x %02x %04x %016llx\n", 
+			pDT->BE_data_dma, 
+			(uint32_t)((pDT->fields >> 40) & 0x1FFFFF),   // byte count
+			(uint8_t) ((pDT->fields >> 60) & 0xF),        // ctrl bits
+			(uint16_t)((pDT->fields      ) & 0xFFFF),     // sig
+			pDT->BE_next_dma);
+}
+
+
+void
+dt_set_hrb_len(struct list_head *dt_list, uint32_t hrb_len)
+{
+	struct list_head *pLH = dt_list->next;
+	struct dt_t *pDT;
+	uint64_t tmp64;
+
+	PDEBUG(2, "Enter %s (0x%x bytes)\n", __FUNCTION__, hrb_len);
+
+	if (!pLH) {
+		PDEBUG(2, "pLH == NULL!!\n");
+		return;
+	}
+	
+	tmp64 = cpu_to_be64((uint64_t)hrb_len << 16);
+	PDEBUG(2, "new tmp64 = x%016llx\n", tmp64);
+	
+	pDT = list_entry(pLH, struct dt_t, list_head);
+	pDT->fields &= ~DT_HRB_LEN_MASK;
+	pDT->fields |= tmp64;
+}
+
+
+// to implement request-tracking, we'll set the NOTIFY_RX and RMRI bits on
+// the last DT in the chain.  naturally, this routine should be called only
+// after the DT chain is fully-assembled
+//
+void
+dt_set_rmri_last(struct list_head *dt_list, request_t *pReq)
+{
+	struct list_head *lh;
+	struct dt_t      *pDT;
+
+	PDEBUG(2, "Enter %s\n", __FUNCTION__);
+
+	if ((!dt_list) || (list_empty(dt_list)))
+		return;
+
+	lh = dt_list->prev;
+	pDT = list_entry(lh, struct dt_t, list_head);
+
+	PDEBUG(5, "Setting RMRI for DT @ DMA = %016llx, data VA = %p, data LA = %016llx\n", pDT->this_dma, pDT->data_va, pDT->BE_data_dma);
+
+	pDT->fields |= (DT_CTRL_NOTIFYRX | DT_CTRL_RMRI);
+	pDT->pReq = pReq;
+}
+
diff --git drivers/misc/ibm4767/host_err.h drivers/misc/ibm4767/host_err.h
new file mode 100755
index 000000000000..cdb8092439d4
--- /dev/null
+++ drivers/misc/ibm4767/host_err.h
@@ -0,0 +1,145 @@
+/*************************************************************************
+ *  Filename:host_err.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  Error codes                                         
+ *                                         
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#ifndef _HOST_ERR_H_
+#define _HOST_ERR_H_
+
+
+#define HOST_DD_ERR         0x80400000L  /* Host DD    generated error */
+#define HOST_OS_ERR         0x81400000L  /* Host OS    generated error */
+#define POST_ERR            0x82400000L  /* POST       generated error */
+#define MINIBOOT0_ERR       0x83400000L  /* MiniBoot 0 generated error */
+#define MINIBOOT1_ERR       0x84400000L  /* MiniBoot 1 generated error */
+#define CDU_ERR             0x85400000L  /* CDU        generated error */
+#define OTHER_ERR           0x86000000L
+
+#ifdef LINUX
+// the following are internal values.  do not reference them directly in user code...
+#  define HOST_DD_Good            0x0000L
+#  define HOST_DD_NotPermitted    0x0001L  /* Operation not Permitted    EPERM   */
+#  define HOST_DD_Interrupted     0x0004L  /* Operation interrupted      EINTR   */
+#  define HOST_DD_NoDevice        0x0006L  /* Device offline or missing  ENXIO   */
+#  define HOST_DD_NoMemory        0x000CL  /* HDD memory error           ENOMEM  */
+#  define HOST_DD_AccessDenied    0x000DL  /* Access Denied              EACCES  */
+#  define HOST_DD_BadAddress      0x000EL  /* Bad address specified      EFAULT  */
+#  define HOST_DD_DeviceBusy      0x0010L  /* Cannot execute request     EBUSY   */
+#  define HOST_DD_InvalidParm     0x0016L  /* Invalid parameter          EINVAL  */
+#  define HOST_DD_HardwareError   0x0020L  /* Hardware error             EPIPE   */
+#  define HOST_DD_FirmwareError   0x0021L  /* Firmware error             EDOM    */
+#  define HOST_DD_Temperature     0x0028L  /* Temperature shutdown       ELOOP   */
+#  define HOST_DD_TamperDetected  0x002AL  /* Device tamper              ENOMSG  */
+#  define HOST_DD_Aborted         0x002FL  /* Generic abort              EL3RST  */
+#  define HOST_DD_BadRequest      0x0038L  /* Bad Request                EBADRQC */
+#  define HOST_DD_Timeout         0x003EL  /* Request timeout            ETIME   */
+#  define HOST_DD_BufferTooSmall  0x0069L  /* Reply Buffer Too Small     ENOBUFS */
+
+// the following are the values that will be seen by user code...
+#define HOSTGood            (HOST_DD_Good)                         /* 0x00000000 */
+#define HOSTNotPermitted    (HOST_DD_ERR | HOST_DD_NotPermitted)   /* 0x80400001 */
+#define HOSTInterrupted     (HOST_DD_ERR | HOST_DD_Interrupted)    /* 0x80400004 */
+#define HOSTNoDevice        (HOST_DD_ERR | HOST_DD_NoDevice)       /* 0x80400006 */
+#define HOSTNoMemory        (HOST_DD_ERR | HOST_DD_NoMemory)       /* 0x8040000C */
+#define HOSTAccessDenied    (HOST_DD_ERR | HOST_DD_AccessDenied)   /* 0x8040000D */
+#define HOSTBadAddress      (HOST_DD_ERR | HOST_DD_BadAddress)     /* 0x8040000E */
+#define HOSTDeviceBusy      (HOST_DD_ERR | HOST_DD_DeviceBusy)     /* 0x80400010 */
+#define HOSTInvalidParm     (HOST_DD_ERR | HOST_DD_InvalidParm)    /* 0x80400016 */
+#define HOSTHardwareError   (HOST_DD_ERR | HOST_DD_HardwareError)  /* 0x80400020 */
+#define HOSTFirmwareError   (HOST_DD_ERR | HOST_DD_FirmwareError)  /* 0x80400021 */
+#define HOSTTemperature     (HOST_DD_ERR | HOST_DD_Temperature)    /* 0x80400028 */
+#define HOSTTamperDetected  (HOST_DD_ERR | HOST_DD_TamperDetected) /* 0x8040002A */
+#define HOSTAborted         (HOST_DD_ERR | HOST_DD_Aborted)        /* 0x8040002F */
+#define HOSTBadRequest      (HOST_DD_ERR | HOST_DD_BadRequest)     /* 0x80400038 */
+#define HOSTTimeout         (HOST_DD_ERR | HOST_DD_Timeout)        /* 0x8040003E */
+#define HOSTBufferTooSmall  (HOST_DD_ERR | HOST_DD_BufferTooSmall) /* 0x80400069 */
+
+#else
+// in the windows universe, userspace error codes and kernel status codes
+// are not the same.  for example, the driver can return STATUS_DEVICE_BUSY
+// (0x80000011L as defined in ntstatus.h) and GetLastError() will convert this
+// to ERROR_BUSY (0xAA as defined in WinError.h)...
+//
+// FIXME
+#  define HOST_DD_Good            0x0000L
+#  define HOST_DD_NotPermitted    0x0001L
+#  define HOST_DD_NoMemory        0x0002L
+#  define HOST_DD_BadAddress      0x0003L
+#  define HOST_DD_DeviceBusy      0x0004L
+#  define HOST_DD_InvalidParm     0x0005L
+#  define HOST_DD_HardwareError   0x0006L
+#  define HOST_DD_FirmwareError   0x0007L
+#  define HOST_DD_Temperature     0x0008L
+#  define HOST_DD_TamperDetected  0x0009L
+#  define HOST_DD_Aborted         0x000AL
+#  define HOST_DD_BadRequest      0x000BL
+#  define HOST_DD_Timeout         0x000CL
+#  define HOST_DD_NoDevice        0x000DL
+#  define HOST_DD_Interrupted     0x000EL
+#  define HOST_DD_AccessDenied    0x000FL
+#  define HOST_DD_BufferTooSmall  0x0010L
+
+#define HOSTGood            (HOST_DD_Good)
+#define HOSTInterrupted     (HOST_DD_ERR | HOST_DD_Interrupted)
+#define HOSTNotPermitted    (HOST_DD_ERR | HOST_DD_NotPermitted)
+#define HOSTNoMemory        (HOST_DD_ERR | HOST_DD_NoMemory)
+#define HOSTAccessDenied    (HOST_DD_ERR | HOST_DD_AccessDenied)
+#define HOSTBadAddress      (HOST_DD_ERR | HOST_DD_BadAddress)
+#define HOSTDeviceBusy      (HOST_DD_ERR | HOST_DD_DeviceBusy) 
+#define HOSTNoDevice        (HOST_DD_ERR | HOST_DD_NoDevice)
+#define HOSTInvalidParm     (HOST_DD_ERR | HOST_DD_InvalidParm) 
+#define HOSTHardwareError   (HOST_DD_ERR | HOST_DD_HardwareError)
+#define HOSTFirmwareError   (HOST_DD_ERR | HOST_DD_FirmwareError)
+#define HOSTTemperature     (HOST_DD_ERR | HOST_DD_Temperature)
+#define HOSTTamperDetected  (HOST_DD_ERR | HOST_DD_TamperDetected) 
+#define HOSTAborted         (HOST_DD_ERR | HOST_DD_Aborted)
+#define HOSTBadRequest      (HOST_DD_ERR | HOST_DD_BadRequest) 
+#define HOSTTimeout         (HOST_DD_ERR | HOST_DD_Timeout) 
+#define HOSTBufferTooSmall  (HOST_DD_ERR | HOST_DD_BufferTooSmall) 
+
+#endif
+
+#define POSTError           (POST_ERR       | 0x0001L)
+#define MB0Error            (MINIBOOT0_ERR  | 0x0001L)
+#define MB1Error            (MINIBOOT1_ERR  | 0x0001L)
+
+
+#define CDU_ERR_NOT_CDUABLE       0x85400080
+#define CDU_ERR_NOT_ATTACHED      0x85400081
+#define CDU_ERR_SEG3_NONEXISTENT  0x85400082
+#define CDU_ERR_SEG3_OA_FAILURE   0x85400083
+#define CDU_ERR_SEG3_MOUNT_FAIL   0x85400084
+#define CDU_ERR_SEG3_UMOUNT_FAIL  0x85400085
+#define CDU_ERR_SEG3_LOCK_FAIL    0x85400086
+#define CDU_ERR_AGENTID_RUNNING   0x85400087
+#define CDU_ERR_TIMEOUT           0x85400088
+#define CDU_ERR_MCPU_FAIL         0x85400089
+#define CDU_ERR_OTHER_FAILURE     0x85400090
+
+#endif
diff --git drivers/misc/ibm4767/htb.c drivers/misc/ibm4767/htb.c
new file mode 100755
index 000000000000..feba9085cd46
--- /dev/null
+++ drivers/misc/ibm4767/htb.c
@@ -0,0 +1,224 @@
+/*************************************************************************
+ *  Filename:htb.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+
+int
+htb_allocate(ibm4767_ctx_t *ctx)
+{
+	char *virt;
+	dma_addr_t dma;
+	int       bytes;
+
+	PDEBUG(1, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	if (ctx->htb[0].va)
+		htb_unload(ctx);
+
+	bytes = 2 * HTB_SIZE;
+
+	virt = dma_alloc_coherent(ctx->dev, bytes, &dma, GFP_ATOMIC);
+	if (!virt) {
+		PDEBUG(1, "Device %d: dma_alloc_coherent failed for HTB.  %d bytes.\n", ctx->dev_index, bytes);
+		return HOST_DD_NoMemory;
+	}
+
+	ctx->htb[0].va  = virt;
+	ctx->htb[0].dma = dma;
+	ctx->htb[0].len = HTB_SIZE;
+
+	ctx->htb[1].va  = ctx->htb[0].va  + HTB_SIZE;
+	ctx->htb[1].dma = ctx->htb[0].dma + HTB_SIZE;
+	ctx->htb[1].len = HTB_SIZE;
+	
+	ctx->htb_active = 0;
+
+	PDEBUG(1, "Device %d: %d bytes, VA[0] = %p, LA[0] = %016llx\n", ctx->dev_index, ctx->htb[0].len, ctx->htb[0].va, ctx->htb[0].dma);
+	PDEBUG(1, "Device %d: %d bytes, VA[1] = %p, LA[1] = %016llx\n", ctx->dev_index, ctx->htb[1].len, ctx->htb[1].va, ctx->htb[1].dma);
+
+#if defined(ENABLE_HTB_SCRUBBING)
+	{
+		int i;
+		htb_vector_t *pHTB = ctx->htb[0].va;
+		for (i=0; i < HTB_ENTRIES*2; i++) {
+			pHTB->msg = 0xDEADBEEFDEADBEEFLL;
+			pHTB++;
+		}
+	}
+#endif
+
+
+#if 0
+	ctx->active_htb = 0;
+	ctx->next_HTB_LA = ctx->htb[0].LA.QuadPart;
+	RtlZeroMemory(ctx->htb[0].VA, HTB_SIZE);
+
+	tmp64 = cpu_to_be64(ctx->htb[0].LA.QuadPart);
+	PDEBUG(1, "Setting HTB_WA to %p\n", tmp64);
+	write64(tmp64, HTB_WA(ctx));
+	write32(cpu_to_be32(HTB_SIZE), HTB_TC(ctx));
+	
+	// we can't read the HTB_WA unless we received an interrupt otherwise we
+	// can accidentally clear the int before it gets sent.  but we can read the
+	// HTB_TC without such side effects...
+	//
+	PDEBUG(1, "New HTB_TC = 0x%08x\n", be32_to_cpu(read32(HTB_TC(ctx))));
+#endif
+
+	return HOST_DD_Good;
+}
+
+
+void
+htb_unload(ibm4767_ctx_t *ctx)
+{
+	PDEBUG(1, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	if (ctx->htb[0].va) {
+		dma_free_coherent(ctx->dev, (2 * HTB_SIZE), ctx->htb[0].va, ctx->htb[0].dma);
+	}
+
+	ctx->htb[0].va = 0;
+}
+
+
+void
+htb_swap(ibm4767_ctx_t *ctx)
+{
+	uint64_t tmp64;
+
+	PDEBUG(2, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	if (ctx->htb[0].va == NULL) 
+		htb_allocate(ctx);
+
+	// in the 4765 universe, we could initialize the HTB immediately after a reset.
+	// in the 4767 universe, the DMA registers are held in reset until explicitly
+	// released by the SSP or MCPU.  the workaround in the driver is to initialize
+	// the HTB and enable host-side DMA when we receive the HELLO notifications.
+	//
+	// we need to be careful, though, that we only initialize the HTB once (unless
+	// the card explicitly requests it)
+	//
+	if (ctx->htb_active == 0xFF)
+		ctx->htb_active = 0;
+
+	ctx->htb_active = ctx->htb_active ^ 1;  // toggle 0 or 1
+	ctx->htb_next_dma = ctx->htb[ctx->htb_active].dma;
+
+#if defined(DEBUG)
+	memset(ctx->htb[ctx->htb_active].va, 0x0, HTB_SIZE);
+#endif
+
+	tmp64 = cpu_to_be64(ctx->htb_next_dma);
+	PDEBUG(2, "Device %d: Setting HTB_WA to %016llx\n", ctx->dev_index, tmp64);
+	write64(tmp64, HTB_WA(ctx));
+
+	// HTB_TC register is defined as:
+	// bits  0:39  reserved
+	// bits 40:59  # HTB entries
+	// bits 60:63  reserved
+	//
+	// note: since bits 60:63 are 0, bits 40:63 are simply the HTB size in bytes
+	//
+	write64(cpu_to_be64(HTB_SIZE), HTB_TC(ctx));
+
+	//PDEBUG(2, "Attempted to write 0x%016llx to HTB_TC...\n", tmp64);
+
+	// we can't read the HTB_WA unless we received an interrupt otherwise we
+	// can accidentally clear the int before it gets sent.  but we can read the
+	// HTB_TC without such side effects...
+	//
+	PDEBUG(2, "Device %d: New HTB_TC = 0x%016llx\n", ctx->dev_index, be64_to_cpu(read64(HTB_TC(ctx))));
+}
+
+
+htb_vector_t *
+htb_lookup(ibm4767_ctx_t *ctx, uint64_t  phys)
+{
+	htb_vector_t  *pHTB = (htb_vector_t *)ctx->htb[0].va;
+	uint64_t       off  = phys - ctx->htb[0].dma;
+
+	if ((phys < ctx->htb[0].dma) || (off > (2*HTB_SIZE))) {
+		PRINTKW("Device %d: HTB weirdness: bounds\n", ctx->dev_index);
+		return NULL;
+	}
+
+	if ((off & (HTB_VECTOR_SIZE-1))) {
+		PRINTKW("Device %d: HTB weirdness: %016llx misaligned\n", ctx->dev_index, phys); 
+		return NULL;
+	}
+
+	return &pHTB[off / HTB_VECTOR_SIZE];
+}
+
+
+void
+dump_htb(ibm4767_ctx_t *ctx)
+{
+#if defined(DEBUG)
+	htb_vector_t *pHTB = (htb_vector_t *)ctx->htb[ctx->htb_active].va;
+
+	hex_dump(ctx, 3, "HTB Contents", pHTB, HTB_SIZE);
+
+#if 0
+	for (i=0; i < HTB_SIZE / sizeof(htb_vector_t); i++, pHTB++) {
+		PDEBUG(1, "%3d: CW: %02x RES1: %08x RES2: %02x TYPE: %02x LEN: %02x\n",
+				i, pHTB->CW, pHTB->reserved1, pHTB->reserved2, pHTB->type, pHTB->len);
+		PDEBUG(1, "   : IV: %p\n", pHTB->msg);
+	}
+
+#endif
+	PDEBUG(3, "\n");
+#endif
+}
+
diff --git drivers/misc/ibm4767/ints.c drivers/misc/ibm4767/ints.c
new file mode 100755
index 000000000000..8afa96b44b33
--- /dev/null
+++ drivers/misc/ibm4767/ints.c
@@ -0,0 +1,639 @@
+/*************************************************************************
+ *  Filename:ints.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "xc_host.h"
+#include "y_regs.h"
+#include "y_mailbox.h"
+#include "driver.h"
+#include "y_funcs.h"
+#include "host_err.h"
+
+
+
+irqreturn_t 
+ibm4767_isr(int irq, void *dev_info)
+{
+	ibm4767_ctx_t     *ctx;
+	interrupt_state_t  istate;
+	uint32_t         i, hisr;
+
+
+	ctx = (ibm4767_ctx_t *)dev_info;
+
+	hisr = cpu_to_be32(read32(HISR(ctx)));
+	PDEBUG(2, "Device %d: Enter %s: HISR %08x\n", ctx->dev_index, __FUNCTION__, hisr);
+
+	if (!(hisr & HISR_INT_BITS)) {
+		// if we're sharing an IRQ perhaps another device is interrupting instead?
+		PDEBUG(1, "Device %d: IRQ but HISR = %08x\n", ctx->dev_index, hisr);
+		return IRQ_NONE;
+	}
+
+		
+	ctx->last_hisr = hisr;
+
+	istate.hisr = hisr;
+
+	if (m2h_full(hisr)) {
+		istate.mcpu_mbx_lo = ctx->last_m2h_l = cpu_to_be32(read32(M2H_MBX_L(ctx)));
+		istate.mcpu_mbx_hi = ctx->last_m2h_h = cpu_to_be32(read32(M2H_MBX_H(ctx)));
+	}
+
+	if (s2h_full(hisr)) {
+		istate.ssp_mbx_lo = ctx->last_s2h_l = cpu_to_be32(read32(S2H_MBX_L(ctx)));
+		istate.ssp_mbx_hi = ctx->last_s2h_h = cpu_to_be32(read32(S2H_MBX_H(ctx)));
+	}
+
+	if (xfer_complete(hisr)) {
+#if defined(DEBUG)
+		char *text = NULL;
+		uint64_t       tc;
+
+		if (xfer_complete(hisr) && xfer_buffer_full(hisr))
+			text = "TX cpl + HTB bf";
+		else if (xfer_complete(hisr))
+			text = "TX cpl";
+		else
+			text = "HTB bf";
+		tc = cpu_to_be64(read64(HTB_TC(ctx)));
+#endif
+		ctx->htb_last_wa = cpu_to_be64(read64(HTB_WA(ctx)));
+
+		PDEBUG(2, "Device %d: %s - HTB_TC  = %016llx\n", ctx->dev_index, text, tc);
+
+		istate.htb_start  = ctx->htb_next_dma;
+		istate.htb_end    = ctx->htb_last_wa;
+		ctx->htb_next_dma = ctx->htb_last_wa;
+	}
+
+	// we keep a circular buffer of saved interrupts.  if we're careful, we
+	// don't have to worry about locking because there's only one producer
+	// and one consumer and a full buffer is an error condition (eg. overwriting
+	// an un-consumed slot means that interrupt is effectively lost...bad!)
+	//
+	i = atomic_read(&ctx->int_state_wr_next);
+
+	// BEAM will complain about istate being partially-uninitialized.  it's not a bug.
+	// we only set the fields that are needed for that particular interrupt type
+	//
+	memcpy(&ctx->int_states[i], &istate, sizeof(istate)); /*uninitialized*/ /* BEAM_HACK */
+
+	i = (i + 1) & (MAX_INT_STATES - 1);
+	
+	PDEBUG(3, "Device %d: int state: %d, rd_next: %d\n", ctx->dev_index, i, atomic_read(&ctx->int_state_rd_next));
+
+	if (i == atomic_read(&ctx->int_state_rd_next)) {
+		PRINTKE("Device %d: no free int states!\n", ctx->dev_index);
+		mark_device_offline(ctx, HOSTNoMemory);
+		//FIXME defer_main_reset(ctx, 1, FALSE);
+	}
+	else {
+		atomic_set(&ctx->int_state_wr_next, i);
+	}
+
+	tasklet_schedule(&ctx->interrupt_dpc);
+
+	return IRQ_HANDLED;
+}
+
+
+void 
+ibm4767_interrupt_tasklet(unsigned long arg)
+{ 
+	ibm4767_ctx_t *ctx;
+	int          idx;
+	int          fatal = FALSE;
+	uint32_t     event_code = 0;
+        int          cnt = 0;
+
+	if (tasklet_shutdown[arg]) {
+		PRINTKW("%s: tasklet shutdown!\n", __FUNCTION__);
+		return;
+	}
+
+	ctx = ibm4767_list[arg];
+	if (!ctx) { 
+		PRINTKW("%s: NULL device context\n", __FUNCTION__); 
+		return; 
+	}
+
+
+	idx = atomic_read(&ctx->int_state_rd_next);
+	while (idx != atomic_read(&ctx->int_state_wr_next)) {
+		interrupt_state_t *st= &ctx->int_states[idx];
+		if (++cnt > 4096) {
+			return;
+		}
+
+		PDEBUG(2, "Device %d: %s: %d  hisr=%08x\n", ctx->dev_index, __FUNCTION__, idx, st->hisr);
+
+		if (link_lost32(st->hisr)) {
+			uint32_t hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+			if (link_lost32(hrcsr)) {
+				PRINTKE("ERROR:  device %d taken offline (link lost).  Possible NMI to follow.\n", ctx->dev_index);
+				mark_device_offline(ctx, HOSTHardwareError);
+				notify_observers(ctx, EVENT_HW_ERR);
+				// register dump would be pointless...
+				return;
+			}
+		}
+			
+		if (hardware_error(st->hisr)) {
+			PRINTKE("ERROR: device %d taken offline (hardware error)\n", ctx->dev_index);
+			// clear the interrupt
+			write32(cpu_to_be32(HISR_HW_ERR), HISR(ctx));
+			fatal = TRUE;
+			event_code = EVENT_HW_ERR;
+		}
+
+		if (access_error(st->hisr)) {
+			PRINTKE("ERROR: device %d taken offline (access error)\n", ctx->dev_index);
+			fatal = TRUE;
+			event_code = EVENT_ACCESS_ERR;
+		}
+
+		if (recoverable_dma_error(st->hisr)) {
+			analyze_dma_error(ctx);
+			fatal = TRUE;
+		}
+
+		if (unrecoverable_dma_error(st->hisr)) {
+			PRINTKE("ERROR: device %d taken offline (unrecov dma error 0x%08x)\n", ctx->dev_index, cpu_to_be32(read32(H_CPU_DMAERR(ctx))));
+			// clear the interrupt
+			write32(cpu_to_be32(HISR_UNRECOV_DMA), HISR(ctx));
+			fatal = TRUE;
+			event_code = EVENT_UNRECOV_DMA_ERR;
+		}
+
+		if (fatal) {
+			mark_device_offline(ctx, HOSTHardwareError);
+			dump_regs(ctx);
+			notify_observers(ctx, event_code);
+		}
+
+		if (high_temp_warning(st->hisr))  {
+			PRINTKW("WARNING: device %d reported a high temperature warning\n", ctx->dev_index); 
+			notify_observers(ctx, EVENT_HIGH_TEMP_WARNING);
+		}
+		
+		if (srm_attn(st->hisr)) { 
+			PRINTKE("ERROR: device %d reported SRM ATTN.  Possible emergency dump to follow\n", ctx->dev_index); 
+			// emergency dumps are two-pronged beasts: the firmware notifies the HDD via mailbox that
+			// a dump is pending and then it begins writing the dump to the FIFO.  edumps feature a
+			// DMA header with the NO_TX bit set which causes DMA to stall.  a side effect is that 
+			// the HDD sees an SRM/MRM-attention interrupt if the corresponding host-side DMA channel 
+			// is enabled at the time.
+			//
+			// normally, the HDD will receive and process the mailbox interrupt first and will have
+			// disabled DMA prior to the firmware writing the edump header to the FIFO.  thus, normally
+			// the HDD won't see this attention interrupt.  however, if the edump is the result of
+			// a POST diagnostic error (NOTIFICATION_DIAG_ERROR) and if the module parameter 'postde_echo=1',
+			// then the HDD does not disable DMA and thus the hardware will generate an attention interrupt.
+			//
+			// just ignore it.  i'm debating whether to remove the log message entirely...
+		}
+
+		if (mrm_attn(st->hisr)) { 
+			PRINTKE("ERROR: device %d reported MRM ATTN.  Possible emergency dump to follow\n", ctx->dev_index); 
+		}
+
+		if (is_hard_tamper(st->hisr)) {
+			uint32_t hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+
+			PRINTKE("TAMPER for device %d (S/N %s).  HISR = %08X, HRCSR = %08X\n", ctx->dev_index, ctx->hwinfo.serial_num, st->hisr, hrcsr);
+			ibm4767_dump_tamper_regs(ctx);
+			if (!ignore_tamper) {
+				mark_device_offline(ctx, HOSTTamperDetected);
+					
+				ctx->status_mcpu   = MCPU_TAMPER;
+				ctx->status_ssp    = SSP_TAMPER;
+				ctx->status_tamper = TAMPER_PERM;
+			}
+
+			notify_observers(ctx, EVENT_HARD_TAMPER);
+		}
+
+		if (is_soft_tamper(st->hisr)) {
+			uint32_t hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+			
+			if (hrcsr & HRCSR_SOFT_VOLT)  {
+				PRINTKE("Soft tamper (Volts) for device %d (S/N %s).  HISR = %08x, HRCSR = %08x\n", ctx->dev_index, ctx->hwinfo.serial_num, st->hisr, hrcsr);
+				notify_observers(ctx, EVENT_SOFT_TAMPER_VOLTS);
+				ctx->status_tamper = TAMPER_SOFT_VOLT;
+			}
+			else if (hrcsr & HRCSR_SOFT_INJ) {
+				PRINTKE("Soft tamper (Injected) for device %d (S/N %s).  HISR = %08x, HRCSR = %08x\n", ctx->dev_index, ctx->hwinfo.serial_num, st->hisr, hrcsr);
+				notify_observers(ctx, EVENT_SOFT_TAMPER_INJ);
+				ctx->status_tamper = TAMPER_SOFT_INJ;
+			}
+			else if (hrcsr & HRCSR_SOFT_TEMP) {
+				PRINTKE("Soft tamper (Temp) for device %d (S/N %s).  HISR = %08x, HRCSR = %08x\n", ctx->dev_index, ctx->hwinfo.serial_num, st->hisr, hrcsr);
+				PRINTKE("Taking ALL 4767 cards OFFLINE.  Please examine system airflow...\n");
+				atomic_set(&temp_soft_tamper_detected, 1);
+				ibm4767_do_temperature_shutdown(ctx);
+				ctx->status_tamper = TAMPER_SOFT_TEMP;
+				notify_observers(ctx, EVENT_SOFT_TAMPER_TEMP);
+			}
+			else {
+				PRINTKE("Soft tamper (Unknown) for device %d.  HISR = %08x, HRCSR = %08x\n", ctx->dev_index, st->hisr, hrcsr);
+				ctx->status_tamper = TAMPER_SOFT_OTHER;
+				notify_observers(ctx, EVENT_SOFT_TAMPER_OTHER);
+			}
+
+			ibm4767_dump_tamper_regs(ctx);
+
+			// for soft tampers, card goes into reset and HRCSR::STRST=1 until the condition that
+			// caused the tamper goes away.  after source of the soft tamper goes away, 
+			// HRCSR::STRST=0 and HRCSR::RRAST=1
+			//
+			// the perpetual soft tamper timer will monitor STRST and RRAST for each card and
+			// take the necessary steps to reset them following a soft tamper going away...
+
+			ctx->status_mcpu = MCPU_SOFT_TAMPER;
+			ctx->status_ssp  = SSP_SOFT_TAMPER;
+			
+			// the HRCSR::RRAT and HRCSR::TRst bits will only ever be set immediately following a
+			// tamper.  so we can use this to determine whether or not to reset the card and not
+			// have to worry about getting caught in a reset loop.  the HRCSR::1stRAT doesn't
+			// really prevent us from getting in a loop...
+			//
+			if (hrcsr & (HRCSR_TRST | HRCSR_STRST))
+				defer_main_reset(ctx, 5, FALSE);
+			else if (hrcsr & (HRCSR_RRAT | HRCSR_RRAST))
+				defer_main_reset(ctx, 1, FALSE);
+		}
+
+		if (is_low_batt(st->hisr))  {
+			PRINTKW("WARNING:  device %d reports Low Battery\n", ctx->dev_index); 
+			notify_observers(ctx, EVENT_LOW_BATTERY);
+		}
+
+		if (m2h_full(st->hisr)) 
+			m2h_mbx_handler(ctx, st->mcpu_mbx_hi, st->mcpu_mbx_lo);
+
+		if (s2h_full(st->hisr))
+			s2h_mbx_handler(ctx, st->ssp_mbx_hi, st->ssp_mbx_lo);
+
+		if (xfer_complete(st->hisr))
+			handle_xfer_complete(ctx, st);
+
+		idx = (idx + 1) & (MAX_INT_STATES-1);
+		atomic_set(&ctx->int_state_rd_next, idx);
+	}
+}
+
+
+void
+handle_xfer_complete(ibm4767_ctx_t *ctx, interrupt_state_t *pState)
+{
+	htb_vector_t  *pHTB_start = NULL;
+	htb_vector_t  *pHTB_stop  = NULL;
+	htb_vector_t  *pHTB;
+	request_t     *pReq;
+	uint64_t       phys;
+
+	PDEBUG(1, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	if (!pState->htb_start || !pState->htb_end) {
+		PDEBUG(1, "Device %d: NULL HTB pointer\n", ctx->dev_index);
+		return;
+	}
+
+	pHTB_start = htb_lookup(ctx, pState->htb_start);
+	pHTB_stop  = htb_lookup(ctx, pState->htb_end);
+
+#if defined(ENABLE_HTB_SCRUBBING)
+	if (pHTB_start > pHTB_stop) {
+		PRINTKE("HTBs wrapped!\n");
+	}
+
+	if (pHTB_start == pHTB_stop) {
+		PRINTKW("start == stop!\n");
+	}
+#endif
+
+	for (pHTB = pHTB_start; pHTB != pHTB_stop; pHTB++) {
+		//PDEBUG(1, "HTB:\n");
+		//PDEBUG(1, "   CW: %02x  res1: %08x  res2: %02x  type: %02x  len: %02x\n",
+		//		pHTB->CW, pHTB->reserved1, pHTB->reserved2, pHTB->type, pHTB->len);
+		//PDEBUG(1, "   IV: %016llx\n", pHTB->msg);
+
+#if defined(ENABLE_HTB_SCRUBBING)
+		// if this triggers then we know that we've scanned the same HTB entry twice.  that would
+		// imply a problem in the hardware IRQ routine or perhaps with the card's HTB_WA register
+		// 
+		if (pHTB->UF == 0x5B) {
+			PRINTKE("Re-read an HTB entry!\n");
+		}
+		pHTB->UF = 0x5B;
+#endif
+
+		if (pHTB->msg == 0x0) {
+			PDEBUG(1, "Device %d: Null IV...dumping HTB table\n", ctx->dev_index);
+			dump_htb(ctx);
+			//asym_hra_pool_dump(ctx);
+			dump_regs(ctx);
+
+			mark_device_offline(ctx, HOSTHardwareError);
+			write32(cpu_to_be32(MBX_FORCE_EDUMP), H2M_MBX_H(ctx));
+			return;
+		}
+
+		switch (pHTB->type) {
+			case IV_HTYPE_H2M:
+			case IV_HTYPE_H2S:
+			case IV_HTYPE_H2SKA:
+			case IV_HTYPE_H2SKB:
+				// we typically don't enable this type of interrupt
+				// for performance reasons...
+				//
+				PDEBUG(2, "Device %d: HTB read from host\n", ctx->dev_index);
+#if defined(ENABLE_REQ_TRACKING)
+				phys = cpu_to_be64(pHTB->msg);
+				pReq = request_locate_by_DT(ctx, phys);
+				if (pReq) {
+					if (pReq->status & REQ_STATUS_TIMEOUT)
+						PRINTKE("Device %d: RMRI received for timed-out request\n");
+
+					pReq->status |= REQ_STATUS_XFERRED;
+				}
+				else {
+					PRINTKW("Device %d: read HTB unmatched (%016llx)!\n", ctx->dev_index, phys); 
+					//mark_device_offline(ctx, HOSTHardwareError);
+					//write32(0L, HIER(ctx));
+					//return;
+				}
+#endif
+				break;
+
+			case IV_HTYPE_M2H:
+			case IV_HTYPE_S2H:
+			case IV_HTYPE_SKA2H:
+			case IV_HTYPE_SKB2H:
+			case IV_HTYPE_PKA2H:
+			case IV_HTYPE_PKB2H:
+			case IV_HTYPE_FA2H:
+			case IV_HTYPE_FB2H:
+				{
+					phys = cpu_to_be64(pHTB->msg);
+
+					//PDEBUG(1, "Device %d: HTB write to host\n", ctx->dev_index);
+					pReq = request_locate_by_DT(ctx, phys);
+					if (pReq) {
+						// special-case for HRA load packet responses
+						if (pReq == ctx->hra_req) {
+							asym_hra_pool_load_packet_reply(ctx, pReq);
+						}
+						else {
+							pReq->status |= (((pReq->status & 0x01) << 1) | REQ_STATUS_HRA1_RECD);
+
+							atomic_dec(&pReq->num_hras);
+							if (atomic_read(&pReq->num_hras) == 0)  {
+								pReq->retcode = 0;
+								atomic_set(&pReq->waitEvent, 1);
+								wake_up(&pReq->waitQ);
+
+							}
+						}
+					}
+					else {
+						// okay.  it must be an asym log message then...
+						if (asym_hra_reply_handler(ctx, phys) != HOST_DD_Good) {
+							PRINTKW("Device %d: HTB unmatched (0x%016llx)!\n", ctx->dev_index, phys); 
+							mark_device_offline(ctx, HOSTAborted);
+							write32(0L, HIER(ctx));
+							return;
+						}
+					} 
+				}
+				break;
+
+			default:
+				PRINTKW("Device %d: UNKNOWN HTB TYPE:  0x%02x\n", ctx->dev_index, pHTB->type);
+				mark_device_offline(ctx, HOSTAborted);
+				write32(0L, HIER(ctx));
+				return;
+		}
+
+#if defined(ENABLE_HTB_SCRUBBING)
+		pHTB->msg = 0xDEADBEEFDEADBEEFLL;
+#endif
+	}
+
+	if (pState->htb_end == (ctx->htb[ctx->htb_active].dma + HTB_SIZE)) {
+#if defined(ENABLE_HTB_SCRUBBING)
+		htb_vector_t *p = (htb_vector_t *)ctx->htb[ctx->htb_active].va;
+		int i;
+
+		for (i=0; i < HTB_ENTRIES; i++) {
+			if (p->msg != 0xDEADBEEFDEADBEEFLL) {
+				PRINTKE("Unhandled HTB entry!  i=%d, phys = %016llx\n", i, p->msg);
+			}
+			p++;
+		}
+#endif
+		PDEBUG(1, "Device %d: HTB buffer full --> calling htb_swap\n", ctx->dev_index);
+		htb_swap(ctx);
+	}
+
+}
+
+
+void
+analyze_dma_error(ibm4767_ctx_t *ctx)
+{
+	uint32_t dmaerr = cpu_to_be32(read32(H_CPU_DMAERR(ctx)));
+	uint32_t rch, wch;
+	int      len = 0;
+	char     buf[256];
+
+	rch = (dmaerr & DMAERR_RD_CH_ERR_ID_MASK) >> 28;
+	wch = (dmaerr & DMAERR_WR_CH_ERR_ID_MASK) >> 12;
+
+	PRINTKE("ERROR: device %d taken offline (H_CPU_DMAERR 0x%08x)\n", ctx->dev_index, dmaerr);
+
+	// see the AIB shim layer section in the function spec doc to decipher the channel IDs
+
+	if (rch != 0xF) {
+		len += sprintf(buf+len, "RD CH 0x%02x ", rch);  
+		if (dmaerr & DMAERR_RD_CH_DT_SIG_ERR)
+			len += sprintf(buf+len, "(DT signature) ");
+		if (dmaerr & DMAERR_RD_CH_DT_HRB_ERR)
+			len += sprintf(buf+len, "(HRB len 0 or not mult 8) ");
+		if (dmaerr & DMAERR_RD_CH_DT_MAX_ERR)
+			len += sprintf(buf+len, "(HRB len max) ");
+		if (dmaerr & DMAERR_RD_CH_DT_BC_ERR)
+			len += sprintf(buf+len, "(bad DT byte count) ");
+		if (dmaerr & DMAERR_RD_CH_DT_ALIGN_ERR)
+			len += sprintf(buf+len, "(DT alignment) ");
+		if (dmaerr & DMAERR_RD_CH_DT_512_ERR)
+			len += sprintf(buf+len, "(DT crosses 512b boundary) ");
+		if (dmaerr & DMAERR_RD_CH_DT_EOC_ERR)
+			len += sprintf(buf+len, "(HRB too short) ");
+		if (dmaerr & DMAERR_RD_CH_HRB_SIG_ERR)
+			len += sprintf(buf+len, "(HRB signature)  ");
+		PRINTKE("   %s\n", buf);
+
+		switch (rch) {
+			case DMA_CH_WR_MM:
+				PRINTKE("   H2MM_CH_STATUS:   %08x\n",    cpu_to_be32(read32(H2MM_CH_STATUS(ctx))));
+				PRINTKE("   H2MM_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(H2MM_BUFF_PTR(ctx))));
+				PRINTKE("   H2MM_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(H2MM_DT_CTRL_REG(ctx))));
+				PRINTKE("   H2MM_LAST_DT_PTR: %016llx\n", cpu_to_be64(read64(H2MM_LAST_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_SKA:
+				PRINTKE("   H2SKA_CH_STATUS:   %08x\n",    cpu_to_be32(read32(H2SKA_CH_STATUS(ctx))));
+				PRINTKE("   H2SKA_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(H2SKA_BUFF_PTR(ctx))));
+				PRINTKE("   H2SKA_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(H2SKA_DT_CTRL_REG(ctx))));
+				PRINTKE("   H2SKA_LAST_DT_PTR: %016llx\n", cpu_to_be64(read64(H2SKA_LAST_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_SKB:
+				PRINTKE("   H2SKB_CH_STATUS:   %08x\n",    cpu_to_be32(read32(H2SKB_CH_STATUS(ctx))));
+				PRINTKE("   H2SKB_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(H2SKB_BUFF_PTR(ctx))));
+				PRINTKE("   H2SKB_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(H2SKB_DT_CTRL_REG(ctx))));
+				PRINTKE("   H2SKB_LAST_DT_PTR: %016llx\n", cpu_to_be64(read64(H2SKB_LAST_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_MCPU:
+				PRINTKE("   H2M_CH_STATUS:   %08x\n",    cpu_to_be32(read32(H2M_CH_STATUS(ctx))));
+				PRINTKE("   H2M_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(H2M_BUFF_PTR(ctx))));
+				PRINTKE("   H2M_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(H2M_DT_CTRL_REG(ctx))));
+				PRINTKE("   H2M_LAST_DT_PTR: %016llx\n", cpu_to_be64(read64(H2M_LAST_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_SSP:
+				PRINTKE("   H2S_CH_STATUS:   %08x\n",    cpu_to_be32(read32(H2S_CH_STATUS(ctx))));
+				PRINTKE("   H2S_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(H2S_BUFF_PTR(ctx))));
+				PRINTKE("   H2S_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(H2S_DT_CTRL_REG(ctx))));
+				PRINTKE("   H2S_LAST_DT_PTR: %016llx\n", cpu_to_be64(read64(H2S_LAST_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_FA:
+				PRINTKE("   H2FA_CH_STATUS:   %08x\n",    cpu_to_be32(read32(H2FA_CH_STATUS(ctx))));
+				PRINTKE("   H2FA_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(H2FA_BUFF_PTR(ctx))));
+				PRINTKE("   H2FA_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(H2FA_DT_CTRL_REG(ctx))));
+				PRINTKE("   H2FA_LAST_DT_PTR: %016llx\n", cpu_to_be64(read64(H2FA_LAST_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_FB:
+				PRINTKE("   H2FB_CH_STATUS:   %08x\n",    cpu_to_be32(read32(H2FB_CH_STATUS(ctx))));
+				PRINTKE("   H2FB_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(H2FB_BUFF_PTR(ctx))));
+				PRINTKE("   H2FB_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(H2FB_DT_CTRL_REG(ctx))));
+				PRINTKE("   H2FB_LAST_DT_PTR: %016llx\n", cpu_to_be64(read64(H2FB_LAST_DT_PTR(ctx))));
+				break;
+		}
+	}
+
+	len = 0;
+	if (wch != 0xF) {
+		len += sprintf(buf+len, "WR CH 0x%02x ", wch);
+
+		if (dmaerr & DMAERR_WR_CH_DT_SIG_ERR)
+			len += sprintf(buf+len, "(DT signature) ");
+		if (dmaerr & DMAERR_WR_CH_DT_BC_ERR)
+			len += sprintf(buf+len, "(bad DT byte count) ");
+		if (dmaerr & DMAERR_WR_CH_DT_ALIGN_ERR)
+			len += sprintf(buf+len, "(DT alignment) ");
+		if (dmaerr & DMAERR_WR_CH_DT_512_ERR)
+			len += sprintf(buf+len, "(DT crosses 512b boundary) ");
+		if (dmaerr & DMAERR_WR_CH_DT_EOC_ERR)
+			len += sprintf(buf+len, "(HRA too short) ");
+		if (dmaerr & DMAERR_WR_CH_HRB_SIG_ERR)
+			len += sprintf(buf+len, "(MRM signature)  ");
+		PRINTKE("   %s\n", buf);
+
+		switch (wch) {
+			case DMA_CH_WR_MM:
+				PRINTKE("   MM2H_CH_STATUS:   %08x\n",    cpu_to_be32(read32(MM2H_CH_STATUS(ctx))));
+				PRINTKE("   MM2H_LAST_HDR:    %016llx\n", cpu_to_be64(read64(MM2H_LAST_HDR(ctx))));
+				PRINTKE("   MM2H_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(MM2H_BUFF_PTR(ctx))));
+				PRINTKE("   MM2H_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(MM2H_DT_CTRL_REG(ctx))));
+				PRINTKE("   MM2H_DT_PTR:      %016llx\n", cpu_to_be64(read64(MM2H_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_SKA:
+				PRINTKE("   SKA2H_CH_STATUS:   %08x\n",    cpu_to_be32(read32(SKA2H_CH_STATUS(ctx))));
+				PRINTKE("   SKA2H_LAST_HDR:    %016llx\n", cpu_to_be64(read64(SKA2H_LAST_HDR(ctx))));
+				PRINTKE("   SKA2H_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(SKA2H_BUFF_PTR(ctx))));
+				PRINTKE("   SKA2H_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(SKA2H_DT_CTRL_REG(ctx))));
+				PRINTKE("   SKA2H_DT_PTR:      %016llx\n", cpu_to_be64(read64(SKA2H_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_SKB:
+				PRINTKE("   SKB2H_CH_STATUS:   %08x\n",    cpu_to_be32(read32(SKB2H_CH_STATUS(ctx))));
+				PRINTKE("   SKB2H_LAST_HDR:    %016llx\n", cpu_to_be64(read64(SKB2H_LAST_HDR(ctx))));
+				PRINTKE("   SKB2H_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(SKB2H_BUFF_PTR(ctx))));
+				PRINTKE("   SKB2H_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(SKB2H_DT_CTRL_REG(ctx))));
+				PRINTKE("   SKB2H_DT_PTR:      %016llx\n", cpu_to_be64(read64(SKB2H_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_MCPU:
+				PRINTKE("   M2H_CH_STATUS:   %08x\n",    cpu_to_be32(read32(M2H_CH_STATUS(ctx))));
+				PRINTKE("   M2H_LAST_HDR:    %016llx\n", cpu_to_be64(read64(M2H_LAST_HDR(ctx))));
+				PRINTKE("   M2H_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(M2H_BUFF_PTR(ctx))));
+				PRINTKE("   M2H_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(M2H_DT_CTRL_REG(ctx))));
+				PRINTKE("   M2H_DT_PTR:      %016llx\n", cpu_to_be64(read64(M2H_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_SSP:
+				PRINTKE("   S2H_CH_STATUS:   %08x\n",    cpu_to_be32(read32(S2H_CH_STATUS(ctx))));
+				PRINTKE("   S2H_LAST_HDR:    %016llx\n", cpu_to_be64(read64(S2H_LAST_HDR(ctx))));
+				PRINTKE("   S2H_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(S2H_BUFF_PTR(ctx))));
+				PRINTKE("   S2H_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(S2H_DT_CTRL_REG(ctx))));
+				PRINTKE("   S2H_DT_PTR:      %016llx\n", cpu_to_be64(read64(S2H_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_FA:
+				PRINTKE("   FA2H_CH_STATUS:   %08x\n",    cpu_to_be32(read32(FA2H_CH_STATUS(ctx))));
+				PRINTKE("   FA2H_LAST_HDR:    %016llx\n", cpu_to_be64(read64(FA2H_LAST_HDR(ctx))));
+				PRINTKE("   FA2H_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(FA2H_BUFF_PTR(ctx))));
+				PRINTKE("   FA2H_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(FA2H_DT_CTRL_REG(ctx))));
+				PRINTKE("   FA2H_DT_PTR:      %016llx\n", cpu_to_be64(read64(FA2H_DT_PTR(ctx))));
+				break;
+			case DMA_CH_WR_FB:
+				PRINTKE("   FB2H_CH_STATUS:   %08x\n",    cpu_to_be32(read32(FB2H_CH_STATUS(ctx))));
+				PRINTKE("   FB2H_LAST_HDR:    %016llx\n", cpu_to_be64(read64(FB2H_LAST_HDR(ctx))));
+				PRINTKE("   FB2H_BUFF_PTR:    %016llx\n", cpu_to_be64(read64(FB2H_BUFF_PTR(ctx))));
+				PRINTKE("   FB2H_DT_CTRL_REG: %016llx\n", cpu_to_be64(read64(FB2H_DT_CTRL_REG(ctx))));
+				PRINTKE("   FB2H_DT_PTR:      %016llx\n", cpu_to_be64(read64(FB2H_DT_PTR(ctx))));
+				break;
+		}
+	}
+}
diff --git drivers/misc/ibm4767/ioctl.c drivers/misc/ibm4767/ioctl.c
new file mode 100755
index 000000000000..69a8eeb9ab10
--- /dev/null
+++ drivers/misc/ibm4767/ioctl.c
@@ -0,0 +1,864 @@
+/*************************************************************************
+ *  Filename:ioctl.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0))
+#include <linux/signal.h>
+#else
+#include <linux/sched/signal.h>
+#endif
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_ioctl.h"
+#include "host_err.h"
+#include "driver.h"
+#include "y_funcs.h"
+#include "xc_host.h"   // FIXME - for xcHdwVer_t and xcHdwTmpr_t
+
+#if defined(ENABLE_MFG)
+int ibm4767_ioctl_ssp_mbx_hist(struct file *fp, unsigned long *arg);
+int ibm4767_ioctl_mcpu_mbx_hist(struct file *fp, unsigned long *arg);
+#endif
+
+
+#if defined(YC_USE_COMPAT_IOCTL)
+long 
+ibm4767_compat_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{ 
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// even though the request structure automagically adjusts the padding to 
+	// account for various word sizes, the padding is on the wrong side of the
+	// address fields for LE arch.  this means we cannot simply copy_to|from_user
+	// from a 32-bit user app into the 64-bit driver.  too bad.
+	// 
+	if (cmd == IOCTL_IBM4767_REQUEST) {
+		xcRB32_t rb32; 
+		xcRB_t   rb; 
+		int      rc;
+
+        
+		if (copy_from_user(&rb32, (char *)arg, sizeof(rb32))) { 
+			PRINTKW("copy_from_user compat (xcRB32_t) failed\n"); 
+			return HOST_DD_BadAddress; 
+		}
+        
+		memcpy(&rb, &rb32, sizeof(rb));
+        
+		rb.RequestControlBlkAddr = (char *)(unsigned long)rb32.RequestControlBlkAddr; 
+		rb.RequestDataAddress    = (char *)(unsigned long)rb32.RequestDataAddress; 
+		rb.ReplyControlBlkAddr   = (char *)(unsigned long)rb32.ReplyControlBlkAddr; 
+		rb.ReplyDataAddr         = (char *)(unsigned long)rb32.ReplyDataAddr;
+        
+		rc = ibm4767_ioctl_request(fp, &rb); 
+        
+		rb32.Status      = rb.Status; 
+		rb32.UserDefined = rb.UserDefined;
+        
+		if (copy_to_user((char *)arg, &rb32, sizeof(rb32))) { 
+			PRINTKW("copy_to_user compat (xcRB32_t) failed\n"); 
+			return HOST_DD_BadAddress; 
+		}
+        
+		return rc; 
+	}
+    
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+	return ibm4767_ioctl(fp->f_dentry->d_inode, fp, cmd, arg);
+#else
+	return ibm4767_ioctl(fp, cmd, arg);
+#endif
+}
+#endif
+
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+int 
+ibm4767_ioctl(struct inode *inode, struct file *fp, unsigned int cmd, unsigned long parm1)
+#else
+long
+ibm4767_ioctl(struct file *fp, unsigned int cmd, unsigned long parm1)
+#endif
+{
+	struct priv_data_t *priv = fp->private_data;
+	ibm4767_ctx_t *ctx  = priv->ctx;
+	void *arg = (void *)parm1;
+	int   rc  = 0; 
+
+	if (_IOC_TYPE(cmd) != XC_MAGIC_NR) { 
+		PRINTK("cmd 0x%08X contains a bad type\n", cmd); 
+		return -ENOTTY; 
+	}
+
+	atomic_inc(&ctx->ioctl_counter);
+
+	switch(cmd) { 
+		case IOCTL_IBM4767_OPEN: 
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_OPEN\n", ctx->dev_index);
+			return ibm4767_ioctl_open(fp); 
+    
+		case IOCTL_IBM4767_REQUEST: 
+			{ 
+				xcRB_t  rb;
+          
+				PDEBUG(1, "Device %d: IOCTL_IBM4767_REQUEST\n", ctx->dev_index);
+
+				if (!priv->is_open) {
+					PRINTK("Device %d: REQUEST but not OPEN\n", ctx->dev_index); 
+					return HOST_DD_NotPermitted; 
+				}
+
+				if (copy_from_user(&rb, arg, sizeof(rb))) { 
+					PRINTKW("copy_from_user(xcRB_t) failed\n"); 
+					return HOST_DD_BadAddress; 
+				}
+
+				rc = ibm4767_ioctl_request(fp, &rb); 
+
+#if defined(DEBUG)
+				hex_dump(ctx, 2, "xcRB_t after request", &rb, sizeof(rb));
+#endif
+
+				PDEBUG(2, "AgentID:                  %04x\n", rb.AgentID);
+				PDEBUG(2, "UserDefined:              %08x\n", rb.UserDefined);
+				PDEBUG(2, "RequestControlBlkLength:  %08x\n", rb.RequestControlBlkLength);
+				PDEBUG(2, "RequestDataLength:        %08x\n", rb.RequestDataLength);
+				PDEBUG(2, "ReplyControlBlkLength:    %08x\n", rb.ReplyControlBlkLength);
+				PDEBUG(2, "ReplyDataLength:          %08x\n", rb.ReplyDataLength);
+				PDEBUG(2, "PriorityWindow:           %04x\n", rb.PriorityWindow);
+				PDEBUG(2, "Status:                   %08x\n", rb.Status);
+				
+				if (copy_to_user(arg, &rb, sizeof(rb))) { 
+					PRINTKW("copy_to_user(xcRB_t) failed\n"); 
+					return HOST_DD_BadAddress; 
+				} 
+			} 
+			break;
+
+		case IOCTL_IBM4767_GETCOUNT: 
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_GETCOUNT\n", ctx->dev_index);
+			if (copy_to_user(arg, &ibm4767_count, _IOC_SIZE(cmd))) 
+				return HOST_DD_BadAddress; 
+			break;
+
+		case IOCTL_IBM4767_GETVPD: 
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_GETVPD\n", ctx->dev_index);
+			return ibm4767_ioctl_getvpd(fp, arg); 
+    
+		case IOCTL_IBM4767_RESET:
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_RESET\n", ctx->dev_index);
+			return ibm4767_ioctl_reset(fp);
+
+		case IOCTL_IBM4767_GETHWINFO: 
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_GETHWINFO\n", ctx->dev_index);
+			if (copy_to_user(arg, &ctx->hwinfo, sizeof(xcHWInfo_t)))
+				return HOST_DD_BadAddress;
+			break;
+  
+		case IOCTL_IBM4767_GETHDWTMPR: 
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_GETHDWTMPR\n", ctx->dev_index);
+			return ibm4767_ioctl_gethdwtmpr(fp, arg); 
+
+#if defined(ENABLE_MFG)
+		case IOCTL_IBM4767_SSP_MBX_HIST:
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_SSP_MBX_HIST\n", ctx->dev_index);
+			return ibm4767_ioctl_ssp_mbx_hist(fp, arg);
+
+		case IOCTL_IBM4767_MCPU_MBX_HIST:
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_MCPU_MBX_HIST\n", ctx->dev_index);
+			return ibm4767_ioctl_mcpu_mbx_hist(fp, arg);
+#endif
+
+		case IOCTL_IBM4767_OBSERVER:
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_OBSERVER\n", ctx->dev_index);
+			return ibm4767_ioctl_observer(fp, arg); 
+
+		case IOCTL_IBM4767_MBX_WRITE:
+#if defined(ENABLE_MFG)
+			PDEBUG(1, "Device %d: IOCTL_IBM4767_MBX_WRITE\n", ctx->dev_index);
+			return ibm4767_ioctl_mbx_write(fp, arg);
+#else
+			return HOST_DD_BadRequest;
+#endif
+
+		case IOCTL_IBM4767_BAR_READ:
+#if defined(ENABLE_MFG)
+			{
+				xcBarRB_t rb;
+				PDEBUG(1, "Device %d: IOCTL_IBM4767_BAR_READ\n", ctx->dev_index);
+
+				if (copy_from_user(&rb, arg, sizeof(rb))) {
+					PRINTKW("copy_from_user(xcBarRB_t) failed\n");
+					return HOST_DD_BadAddress;
+				}
+				
+				rc = ibm4767_ioctl_bar_read(fp, &rb);
+
+				if (copy_to_user(arg, &rb, sizeof(rb))) {
+					PRINTKW("copy_to_user(xcBarRB_t) failed\n");
+					return HOST_DD_BadAddress;
+				}
+			}
+			break;
+#else
+			return HOST_DD_BadRequest;
+#endif
+		
+		case IOCTL_IBM4767_BAR_WRITE:
+#if defined(ENABLE_MFG)
+			{
+				xcBarRB_t rb;
+				PDEBUG(1, "Device %d: IOCTL_IBM4767_BAR_WRITE\n", ctx->dev_index);
+				
+				if (copy_from_user(&rb, arg, sizeof(rb))) {
+					PRINTKW("copy_from_user(xcBarRB_t) failed\n");
+					return HOST_DD_BadAddress;
+				}
+				
+				rc = ibm4767_ioctl_bar_write(fp, &rb);
+
+				if (copy_to_user(arg, &rb, sizeof(rb))) {
+					PRINTKW("copy_to_user(xcBarRB_t) failed\n");
+					return HOST_DD_BadAddress;
+				}
+			}
+			break;
+#else
+			return HOST_DD_BadRequest;
+#endif
+
+		default:  
+			PDEBUG(1, "Device %d: cmd 0x%08X contains an invalid ioctl code\n", ctx->dev_index, cmd); 
+	
+			PDEBUG(1, "Device %d: cmd 0x%08X: dir %s, size 0x%04X, type 0x%02X, nr 0x%02X\n", 
+					ctx->dev_index, cmd, !_IOC_DIR(cmd) ? "XX" : 
+					((_IOC_DIR(cmd) == (_IOC_READ|_IOC_WRITE)) ? "RW" : 
+					 ((_IOC_DIR(cmd) == _IOC_READ) ? "RR" : "WW")), 
+					_IOC_SIZE(cmd), _IOC_TYPE(cmd), _IOC_NR(cmd)); 
+			return -ENOTTY; 
+	}
+  
+	return rc;
+}
+
+
+int
+ibm4767_ioctl_open(struct file *fp)
+{ 
+	struct priv_data_t  *priv = fp->private_data;
+	ibm4767_ctx_t       *ctx  = priv->ctx;
+	int                  rc   = HOST_DD_Good;
+
+	
+	spin_lock_bh(&ctx->counter_lock);
+
+	if (ctx->status_flags & FLAG_MANUAL_DISABLED) {
+		PRINTKW("Device %d: OPEN failed.  Device has been manually disabled.  Try later.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		rc = HOST_DD_DeviceBusy;
+		goto done;
+	}
+
+	if (ctx->status_flags & FLAG_LOCKED) {
+		PRINTKW("Device %d: OPEN failed.  Critical command active.  Try later.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		rc = HOST_DD_DeviceBusy;
+		goto done;
+	}
+
+	// while we have the spinlock, might as well check for a pending reset though the lock
+	// isn't necessary
+	//
+	if (ctx->status_flags & (FLAG_MAIN_RESET_PENDING | FLAG_MAIN_RESET_ACTIVE)) {
+		PRINTKW("Device %d: OPEN failed.  Main reset %s.  Try later.\n", 
+				ctx->dev_index, 
+				((ctx->status_flags & FLAG_MAIN_RESET_PENDING) ? "pending" : "active"));
+		atomic_inc(&ctx->ebusy_counter);
+		rc = HOST_DD_DeviceBusy;
+		goto done;
+	}
+
+	rc = ibm4767_check_tamper(ctx, "OPEN");
+	if (rc != HOST_DD_Good) 
+		goto done;
+
+	priv->is_open = 1;
+
+done:	
+	spin_unlock_bh(&ctx->counter_lock);
+	return rc;
+}
+
+
+int
+ibm4767_ioctl_request(struct file *fp, xcRB_t *pRB)
+{ 
+	struct priv_data_t  *priv = fp->private_data;
+	ibm4767_ctx_t       *ctx  = priv->ctx;
+	int  rc = HOST_DD_Good;
+
+
+	spin_lock_bh(&ctx->counter_lock);
+	if (ctx->status_flags & FLAG_MANUAL_DISABLED) {
+		PRINTKW("Device %d: REQUEST failed.  Device has been manually disabled.\n", ctx->dev_index);
+		rc = HOST_DD_NoDevice;
+	}
+	else if (ctx->status_flags & (FLAG_MAIN_RESET_PENDING | FLAG_MAIN_RESET_ACTIVE)) {
+		if (ctx->ebusy_resetmsg_count < REQFAIL_MSG_COUNT) {
+			PRINTKW("Device %d: REQUEST failed.  Main reset %s.  Try later.\n", 
+				ctx->dev_index, 
+				((ctx->status_flags & FLAG_MAIN_RESET_PENDING) ? "pending" : "active"));
+
+			if (++ctx->ebusy_resetmsg_count == REQFAIL_MSG_COUNT) {
+				PRINTKW("Device %d: Fail threshhold reached.  Message disabled for this device until reset completes.\n", ctx->dev_index);
+			}
+		}
+		atomic_inc(&ctx->ebusy_counter);
+		rc = HOST_DD_DeviceBusy;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+
+	if (rc != HOST_DD_Good)
+		return rc;
+
+	rc = ibm4767_check_tamper(ctx, "REQUEST");
+	if (rc != HOST_DD_Good) 
+		return rc;
+
+	switch (pRB->AgentID) {
+		case XC_AGENTID_MB0:
+		case XC_AGENTID_MB1:
+			priv->need_ssp_reset = 1;
+			rc = special_request_mb(ctx, pRB);
+			break;
+
+#if defined(ENABLE_MFG)
+		case XC_AGENTID_POST0:
+		case XC_AGENTID_POST1:
+		case XC_AGENTID_IBM_TAS_SSP:
+		case XC_AGENTID_IBM_TAPP_SSP:
+			// do not set 'need_ssp_reset'...this will break FTE...
+			rc = special_request_ssp(ctx, pRB);
+			break;
+
+		case XC_AGENTID_POST2:
+		case XC_AGENTID_IBM_TAS_MCPU:
+		case XC_AGENTID_IBM_TAPP_MCPU:
+			rc = special_request_mcpu(ctx, pRB);
+			break;
+#endif
+		
+		case XC_AGENTID_SKCH_FASTPATH:
+		case XC_AGENTID_PKA_FASTPATH:
+		case XC_AGENTID_FPGAA_FASTPATH:
+		case XC_AGENTID_FPGAB_FASTPATH:
+			rc = fastpath_request(ctx, pRB);
+			break;
+
+		default:
+			ibm4767_request_throttle(ctx);
+			rc = mcpu_request(ctx, pRB);
+			break;
+	}
+
+	if (rc != HOST_DD_NoDevice)
+		priv->req_sent = 1;
+	
+	return rc;
+}
+
+
+int
+ibm4767_ioctl_getvpd(struct file *fp, unsigned long *arg)
+{
+	struct priv_data_t *priv = fp->private_data;
+	xcVpd_t  vpd;
+	int      rc;
+
+	rc = ibm4767_query_vpd(priv->ctx, &vpd);
+	if (rc == HOST_DD_Good) {
+		if (copy_to_user(arg, &vpd, sizeof(xcVpd_t))) 
+			rc = HOST_DD_BadAddress;
+	}
+  
+	return rc;
+}
+
+
+#if defined(ENABLE_MFG)
+int
+ibm4767_ioctl_mbx_write(struct file *fp, xcMbxRB_t *pRB)
+{
+	struct priv_data_t *priv = fp->private_data;
+	ibm4767_ctx_t      *ctx  = priv->ctx;
+	request_t          *pReq = NULL;
+	char               *err_txt = NULL;
+	request_wait_t      rwt;
+	int                 rc = HOST_DD_Good;
+
+
+	spin_lock_bh(&ctx->counter_lock);
+	if (ctx->status_flags & FLAG_MANUAL_DISABLED) {
+		spin_unlock_bh(&ctx->counter_lock);
+		PRINTKW("Device %d: MBX WRITE failed.  Device has been manually disabled.  Try later.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	// don't knowingly start handling a request if a reset is pending or active.  you'll note
+	// that we don't bother grabbing the counter_lock here.  
+	//
+	if (ctx->status_flags & (FLAG_MAIN_RESET_PENDING | FLAG_MAIN_RESET_ACTIVE)) {
+		spin_unlock_bh(&ctx->counter_lock);
+		PRINTKW("Device %d: MBX WRITE failed.  Main reset %s.  Try later.\n", 
+				ctx->dev_index, 
+				((ctx->status_flags & FLAG_MAIN_RESET_PENDING) ? "pending" : "active"));
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+
+	rc = ibm4767_check_tamper(ctx, "MBX WRITE");
+	if (rc != HOST_DD_Good) 
+		return rc;
+
+	spin_lock_bh(&ctx->counter_lock);
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST1:
+			if (ctx->active_p1) 
+				err_txt = "POST1";
+			else
+				ctx->active_p1++;
+			break;
+		case XC_AGENTID_IBM_TAS_SSP:
+			if (ctx->active_ssp_tas) 
+				err_txt = "TAS SSP";
+			else
+				ctx->active_ssp_tas++;
+			break;
+		case XC_AGENTID_IBM_TAPP_SSP:
+			if (ctx->active_ssp_tapp) 
+				err_txt = "TAPP SSP";
+			else
+				ctx->active_ssp_tapp++;
+			break;
+		case XC_AGENTID_POST2:
+			if (ctx->active_p2) 
+				err_txt = "POST2";
+			else
+				ctx->active_p2++;
+			break;
+		case XC_AGENTID_IBM_TAS_MCPU:
+			if (ctx->active_mcpu_tas) 
+				err_txt = "TAS MCPU";
+			else
+				ctx->active_mcpu_tas++;
+			break;
+		case XC_AGENTID_IBM_TAPP_MCPU:
+			if (ctx->active_mcpu_tapp) 
+				err_txt = "TAPP MCPU";
+			else
+				ctx->active_mcpu_tapp++;
+			break;
+		default:
+			spin_unlock_bh(&ctx->counter_lock);
+			PRINTKW("Attempt to send MBX WRITE to invalid AgentID: 0x%x\n", pRB->AgentID);
+			rc = HOST_DD_InvalidParm;
+			goto done;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+
+	if (err_txt) {
+		PRINTKW("Device %d: MBX WRITE for %s already active.  Mailbox not written\n", ctx->dev_index, err_txt);
+		atomic_inc(&ctx->ebusy_counter);
+		rc = HOST_DD_DeviceBusy;
+		goto done;
+	}
+
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST1:
+			rc = request_allocate(ctx, REQUEST_SSP, &rwt);
+			if (rc != HOST_DD_Good)
+				goto done;
+			ctx->p1_req = pReq = rwt.pReq;
+			rc = special_request_post1_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAS_SSP:
+			rc = request_allocate(ctx, REQUEST_SSP, &rwt);
+			if (rc != HOST_DD_Good)
+				goto done;
+			ctx->ssp_tas_req = pReq = rwt.pReq;
+			rc = special_request_ssp_tas_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAPP_SSP:
+			rc = request_allocate(ctx, REQUEST_SSP, &rwt);
+			if (rc != HOST_DD_Good)
+				goto done;
+			ctx->ssp_tapp_req = pReq = rwt.pReq;
+			rc = special_request_ssp_tapp_pre(ctx, pReq);
+			break;
+		
+		case XC_AGENTID_POST2:
+			rc = request_allocate(ctx, REQUEST_MCPU, &rwt);
+			if (rc != HOST_DD_Good)
+				goto done;
+			ctx->p2_req = pReq = rwt.pReq;
+			rc = special_request_post2_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAS_MCPU:
+			rc = request_allocate(ctx, REQUEST_MCPU, &rwt);
+			if (rc != HOST_DD_Good)
+				goto done;
+			ctx->mcpu_tas_req = pReq = rwt.pReq;
+			rc = special_request_mcpu_tas_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAPP_MCPU:
+			rc = request_allocate(ctx, REQUEST_MCPU, &rwt);
+			if (rc != HOST_DD_Good)
+				goto done;
+			ctx->mcpu_tapp_req = pReq = rwt.pReq;
+			rc = special_request_mcpu_tapp_pre(ctx, pReq);
+			break;
+	}
+
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: MBX REQUEST state transition failure\n", ctx->dev_index);
+		goto done;
+	}
+
+	// did someone signal?
+	//
+	if (signal_pending(current)) {
+		PRINTKW("Device %d: %s woke up due to signal...aborting...\n", ctx->dev_index, __FUNCTION__);
+		rc = HOST_DD_Interrupted;
+		goto done;
+	}
+
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST1:
+		case XC_AGENTID_IBM_TAS_SSP:
+		case XC_AGENTID_IBM_TAPP_SSP:
+			PDEBUG(2, "Device %d: Writing 0x%016llx to H2S MBX\n", ctx->dev_index, pRB->mbx);
+			write64(cpu_to_be64(pRB->mbx), H2S_MBX(ctx));
+			break;
+
+		case XC_AGENTID_POST2:
+		case XC_AGENTID_IBM_TAS_MCPU:
+		case XC_AGENTID_IBM_TAPP_MCPU:
+			PDEBUG(2, "Device %d: Writing 0x%016llx to H2M MBX\n", ctx->dev_index, pRB->mbx);
+			write64(cpu_to_be64(pRB->mbx), H2M_MBX(ctx));
+			break;
+	}
+
+done:
+	if (pReq) 
+		request_release(ctx, pReq);
+
+	spin_lock_bh(&ctx->counter_lock);
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST1:
+			ctx->active_p1--;
+			ctx->p1_req = NULL;
+			break;
+		case XC_AGENTID_IBM_TAS_SSP:
+			ctx->active_ssp_tas--;
+			ctx->ssp_tas_req = NULL;
+			break;
+		case XC_AGENTID_IBM_TAPP_SSP:
+			ctx->active_ssp_tapp--;
+			ctx->ssp_tapp_req = NULL;
+			break;
+		case XC_AGENTID_POST2:
+			ctx->active_p2--;
+			ctx->p2_req = NULL;
+			break;
+		case XC_AGENTID_IBM_TAS_MCPU:
+			ctx->active_mcpu_tas--;
+			ctx->mcpu_tas_req = NULL;
+			break;
+		case XC_AGENTID_IBM_TAPP_MCPU:
+			ctx->active_mcpu_tapp--;
+			ctx->mcpu_tapp_req = NULL;
+			break;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+  
+	return rc;
+}
+#endif
+
+
+int
+ibm4767_ioctl_reset(struct file *fp)
+{ 
+	struct priv_data_t  *priv = fp->private_data;
+	ibm4767_ctx_t       *ctx  = priv->ctx;
+
+	// there are two ways to manually reset the device.  this IOCTL is the 'polite' way
+	// in that it insists on exclusive access to the device.  the other way is via procfs
+	// which performs an immediate reset, aborting all active requests.
+
+	// we'll use the spinlock here to mutually-exclude the open ioctl...
+	//
+	spin_lock_bh(&ctx->counter_lock);
+
+	if (ctx->active_opens > 1) {
+		spin_unlock_bh(&ctx->counter_lock);
+		PRINTKW("Device %d: RESET attempt requires exclusive access to the device\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	ctx->status_flags |= FLAG_MAIN_RESET_PENDING;
+
+	PRINTKW("Device %d: RESET via ioctl\n", ctx->dev_index);
+	sev0_purge(ctx);
+
+	spin_unlock_bh(&ctx->counter_lock);
+
+	return ibm4767_main_reset(ctx, TRUE);
+}
+
+
+int
+ibm4767_ioctl_gethdwtmpr(struct file *fp, unsigned long *arg)
+{ 
+	// in the 4764 universe, we'd just read the tamper bits from HTmpR.
+	//
+	// there's no straightforward analogue in the 4765 universe since tamper
+	// status is spread across several registers (RMSR1, HRCSR and some
+	// indirect regs).  i could assemble individual bits into 
+	// xcHdwTmpr_t::tmprbits but a better solution is to replace xcHdwTmpr_t
+	// with something more up to date...
+	//
+	// I don't think anybody uses this routine anyway...
+	
+	return HOST_DD_BadRequest;
+}
+
+
+#if defined(ENABLE_MFG)
+int
+ibm4767_ioctl_ssp_mbx_hist(struct file *fp, unsigned long *arg)
+{
+	struct priv_data_t *priv = fp->private_data;
+	ibm4767_ctx_t        *ctx  = priv->ctx;
+	xcMbxHist_t        *hist = (xcMbxHist_t *)arg;
+	uint64_t           *src  = NULL;
+	uint32_t            count = 0, r, w;
+	int                 rc = 0;
+
+	spin_lock_bh(&ctx->ssp_mbxhist_lock);
+
+	src = ctx->ssp_mbxhist_list;
+	r   = ctx->ssp_mbxhist_r_idx;
+	w   = ctx->ssp_mbxhist_w_idx;
+	count = w - r;
+
+	if (r == w) {
+		// nothing to copy
+	}
+	else if (w > r) {
+		if (copy_to_user(hist->mbx, &src[r], count * sizeof(uint64_t))) {
+			rc = HOST_DD_BadAddress; 
+			goto error;
+		}
+	
+		ctx->ssp_mbxhist_r_idx = ctx->ssp_mbxhist_w_idx;
+	}
+	else if (r > w) {
+		// wrapped...so copy in two steps
+		if (copy_to_user(hist->mbx, &src[r], (MAILBOX_HISTORY_LEN - r) * sizeof(uint64_t))) {
+			rc = HOST_DD_BadAddress;
+			goto error;
+		}
+		
+		if (copy_to_user(&hist->mbx[MAILBOX_HISTORY_LEN - r], src, w * sizeof(uint64_t)))  {
+			rc = HOST_DD_BadAddress;
+			goto error;
+		}
+
+		count = (MAILBOX_HISTORY_LEN - r) + w;
+		ctx->ssp_mbxhist_r_idx = ctx->ssp_mbxhist_w_idx;
+	}
+
+	if (copy_to_user(&hist->num, &count, sizeof(count)))
+		rc = HOST_DD_BadAddress;
+
+error:
+	spin_unlock_bh(&ctx->ssp_mbxhist_lock);
+	return rc;
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+int
+ibm4767_ioctl_mcpu_mbx_hist(struct file *fp, unsigned long *arg)
+{
+	struct priv_data_t *priv = fp->private_data;
+	ibm4767_ctx_t        *ctx  = priv->ctx;
+	xcMbxHist_t        *hist = (xcMbxHist_t *)arg;
+	uint64_t           *src  = NULL;
+	uint32_t            count = 0, r, w;
+	int                 rc = 0;
+
+	spin_lock_bh(&ctx->mcpu_mbxhist_lock);
+
+	src = ctx->mcpu_mbxhist_list;
+	r   = ctx->mcpu_mbxhist_r_idx;
+	w   = ctx->mcpu_mbxhist_w_idx;
+	count = w - r;
+
+	if (r == w) {
+		// nothing to copy
+	}
+	else if (w > r) {
+		if (copy_to_user(hist->mbx, &src[r], count * sizeof(uint64_t))) {
+			rc = HOST_DD_BadAddress; 
+			goto error;
+		}
+		
+		ctx->mcpu_mbxhist_r_idx = ctx->mcpu_mbxhist_w_idx;
+	}
+	else {
+		// wrapped...so copy in two steps
+		if (copy_to_user(hist->mbx, &src[r], (MAILBOX_HISTORY_LEN - r) * sizeof(uint64_t))) {
+			rc = HOST_DD_BadAddress;
+			goto error;
+		}
+
+		if (copy_to_user(&hist->mbx[MAILBOX_HISTORY_LEN - r], src, w * sizeof(uint64_t))) {
+			rc = HOST_DD_BadAddress;
+			goto error;
+		}
+
+		count = (MAILBOX_HISTORY_LEN - r) + w;
+		ctx->mcpu_mbxhist_r_idx = ctx->mcpu_mbxhist_w_idx;
+	}
+	
+	if (copy_to_user(&hist->num, &count, sizeof(count)))
+		rc = HOST_DD_BadAddress;
+
+error:
+	spin_unlock_bh(&ctx->mcpu_mbxhist_lock);
+	return rc;
+}
+#endif
+
+#if defined(ENABLE_MFG)
+int
+ibm4767_ioctl_bar_read(struct file *fp, xcBarRB_t *pRB)
+{
+	struct priv_data_t *priv = fp->private_data;
+	ibm4767_ctx_t      *ctx  = priv->ctx;
+
+	// not going to do much sanity-checking on this data...be careful!
+	//
+	pRB->value = cpu_to_be64(read64(ctx->io_va + pRB->offset));
+
+	return HOST_DD_Good;
+}
+#endif
+
+#if defined(ENABLE_MFG)
+int
+ibm4767_ioctl_bar_write(struct file *fp, xcBarRB_t *pRB)
+{
+	struct priv_data_t *priv = fp->private_data;
+	ibm4767_ctx_t      *ctx  = priv->ctx;
+
+	// not going to do much sanity-checking on this data...be careful!
+	//
+	write64(cpu_to_be64(pRB->value), ctx->io_va + pRB->offset);
+
+	return HOST_DD_Good;
+}
+#endif
+
+
+
+
+int
+ibm4767_ioctl_observer(struct file *fp, xcCritEvent_t *arg)
+{
+	struct priv_data_t  *priv = fp->private_data;
+	ibm4767_ctx_t       *ctx  = priv->ctx;
+	observer_t          *pObs = NULL;
+	xcCritEvent_t        evt;
+	int                  rc = HOST_DD_Good;
+
+	pObs = event_observer_allocate(ctx);
+	if (!pObs) {
+		PRINTKW("Device %d: Too many observers\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	wait_event_interruptible(pObs->waitQ, atomic_read(&pObs->waitEvent));
+	if (signal_pending(current)) {
+		rc = HOST_DD_Interrupted;
+		goto done;
+	}
+		
+	evt.event = pObs->event_code;
+
+	if (copy_to_user(arg, &evt, sizeof(evt))) {
+		PRINTKW("copy_to_user(xcCritEvent_t) failed\n"); 
+		rc = HOST_DD_BadAddress; 
+	}
+
+done:
+	event_observer_release(pObs);
+	return rc;
+}
+
diff --git drivers/misc/ibm4767/lib4767_gpl.c drivers/misc/ibm4767/lib4767_gpl.c
new file mode 100755
index 000000000000..859ab08b3192
--- /dev/null
+++ drivers/misc/ibm4767/lib4767_gpl.c
@@ -0,0 +1,426 @@
+/*
+ * Filename: lib4767_gpl.c
+ *
+ * IBM 4767 xSeries Host device driver - host interface
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+
+
+#include <stdio.h>
+#include <sys/ioctl.h>
+#include <unistd.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <linux/types.h>
+#include <errno.h>
+#include <fcntl.h>
+
+#include "y_hostRB.h"
+#include "y_ioctl.h"
+#include "xc_host.h"
+#include "host_err.h"
+
+#ifdef LINUX_ON_390
+//   #include <linux/compiler.h>
+   #ifndef __user
+     #define __user
+   #endif
+   #include "zcrypt.h"
+   #define xcRB ica_xcRB
+   #define OPENMODE O_RDONLY
+   #define DEVICE "/dev/z90crypt"
+#else
+   #define OPENMODE O_RDWR
+   #define DEVICE "/dev/ibm4767/"
+#endif
+
+#include <string.h>
+#ifdef LINUX_ON_390
+ #define CARD_NUMBER 64
+ #define CEX3C_ID    0x07
+ #define CEX2C_ID    0x05
+#endif
+
+
+#if 0
+static void
+hex_dump(char *descr, void *data, uint32_t len)
+{
+	char *p = NULL, *buf = (char *)data;
+	char out[128];
+	uint32_t i, j;
+
+	if (descr) { 
+		printf("%s\n", descr);
+	}
+
+	if (len == 0) {
+		printf("EMPTY BLOCK\n");
+	}
+
+
+	i = 0;
+	while (i < len) {
+		memset(out, 0, sizeof(out));
+		sprintf(out, "%04x: ", i);
+		p = out+6;
+
+		for (j=0; (j < 16) && (i < len); j++, i++) {
+			sprintf(p, "%02X", (unsigned char)buf[i]);
+			p += 2;
+			if ((j & 3) == 3)
+				sprintf(p++, " ");
+		}
+
+		printf("%s\n", out);
+	}
+}
+#endif
+
+
+
+// maps an operating system error (typically errno) into a crypto error
+//
+static uint32_t
+mapSysErr(int rc)
+{ 
+	uint32_t urc;
+
+	switch (rc) { 
+		case 0: 
+			return 0; 
+        
+		case -1: 
+			// POSIX-style return code
+
+			// experience has shown that badly-written host apps 
+			// simply immediately retry a request if the previous 
+			// one returns EBUSY.  this can cause a tremendous load 
+			// so we make an app wait at least 10ms before allowing 
+			// a retry
+			// 
+			if (errno == EBUSY)  {
+				usleep(10*1000);
+
+				// usleep might modify errno so we can't reuse it here
+				return HOSTDeviceBusy;
+			}
+
+			switch (errno) {
+				// some kernels incorrectly return ENODEV instead of ENXIO.
+				// in addition, we'll see ENOENT if the /dev node doesn't
+				// exist (driver not loaded, etc)
+				case ENODEV:
+				case ENXIO:
+				case ENOENT:
+					return HOSTNoDevice;
+
+				default:
+					return HOST_DD_ERR | (errno & 0xFFFF);
+			}
+
+		default:
+			urc = (uint32_t)rc;
+
+			if ((urc == HOST_DD_DeviceBusy) || (urc == HOSTDeviceBusy))
+				usleep(10*1000);
+
+			if (urc < 0xFFFF)
+				return HOST_DD_ERR | urc;
+
+			return urc;
+	}
+}
+
+
+// open a specific adapter
+//
+// note: this merely opens the physical device
+//
+static int 
+openDevice(xcAdapterNumber_t    adapter_num, 
+	   xcAdapterHandle_t   *phHandle)
+{ 
+	int  fh; 
+	char dev_path[15];
+
+#ifndef LINUX_ON_390 
+	if (adapter_num > MAX_DEV_COUNT) { 
+		errno = EINVAL; 
+		return -1; 
+	} 
+    
+	sprintf(dev_path, "%s%X", DEVICE, adapter_num);
+    
+	//printf("Trying to open %s\n", dev_path);
+#else   
+	strcpy(dev_path, DEVICE);
+#endif 
+    
+	if ((fh = open(dev_path, OPENMODE)) < 0) 
+		return -1; 
+    
+	*phHandle = fh; 
+	return 0;
+}
+
+
+// attach to the specified crypto adapter for normal mode operations
+//
+// under the covers, this routine will open the physical device and issue the
+// necessary ioctl to attach
+//
+unsigned int 
+xcOpenAdapter(xcAdapterNumber_t  AdapterNum, 
+	      xcAdapterHandle_t *phAdapterHandle)
+{ 
+	xcAdapterHandle_t hHandle = 0; 
+	int               rc = 0; 
+    
+	if ((rc = openDevice(AdapterNum, &hHandle)) != 0) 
+		return mapSysErr(rc); 
+
+#ifndef LINUX_ON_390 
+	if ((rc = ioctl(hHandle, IOCTL_IBM4767_OPEN)) != 0) { 
+		close(hHandle); 
+		return mapSysErr(rc); 
+	}
+#endif 
+    
+	*phAdapterHandle = hHandle; 
+	return 0;
+}
+
+
+// query the driver for the number of adapters.  it may seem a little odd but
+// we have to first open an adapter in order to communicate with the driver...
+//
+// for linux, we could have just as easily had the driver create an entry in
+// /proc/drivers/ibm4767 or in /sys with the number of crypto cards but
+// that would be more difficult to port to non-POSIX systems
+//
+
+unsigned int xcAdapterCount(xcAdapterNumber_t *pAdapterCount)
+{ 
+	int rc = 0;
+
+#ifdef LINUX_ON_390 
+	/* Apr 9,2018
+	 * --remove deprecated Z90STAT_STATUS_MASK ioctl, 
+	 * --deprecate this service
+	 * --this service is replaced by
+	 *   CCA library service adapt_getAdapterCount()
+	 */
+	rc = -1;
+#else 
+	xcAdapterHandle_t   hHandle = -1; 
+	xcAdapterNumber_t   i;
+
+	// try to find a valid handle.  don't care which device.  
+	// 
+	for (i=0; i<MAX_DEV_COUNT; i++) { 
+		if ((rc = openDevice(i, &hHandle))==0) 
+			break; 
+	} 
+    
+	if (rc) 
+		return mapSysErr(rc);
+    
+	rc = ioctl(hHandle, IOCTL_IBM4767_GETCOUNT, pAdapterCount); 
+	close(hHandle);
+#endif 
+	return mapSysErr(rc);
+}
+
+
+unsigned int 
+xcCloseAdapter(xcAdapterHandle_t hAdapterHandle)
+{ 
+    return mapSysErr(close(hAdapterHandle));
+}
+
+
+
+unsigned int 
+xcRequest(xcAdapterHandle_t  hAdapterHandle, 
+	  xcRB_t *           pPacketRB)
+{ 
+#ifdef LINUX_ON_390 
+	return mapSysErr(ioctl(hAdapterHandle, ZSECSENDCPRB, pPacketRB));
+#else 
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_REQUEST, pPacketRB));
+#endif 
+}
+
+
+unsigned int 
+xcGetAdapterData(xcAdapterHandle_t hAdapterHandle, 
+		 xcVpd_t *         pVpd)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!pVpd)
+		return HOSTInvalidParm;
+    
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_GETVPD, pVpd));
+#endif
+}
+
+
+unsigned int 
+xcGetHardwareInfo(xcAdapterHandle_t hAdapterHandle, 
+		  xcHWInfo_t *      pHWInfo)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!pHWInfo)
+		return HOSTInvalidParm; 
+    
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_GETHWINFO, pHWInfo));
+#endif
+}
+
+
+unsigned int 
+xcGetTamperBits(xcAdapterHandle_t hAdapterHandle, 
+		xcHdwTmpr_t *     pHdwTamperBits)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!pHdwTamperBits)
+		return HOSTInvalidParm;
+    
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_GETHDWTMPR, pHdwTamperBits));
+#endif
+}
+
+
+unsigned int 
+xcResetAdapter(xcAdapterHandle_t hAdapterHandle)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_RESET));
+#endif
+}
+
+
+#ifdef ENABLE_MFG
+unsigned int 
+xcMailboxHistorySSP(xcAdapterHandle_t hAdapterHandle,
+		    xcMbxHist_t *     hist)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!hist)
+		return HOSTInvalidParm;
+
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_SSP_MBX_HIST, hist));
+#endif
+}
+#endif
+
+
+
+#ifdef ENABLE_MFG
+unsigned int 
+xcMailboxHistoryMCPU(xcAdapterHandle_t hAdapterHandle, 
+		     xcMbxHist_t *     hist)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!hist)
+		return HOSTInvalidParm;
+
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_MCPU_MBX_HIST, hist));
+#endif
+}
+#endif
+
+
+unsigned int
+xcCriticalEventNotifier(xcAdapterHandle_t  hAdapterHandle,
+			xcCritEvent_t *    event)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!event)
+		return HOSTInvalidParm;
+
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_OBSERVER, event));
+#endif
+}
+
+
+unsigned int
+xcMailboxWrite(xcAdapterHandle_t  hAdapterHandle,
+	       xcMbxRB_t *        req)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!req)
+		return HOSTInvalidParm;
+
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_MBX_WRITE, req));
+#endif
+}
+
+
+#ifdef ENABLE_MFG
+unsigned int 
+xcBarRead(xcAdapterHandle_t hAdapterHandle, 
+          xcBarRB_t *       req)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!req)
+		return HOSTInvalidParm;
+
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_BAR_READ, req));
+#endif
+}
+#endif
+
+#ifdef ENABLE_MFG
+unsigned int 
+xcBarWrite(xcAdapterHandle_t hAdapterHandle, 
+           xcBarRB_t *       req)
+{
+#ifdef LINUX_ON_390 
+	return mapSysErr(ENOTTY);
+#else 
+	if (!req)
+		return HOSTInvalidParm;
+
+	return mapSysErr(ioctl(hAdapterHandle, IOCTL_IBM4767_BAR_WRITE, req));
+#endif
+}
+#endif
diff --git drivers/misc/ibm4767/mailbox.c drivers/misc/ibm4767/mailbox.c
new file mode 100755
index 000000000000..f5569a2bdb9d
--- /dev/null
+++ drivers/misc/ibm4767/mailbox.c
@@ -0,0 +1,1448 @@
+/*************************************************************************
+ *  Filename:mailbox.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "xc_host.h"
+#include "y_regs.h"
+#include "driver.h"
+#include "y_funcs.h"
+#include "y_mailbox.h"
+#include "host_err.h"
+
+// this code is pretty verbose right now and there's some code duplication.  once
+// things settle down, it will be revisited and refactored into something cleaner
+//
+
+void
+handle_post0_mbx(ibm4767_ctx_t  *ctx, uint8_t hotbyte, uint32_t hi, uint32_t lo)
+{
+	uint32_t hrcsr;
+
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			PDEBUG(2, "Device %d: POST0 START received: 0x%08x version 0x%08x\n", ctx->dev_index, hi, lo);
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce |= TIMER_QUIESCE_DEFERRED_SSP_WUR;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			del_timer_sync(&ctx->ssp_timer);
+			del_timer_sync(&ctx->deferred_ssp_wakeup_timer);
+
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce &= ~TIMER_QUIESCE_DEFERRED_SSP_WUR;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			spin_lock_bh(&ctx->counter_lock);
+			ctx->status_flags &= ~FLAG_SSP_RESET_PENDING;
+			ctx->status_flags &= ~FLAG_SSP_WUR_PENDING;
+			ctx->status_flags &= ~FLAG_MBFAIL_EXPECTED;
+			spin_unlock_bh(&ctx->counter_lock);
+
+			ctx->hwinfo.post0_ver = lo;
+
+			// as I understand, the 4767 will stop generating tamper interrupts after the first reset following
+			// a tamper event.  thus, if you plug a tampered card into a machine, there's a good chance that
+			// the driver will not be notified via interrupt of the tamper condition so we'd likely time-out waiting
+			// for the card to boot...
+			//
+			hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+			if (!ignore_tamper && (hrcsr & HRCSR_PCT)) {
+				PRINTKE("TAMPER for device %d (S/N %s).  HRCSR = %08X.  Marking device offline.\n", ctx->dev_index, ctx->hwinfo.serial_num, hrcsr);
+				mark_device_offline(ctx, HOSTTamperDetected);
+				ctx->status_mcpu = MCPU_TAMPER;
+				ctx->status_ssp  = SSP_TAMPER;
+				ctx->status_tamper = TAMPER_PERM;
+				break;
+			}
+
+			ctx->status_ssp = SSP_WAIT_FOR_POST0_END;
+			mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			break;
+
+		case NOTIFICATION_HELLO:
+			// pretty sure POST0 will never send _HELLO but keep this for completeness...
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: POST0 HELLO received: 0x%08x\n", ctx->dev_index, hi);
+			dma_enable(ctx, HBMCR_BMEN_SSP | HBMCR_HTB_BMEN );
+			if (ctx->htb_active == 0xFF)
+				htb_swap(ctx);
+			break;
+
+		case NOTIFICATION_END:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: POST0 END received: 0x%08x\n", ctx->dev_index, hi);
+			dma_disable(ctx, HBMCR_BMEN_SSP);
+			ctx->status_ssp = SSP_WAIT_FOR_MB0_START;
+			mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			break;
+
+		case NOTIFICATION_FAIL:
+			del_timer_sync(&ctx->ssp_timer);
+			PRINTKW("Device %d: POST0 FAIL -- %08x:%08x\n", ctx->dev_index, hi, lo);
+			notify_observers(ctx, EVENT_POST0_ERR);
+			mark_device_offline(ctx, POSTError);
+
+			// marking the card offline will have the side effect of aborting all requests
+			// which is what we want to happen here
+			//
+			defer_main_reset(ctx, 5, FALSE);
+			break;
+
+		case NOTIFICATION_CRIT_ERROR:
+			del_timer_sync(&ctx->ssp_timer);
+			PRINTKW("Device %d: POST0 CRIT ERR -- %08x:%08x.  Obtain emergency dump\n", ctx->dev_index, hi, lo);
+			schedule_work(&ctx->post0_edump_work);
+			notify_observers(ctx, EVENT_POST0_ERR);
+			mark_ssp_offline(ctx, POSTError);
+
+			// is there any reason to expect a reset to "fix" things?
+			//
+			break;
+		
+		case NOTIFICATION_DIAG_ERROR:
+			del_timer_sync(&ctx->ssp_timer);
+			PRINTKW("Device %d: POST0 DIAG ERR -- %08x:%08x\n", ctx->dev_index, hi, lo);
+
+			if (postde_echo) {
+				spin_lock_bh(&ctx->edump_lock);
+				ctx->edump_flag |= EDUMP_FLAG_BYPASS_DE_SSP;
+				spin_unlock_bh(&ctx->edump_lock);
+			}
+			else {
+				spin_lock_bh(&ctx->edump_lock);
+				ctx->edump_flag |= EDUMP_FLAG_MARK_SSP_OFFLINE;
+				spin_unlock_bh(&ctx->edump_lock);
+			}
+
+			schedule_work(&ctx->post0_edump_work);
+			notify_observers(ctx, EVENT_POST0_ERR);
+			break;
+
+		case NOTIFICATION_INFO:
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: POST0 INFO received: 0x%08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+
+		default:
+			PDEBUG(2, "Device %d: UNKNOWN POST0 MBX: %08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+	}
+}
+
+
+void
+handle_mb0_mbx(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: MB0 START received: 0x%08x version 0x%08x\n", ctx->dev_index, hi, lo);
+			ctx->mb_round = 0;
+
+			ctx->hwinfo.mb0_ver = lo;
+			ctx->status_ssp = SSP_WAIT_FOR_MB0_HELLO;
+			mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			break;
+
+		case NOTIFICATION_HELLO:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(1, "Device %d: MB0 HELLO received: 0x%08X\n", ctx->dev_index, hi);
+			ctx->mb_round = 0;
+
+			ctx->status_ssp = SSP_MB0;
+			if (ibm4767_check_ssp_reset_pending(ctx))
+				break;
+
+			// we grab this lock because 'mb_req' might get flushed during an out-of-band reset...
+			spin_lock_bh(&ctx->req_pool_lock);
+			if (ctx->mb_req && ctx->mb_req->reqd_state == SSP_MB0) {
+				PDEBUG(2, "Device %d: Sending CONTINUE_MB_MODE\n", ctx->dev_index);
+				write32(cpu_to_be32(CONTINUE_MB_MODE), H2S_MBX_H(ctx));
+
+
+				// in the 4767 universe, MB does not respond with an ACK.  we go ahead
+				// and start the DMA (re)fetch.  the DMA will stall until MB reads it
+				// in accordance with the fragmented DMA scheme...
+				//
+				dma_enable(ctx, HBMCR_BMEN_SSP | HBMCR_HTB_BMEN);
+				if (ctx->htb_active == 0xFF)
+					htb_swap(ctx);
+			
+				PDEBUG(1, "Device %d: Waking up MB REQUEST thread...\n", ctx->dev_index);
+				atomic_set(&ctx->mb_req->waitEvent, 1);
+				wake_up(&ctx->mb_req->waitQ);
+			} 
+			else { 
+				PDEBUG(2, "Device %d: Sending CONTINUE_NORMAL\n", ctx->dev_index);
+				write32(cpu_to_be32(CONTINUE_NORMAL), H2S_MBX_H(ctx));
+				ctx->status_ssp = SSP_WAIT_FOR_MB0_END;
+				mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			} 
+			spin_unlock_bh(&ctx->req_pool_lock);
+
+			break;
+
+		case NOTIFICATION_ACK:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PRINTK("Device %d: Unexpected MB0 ACK received.  Marking SSP offline\n", ctx->dev_index);
+			mark_ssp_offline(ctx, MB0Error);
+			break;
+
+		case NOTIFICATION_END:	
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: MB0 END received\n", ctx->dev_index);
+			
+			dma_disable(ctx, HBMCR_BMEN_SSP);
+			ctx->status_ssp = SSP_WAIT_FOR_POST1_START;
+			mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			break;
+
+		case NOTIFICATION_FAIL:
+			del_timer_sync(&ctx->ssp_timer);
+			dma_disable(ctx, HBMCR_BMEN_SSP);
+
+			ctx->status_ssp = SSP_OFFLINE;
+			if (ibm4767_check_mbfail_expected(ctx))
+				break;
+
+			if (lo & MB_ERROR_MASK) {
+				PDEBUG(1, "Device %d: MB0 HALT with error received (0x%08x)\n", ctx->dev_index, lo);
+				mark_ssp_offline(ctx, MINIBOOT0_ERR | (lo & MB_ERROR_MASK));
+				notify_observers(ctx, EVENT_MB0_ERR);
+			}
+			else {
+				PDEBUG(1, "Device %d: MB0 HALT received\n", ctx->dev_index);
+				mark_ssp_offline(ctx, 0x0);
+			}
+			break;
+
+		case NOTIFICATION_INFO:
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: MB0 INFO received: 0x%08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+
+		default:
+			PDEBUG(2, "Device %d: UNKNOWN MB0 MBX: %08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+	}
+}
+
+
+// we have to be careful with this routine.  rather than duplicate code with dedicated handlers
+// for manufacturing and production versions of POST, we combine them into one handler.  there
+// are several places where we need to be aware of the flavor of POST that we're talking to.
+//
+// a production driver can talk to manufacturing POST well enough to boot the card 
+// 
+// a manufacturing driver can talk to both versions of POST
+//
+void
+handle_post1_mbx(ibm4767_ctx_t  *ctx, uint8_t hotbyte, uint32_t hi, uint32_t lo)
+{
+	char *agent = "POST1";
+
+	if (hotbyte == MBX_HOTBYTE_POST1_MFG) 
+		agent = "POST1 MFG";
+
+
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: %s START received: 0x%08x version 0x%08x\n", ctx->dev_index, agent, hi, lo);
+			ctx->hwinfo.post1_ver = lo;
+			ctx->status_ssp = SSP_WAIT_FOR_POST1_HELLO;
+			mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			break;
+		
+		case NOTIFICATION_HELLO:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(1, "Device %d: %s HELLO received: 0x%08x\n", ctx->dev_index, agent, hi);
+			ctx->status_ssp = SSP_POST1_HELLO;
+
+#if defined(ENABLE_MFG)
+			// there's no mechanism to inform the host driver whether fast path has been enabled
+			// POST1 definitely enables FP in manufacturing mode.  the MCPU seg2 code might disable
+			// it later depending on the value of the function control vector.
+			ctx->status_fp = FP_READY;
+
+			dma_enable(ctx, HBMCR_BMEN_SSP | HBMCR_HTB_BMEN);
+			if (ctx->htb_active == 0xFF)
+				htb_swap(ctx);
+
+			if (ctx->p1_req && ctx->p1_req->reqd_state == SSP_POST1_HELLO) {
+				if (hotbyte == MBX_HOTBYTE_POST1_MFG) {
+					PDEBUG(1, "Device %d: Waking up POST1 REQUEST thread...\n", ctx->dev_index);
+					atomic_set(&ctx->p1_req->waitEvent, 1);
+					wake_up(&ctx->p1_req->waitQ);
+					break;
+				} 
+				else {
+					PRINTKW("Device %d: Error.  Production POST1 doesn't support DMA!\n", ctx->dev_index);
+					ctx->p1_req->retcode = HOSTBadRequest;
+					atomic_set(&ctx->p1_req->waitEvent, 1);
+					wake_up(&ctx->p1_req->waitQ);
+				}
+			}
+#endif
+			if (post1_jump) { 
+				PDEBUG(2, "Device %d: snd JUMP_TO_MB1...\n", ctx->dev_index);
+				write32(cpu_to_be32(POST1_JUMP_TO_MB1), H2S_MBX_H(ctx));
+				ctx->status_ssp = SSP_WAIT_FOR_POST1_END;
+				mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			} else { 
+				PDEBUG(2, "Device %d: post1_jump==0 so do not send JUMP_TO_MB1...\n", ctx->dev_index);
+				// if we're not jumping to MB1, chances are the user wants to do stuff
+				// in the POST1 debugger.  so we don't want to time-out the SSP...
+			}
+			break;
+
+#if defined(ENABLE_MFG)
+		case NOTIFICATION_SERIAL_DBG_START:
+			PDEBUG(2, "Device %d: %s SERIAL_DBG_START received: 0x%08x\n", ctx->dev_index, agent, hi);
+			ctx->status_ssp = SSP_POST1_SERIAL_DBG;
+			break;
+		
+		case NOTIFICATION_SERIAL_DBG_END:
+			// in principle, we should never see this since the serial debugger should never terminate
+			PDEBUG(2, "Device %d: %s SERIAL_DBG_END received: 0x%08x\n", ctx->dev_index, agent, hi);
+			break;
+#endif
+
+		case NOTIFICATION_END:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			dma_disable(ctx, HBMCR_BMEN_SSP);
+
+			PDEBUG(2, "Device %d: %s END received: 0x%08x\n", ctx->dev_index, agent, hi);
+			
+			// production: POST1 _END should be followed by MB1 _START so we set a timer and wait
+			//
+			// manufacturing: MB1 might not exist and the card might be about to begin running 
+			// some kind of FTE test.  so we don't do anything.  we'll resync when the next 
+			// notification arrives.
+			//
+			if (hotbyte == MBX_HOTBYTE_POST1) {
+				ctx->status_ssp = SSP_WAIT_FOR_MB1_START;
+				mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			}
+			break;
+
+		case NOTIFICATION_FAIL:
+			del_timer_sync(&ctx->ssp_timer);
+
+			PRINTKW("Device %d: %s FAIL -- %08x:%08x\n", ctx->dev_index, agent, hi, lo);
+			mark_device_offline(ctx, POSTError);
+			notify_observers(ctx, EVENT_POST1_ERR);
+
+#if !defined(ENABLE_MFG)
+			// in production mode, I believe the correct response is to reset the card at this point.
+			// for manufacturing mode, we can't risk interrupting riscwatch or the bus analyzer...
+			defer_main_reset(ctx, 5, FALSE);
+#endif
+			break;
+
+		case NOTIFICATION_DIAG_ERROR:
+			// the POST guys assure me that any NOTIFICATION_CRIT_ERROR notifications
+			// will appear to come from POST0 or POST2...
+			//
+			PRINTKW("Device %d: %s DIAG ERR -- %08x:%08x\n", ctx->dev_index, agent, hi, lo);
+			del_timer_sync(&ctx->ssp_timer);
+
+			if (postde_echo) {
+				spin_lock_bh(&ctx->edump_lock);
+				ctx->edump_flag |= EDUMP_FLAG_BYPASS_DE_SSP;
+				spin_unlock_bh(&ctx->edump_lock);
+			}
+			else {
+				spin_lock_bh(&ctx->edump_lock);
+				ctx->edump_flag |= EDUMP_FLAG_MARK_SSP_OFFLINE;
+				spin_unlock_bh(&ctx->edump_lock);
+			}
+			schedule_work(&ctx->post0_edump_work);
+			notify_observers(ctx, EVENT_POST1_ERR);
+			break;
+
+		case NOTIFICATION_INFO:
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: %s INFO received: 0x%08x:%08x\n", ctx->dev_index, agent, hi, lo);
+			break;
+
+		default:
+			PDEBUG(2, "Device %d: UNKNOWN %s MBX: %08x:%08x\n", ctx->dev_index, agent, hi, lo);
+			break;
+	}
+}
+
+void
+handle_mb1_mbx(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+			
+			PDEBUG(2, "Device %d: MB1 START received: 0x%08x version 0x%08x\n", ctx->dev_index, hi, lo);
+
+			ctx->hwinfo.mb1_ver = lo;
+			ctx->status_ssp = SSP_WAIT_FOR_MB1_HELLO;
+			mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			break;
+
+		case NOTIFICATION_HELLO:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: MB1 HELLO received: 0x%08X\n", ctx->dev_index, hi);
+
+			ctx->status_ssp = SSP_MB1;
+			if (ibm4767_check_ssp_reset_pending(ctx))
+				break;
+			
+            // we grab this lock because 'mb_req' might get flushed during an out-of-band reset...
+            spin_lock_bh(&ctx->req_pool_lock);
+			if (ctx->mb_req && (ctx->mb_req->reqd_state == SSP_MB1)) { 
+				PDEBUG(2, "Device %d: Sending CONTINUE_MB_MODE\n", ctx->dev_index);
+				write32(cpu_to_be32(CONTINUE_MB_MODE), H2S_MBX_H(ctx));
+
+				// in the 4767 universe, MB does not respond with an ACK.  we go ahead
+				// and start the DMA (re)fetch.  the DMA will stall until MB reads it
+				// in accordance with the fragmented DMA scheme...
+				//
+				dma_enable(ctx, HBMCR_BMEN_SSP | HBMCR_HTB_BMEN);
+				if (ctx->htb_active == 0xFF)
+					htb_swap(ctx);
+				
+				PDEBUG(1, "Device %d: Waking up MB REQUEST thread...\n", ctx->dev_index);
+				atomic_set(&ctx->mb_req->waitEvent, 1);
+				wake_up(&ctx->mb_req->waitQ);
+			} 
+			else { 
+				PDEBUG(2, "Device %d: Sending CONTINUE_NORMAL\n", ctx->dev_index);
+				write32(cpu_to_be32(CONTINUE_NORMAL), H2S_MBX_H(ctx));
+				ctx->status_ssp = SSP_WAIT_FOR_MB1_END;
+				mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+			}
+            spin_unlock_bh(&ctx->req_pool_lock);
+
+			break;
+
+		case NOTIFICATION_ACK:
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PRINTK("Device %d: Unexpected MB1 ACK received.  Marking SSP offline\n", ctx->dev_index);
+			mark_ssp_offline(ctx, MB1Error);
+			break;
+
+		case NOTIFICATION_END:
+			del_timer_sync(&ctx->ssp_timer);
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+			
+			PDEBUG(2, "Device %d: MB1 END received\n", ctx->dev_index);
+			dma_disable(ctx, HBMCR_BMEN_SSP);
+
+			// this is most likely a fleeting status update as we should soon get a _FAIL notification
+			// indicating that MB1 has halted the SSP...
+			//
+			ctx->status_ssp = SSP_MB_DONE;
+			break;
+
+		case NOTIFICATION_FAIL:
+			del_timer_sync(&ctx->ssp_timer);
+			dma_disable(ctx, HBMCR_BMEN_SSP);
+
+			ctx->status_ssp = SSP_OFFLINE;
+			if (ibm4767_check_mbfail_expected(ctx))
+				break;
+
+			if (lo & MB_ERROR_MASK) {
+				PDEBUG(1, "Device %d: MB1 HALT with error received (0x%08x)\n", ctx->dev_index, lo);
+				mark_ssp_offline(ctx, MINIBOOT1_ERR | (lo & MB_ERROR_MASK));
+				notify_observers(ctx, EVENT_MB1_ERR);
+			}
+			else {
+				PDEBUG(1, "Device %d: MB1 HALT received\n", ctx->dev_index);
+				mark_ssp_offline(ctx, 0x0);
+			}
+			break;
+
+		case NOTIFICATION_INFO:
+			if (ctx->status_ssp == SSP_OFFLINE)
+				break;
+
+			PDEBUG(2, "Device %d: MB1 INFO received: 0x%08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+
+		default:
+			PDEBUG(2, "Device %d: UNKNOWN MB1 MBX: %08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+	}
+}
+
+
+#if defined(ENABLE_MFG)
+void
+handle_ssp_tas_mbx(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			PDEBUG(2, "Device %d: IBM TAS SSP START received: 0x%08x version 0x%08x\n", ctx->dev_index, hi, lo);
+			ctx->status_ssp = SSP_IBM_TAS_START;
+			break;
+		
+		case NOTIFICATION_HELLO:
+			PDEBUG(2, "Device %d: IBM TAS SSP HELLO received: 0x%08x\n", ctx->dev_index, hi);
+			ctx->status_ssp = SSP_IBM_TAS_ACTIVE;
+
+			dma_enable(ctx, HBMCR_BMEN_SSP | HBMCR_HTB_BMEN );
+			if (ctx->htb_active == 0xFF)
+				htb_swap(ctx);
+
+			if (ctx->ssp_tas_req && ctx->ssp_tas_req->reqd_state == SSP_IBM_TAS_ACTIVE) {
+				PDEBUG(1, "Device %d: Waking up IBM TAS SSP REQUEST thread...\n", ctx->dev_index);
+				atomic_set(&ctx->ssp_tas_req->waitEvent, 1);
+				wake_up(&ctx->ssp_tas_req->waitQ);
+			} 
+			break;
+
+		case NOTIFICATION_FAIL:
+			PRINTKW("Device %d, IBM TAS SSP FAIL:  %08x:%08x\n", ctx->dev_index, hi, lo);
+			notify_observers(ctx, EVENT_TAS_SSP_ERR);
+
+			// TAS and TAPP may or may not halt the SSP upon failure so we don't mark the SSP offline.  
+			// just abort the outstanding requests...
+			//
+			if (ctx->ssp_tas_req) {
+				ctx->ssp_tas_req->retcode = (OTHER_ERR | 
+							    (XC_AGENTID_IBM_TAS_SSP << 16) |
+							    (lo & 0xFFFF));
+				atomic_set(&ctx->ssp_tas_req->waitEvent, 1);
+				wake_up(&ctx->ssp_tas_req->waitQ);
+			}
+			if (ctx->ssp_tapp_req) {
+				ctx->ssp_tapp_req->retcode = (OTHER_ERR | 
+							    (XC_AGENTID_IBM_TAS_SSP << 16) |
+							    (lo & 0xFFFF));
+				atomic_set(&ctx->ssp_tapp_req->waitEvent, 1);
+				wake_up(&ctx->ssp_tapp_req->waitQ);
+			}
+			break;
+
+		case NOTIFICATION_END:
+			PDEBUG(2, "Device %d: IBM TAS SSP END received: 0x%08x\n", ctx->dev_index, hi);
+			dma_disable(ctx, HBMCR_BMEN_SSP);
+			break;
+		
+		case NOTIFICATION_CRIT_ERROR:
+		case NOTIFICATION_DIAG_ERROR:
+			PRINTKW("Device %d, TAS SSP ERR:  %08x:%08x.  Obtain emergency dump\n", ctx->dev_index, hi, lo);
+			mark_device_offline(ctx, HOSTAborted);
+			
+			schedule_work(&ctx->ssp_tas_edump_work);
+			notify_observers(ctx, EVENT_TAS_SSP_ERR);
+
+			// the manufacturing guys do not want to reset the card here...
+			break;
+
+		case NOTIFICATION_INFO:
+			PDEBUG(2, "Device %d: TAS SSP INFO received: 0x%08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+	}
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+void
+handle_ssp_tapp_mbx(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			PDEBUG(2, "Device %d: IBM TAPP SSP START received: 0x%08x version 0x%08x\n", ctx->dev_index, hi, lo);
+			ctx->status_ssp = SSP_IBM_TAPP_START;
+			break;
+		
+		case NOTIFICATION_HELLO:
+			PDEBUG(2, "Device %d: IBM TAPP SSP HELLO received: 0x%08x\n", ctx->dev_index, hi);
+			ctx->status_ssp = SSP_IBM_TAPP_ACTIVE;
+
+			// we do not enable/disable DMA for IBM TAPP HELLO/END because TAPP is
+			// still active
+			
+			if (ctx->ssp_tapp_req && ctx->ssp_tapp_req->reqd_state == SSP_IBM_TAPP_ACTIVE) {
+				PDEBUG(1, "Device %d: Waking up IBM TAPP SSP REQUEST thread...\n", ctx->dev_index);
+				atomic_set(&ctx->ssp_tapp_req->waitEvent, 1);
+				wake_up(&ctx->ssp_tapp_req->waitQ);
+			} 
+			break;
+		
+		case NOTIFICATION_FAIL:
+			PRINTKW("Device %d, IBM TAPP SSP FAIL:  %08x:%08x\n", ctx->dev_index, hi, lo);
+			notify_observers(ctx, EVENT_TAPP_SSP_ERR);
+		
+			// rather than make assumptions about what the SSP is doing at this point,
+			// just abort the outstanding requests.  the FTE guys will manually reset
+			// the device when they restart their tests anyway...
+			//
+			if (ctx->ssp_tas_req) {
+				ctx->ssp_tas_req->retcode = (OTHER_ERR | 
+							    (XC_AGENTID_IBM_TAPP_SSP << 16) |
+							    (lo & 0xFFFF));
+				atomic_set(&ctx->ssp_tas_req->waitEvent, 1);
+				wake_up(&ctx->ssp_tas_req->waitQ);
+			}
+			if (ctx->ssp_tapp_req) {
+				ctx->ssp_tapp_req->retcode = (OTHER_ERR | 
+							    (XC_AGENTID_IBM_TAPP_SSP << 16) |
+							    (lo & 0xFFFF));
+				atomic_set(&ctx->ssp_tapp_req->waitEvent, 1);
+				wake_up(&ctx->ssp_tapp_req->waitQ);
+			}
+			break;
+
+		case NOTIFICATION_END:
+			PDEBUG(2, "Device %d: IBM TAPP SSP END received: 0x%08x\n", ctx->dev_index, hi);
+			ctx->status_ssp = SSP_IBM_TAS_ACTIVE;
+			
+			// we do not enable/disable DMA for IBM TAPP HELLO/END because TAPP is
+			// still active
+			break;
+
+		// the assumption is that a TAPP will not generate a CRIT or DIAG error notification.
+		// if on-card TAPP generates an expected interrupt, it should handle it.  if it generates
+		// an unexpected interrupt, the TAS interrupt handler should catch it and send a CRIT or
+		// DIAG error with the TAS hotbyte...
+
+		case NOTIFICATION_INFO:
+			PDEBUG(2, "Device %d: TAPP SSP INFO received: 0x%08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+	}
+}
+#endif
+
+
+void
+handle_post2_mbx(ibm4767_ctx_t  *ctx, uint8_t hotbyte, uint32_t hi, uint32_t lo)
+{
+	char *agent = "POST2";
+
+	if (hotbyte == MBX_HOTBYTE_POST2_MFG) 
+		agent = "POST2 MFG";
+
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			ctx->hwinfo.post2_ver = lo;
+			PDEBUG(2, "Device %d: %s START received: 0x%08x version 0x%08x\n", ctx->dev_index, agent, hi, lo);
+			ctx->status_mcpu = MCPU_POST2_START;
+			break;
+		
+		case NOTIFICATION_HELLO:
+			PDEBUG(2, "Device %d: %s HELLO received: 0x%08x\n", ctx->dev_index, agent, hi);
+			ctx->status_mcpu = MCPU_POST2_HELLO;
+		
+#if defined(ENABLE_MFG)	
+			dma_enable(ctx, HBMCR_BMEN_MCPU | HBMCR_HTB_BMEN );
+			if (ctx->htb_active == 0xFF)
+				htb_swap(ctx);
+
+			if (ctx->p2_req && ctx->p2_req->reqd_state == MCPU_POST2_HELLO) {
+				if (hotbyte == MBX_HOTBYTE_POST2_MFG) {
+					PDEBUG(1, "Device %d: Waking up POST2 REQUEST thread...\n", ctx->dev_index);
+					atomic_set(&ctx->p2_req->waitEvent, 1);
+					wake_up(&ctx->p2_req->waitQ);
+				}
+				else {
+					PRINTKW("Device %d: Error.  Production POST2 doesn't support DMA!\n", ctx->dev_index);
+					ctx->p2_req->retcode = HOSTBadRequest;
+					atomic_set(&ctx->p2_req->waitEvent, 1);
+					wake_up(&ctx->p2_req->waitQ);
+				}
+			} 
+#endif
+			break;
+
+#if defined(ENABLE_MFG)
+		case NOTIFICATION_SERIAL_DBG_START:
+			PDEBUG(2, "Device %d: %s SERIAL_DBG_START received: 0x%08x\n", ctx->dev_index, agent, hi);
+			ctx->status_mcpu = MCPU_POST2_SERIAL_DBG;
+			break;
+		
+		case NOTIFICATION_SERIAL_DBG_END:
+			// in principle, we should never see this since the serial debugger should never terminate
+			PDEBUG(2, "Device %d: %s SERIAL_DBG_END received: 0x%08x\n", ctx->dev_index, agent, hi);
+			break;
+#endif
+
+		case NOTIFICATION_END:
+			PDEBUG(2, "Device %d: %s END received: 0x%08x\n", ctx->dev_index, agent, hi);
+			dma_disable(ctx, HBMCR_BMEN_MCPU);
+			ctx->status_mcpu = MCPU_POST2_END;
+			break;
+		
+		case NOTIFICATION_FAIL:
+			PRINTKW("Device %d: %s FAIL received -- 0x%08x : 0x%08x\n", ctx->dev_index, agent, hi, lo);
+			mark_device_offline(ctx, POSTError);
+			notify_observers(ctx, EVENT_POST2_ERR);
+#if !defined(ENABLE_MFG) 
+			// in production mode, I believe the correct response is to reset the card at this point.
+			// for manufacturing mode, we can't risk interrupting riscwatch or the bus analyzer...
+			defer_main_reset(ctx, 5, FALSE);
+#endif
+			break;
+		
+		case NOTIFICATION_CRIT_ERROR:
+			PRINTKW("Device %d: %s CRIT ERR -- %08x:%08x.  Obtain emergency dump\n", ctx->dev_index, agent, hi, lo);
+			mark_mcpu_offline(ctx, POSTError);
+			schedule_work(&ctx->post2_edump_work);
+			notify_observers(ctx, EVENT_POST2_ERR);
+
+			// is there any reason to think that a reset might "fix" things?
+			break;
+		
+		case NOTIFICATION_DIAG_ERROR:
+			PRINTKW("Device %d: %s DIAG ERR -- %08x:%08x\n", ctx->dev_index, agent, hi, lo);
+			if (postde_echo) {
+				spin_lock_bh(&ctx->edump_lock);
+				ctx->edump_flag |= EDUMP_FLAG_BYPASS_DE_MCPU;
+				spin_unlock_bh(&ctx->edump_lock);
+			}
+			else {
+				spin_lock_bh(&ctx->edump_lock);
+				ctx->edump_flag |= EDUMP_FLAG_MARK_MCPU_OFFLINE;
+				spin_unlock_bh(&ctx->edump_lock);
+			}
+			schedule_work(&ctx->post2_edump_work);
+			notify_observers(ctx, EVENT_POST2_ERR);
+			break;
+
+		case NOTIFICATION_INFO:
+			PDEBUG(2, "Device %d: %s INFO received: 0x%08x:%08x\n", ctx->dev_index, agent, hi, lo);
+			break;
+
+		default:
+			PDEBUG(2, "Device %d: UNKNOWN %s MBX: %08x:%08x\n", ctx->dev_index, agent, hi, lo);
+			break;
+	}
+}
+
+
+#if defined(ENABLE_MFG)
+void
+handle_mcpu_tas_mbx(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			PDEBUG(2, "Device %d: IBM TAS MCPU START received: 0x%08x version 0x%08x\n", ctx->dev_index, hi, lo);
+			ctx->status_mcpu = MCPU_IBM_TAS_START;
+			break;
+		
+		case NOTIFICATION_HELLO:
+			PDEBUG(2, "Device %d: IBM TAS MCPU HELLO received: 0x%08x\n", ctx->dev_index, hi);
+			ctx->status_mcpu = MCPU_IBM_TAS_ACTIVE;
+			
+			dma_enable(ctx, HBMCR_BMEN_MCPU | HBMCR_HTB_BMEN );
+			if (ctx->htb_active == 0xFF)
+				htb_swap(ctx);
+
+			if (ctx->mcpu_tas_req && ctx->mcpu_tas_req->reqd_state == MCPU_IBM_TAS_ACTIVE) {
+				PDEBUG(1, "Device %d: Waking up IBM TAS MCPU REQUEST thread...\n", ctx->dev_index);
+				atomic_set(&ctx->mcpu_tas_req->waitEvent, 1);
+				wake_up(&ctx->mcpu_tas_req->waitQ);
+			} 
+			break;
+
+		case NOTIFICATION_FAIL:
+			PRINTKW("Device %d, IBM TAS MCPU FAIL:  %08x:%08x\n", ctx->dev_index, hi, lo);
+			notify_observers(ctx, EVENT_TAS_MCPU_ERR);
+
+			if (ctx->mcpu_tas_req) {
+				ctx->mcpu_tas_req->retcode = (OTHER_ERR | 
+							     (XC_AGENTID_IBM_TAS_MCPU << 16) |
+							     (lo & 0xFFFF));
+				atomic_set(&ctx->mcpu_tas_req->waitEvent, 1);
+				wake_up(&ctx->mcpu_tas_req->waitQ);
+			}
+			if (ctx->mcpu_tapp_req) {
+				ctx->mcpu_tapp_req->retcode = (OTHER_ERR | 
+							    (XC_AGENTID_IBM_TAS_MCPU << 16) |
+							    (lo & 0xFFFF));
+				atomic_set(&ctx->mcpu_tapp_req->waitEvent, 1);
+				wake_up(&ctx->mcpu_tapp_req->waitQ);
+			}
+			break;
+
+		case NOTIFICATION_END:
+			PDEBUG(2, "Device %d: IBM TAS MCPU END received: 0x%08x\n", ctx->dev_index, hi);
+			dma_disable(ctx, HBMCR_BMEN_MCPU);
+			break;
+		
+		case NOTIFICATION_CRIT_ERROR:
+		case NOTIFICATION_DIAG_ERROR:
+			PRINTKW("Device %d, TAS MCPU ERR:  %08x:%08x.  Obtain emergency dump\n", ctx->dev_index, hi, lo);
+			mark_mcpu_offline(ctx, HOSTAborted);
+			
+			schedule_work(&ctx->mcpu_tas_edump_work);
+			notify_observers(ctx, EVENT_TAS_MCPU_ERR);
+
+			// FIXME - should we auto-reset the card?
+			break;
+
+		case NOTIFICATION_INFO:
+			PDEBUG(2, "Device %d: TAS MCPU INFO received: 0x%08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+	}
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+void
+handle_mcpu_tapp_mbx(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	switch ((hi & 0xFFFF0000) >> 16) {
+		case NOTIFICATION_START:
+			PDEBUG(2, "Device %d: IBM TAPP MCPU START received: 0x%08x version 0x%08x\n", ctx->dev_index, hi, lo);
+			ctx->status_mcpu = MCPU_IBM_TAS_START;
+			break;
+		
+		case NOTIFICATION_HELLO:
+			PDEBUG(2, "Device %d: IBM TAPP MCPU HELLO received: 0x%08x\n", ctx->dev_index, hi);
+			ctx->status_mcpu = MCPU_IBM_TAPP_ACTIVE;
+			
+			// we do not enable/disable DMA for IBM TAPP HELLO/END because TAPP is
+			// still active
+
+			if (ctx->mcpu_tapp_req && ctx->mcpu_tapp_req->reqd_state == MCPU_IBM_TAPP_ACTIVE) {
+				PDEBUG(1, "Device %d: Waking up IBM TAPP MCPU REQUEST thread...\n", ctx->dev_index);
+				atomic_set(&ctx->mcpu_tapp_req->waitEvent, 1);
+				wake_up(&ctx->mcpu_tapp_req->waitQ);
+			} 
+			break;
+
+		case NOTIFICATION_FAIL:
+			PRINTKW("Device %d, IBM TAPP MCPU FAIL:  %08x:%08x\n", ctx->dev_index, hi, lo);
+			notify_observers(ctx, EVENT_TAPP_MCPU_ERR);
+
+			if (ctx->mcpu_tas_req) {
+				ctx->mcpu_tas_req->retcode = (OTHER_ERR | 
+							     (XC_AGENTID_IBM_TAPP_MCPU << 16) |
+							     (lo & 0xFFFF));
+				atomic_set(&ctx->mcpu_tas_req->waitEvent, 1);
+				wake_up(&ctx->mcpu_tas_req->waitQ);
+			}
+			if (ctx->mcpu_tapp_req) {
+				ctx->mcpu_tapp_req->retcode = (OTHER_ERR | 
+							    (XC_AGENTID_IBM_TAPP_MCPU << 16) |
+							    (lo & 0xFFFF));
+				atomic_set(&ctx->mcpu_tapp_req->waitEvent, 1);
+				wake_up(&ctx->mcpu_tapp_req->waitQ);
+			}
+			break;
+
+		case NOTIFICATION_END:
+			PDEBUG(2, "Device %d: IBM TAPP MCPU END received: 0x%08x\n", ctx->dev_index, hi);
+			ctx->status_mcpu = MCPU_IBM_TAS_ACTIVE;
+			
+			// we do not enable/disable DMA for IBM TAPP HELLO/END because TAPP is
+			// still active
+			break;
+		
+		// the assumption is that a TAPP will not generate a CRIT or DIAG error notification.
+		// if on-card TAPP generates an expected interrupt, it should handle it.  if it generates
+		// an unexpected interrupt, the TAS interrupt handler should catch it and send a CRIT or
+		// DIAG error with the TAS hotbyte...
+
+		case NOTIFICATION_INFO:
+			PDEBUG(2, "Device %d: TAPP MCPU INFO received: 0x%08x:%08x\n", ctx->dev_index, hi, lo);
+			break;
+	}
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+//
+//
+void
+add_mailbox_history(uint64_t *list,
+		    uint16_t *r,
+		    uint16_t *w,
+		    uint32_t  hi,
+		    uint32_t  lo)
+{
+	// unlike the circular buffer for saved interrupts where overlap is considered
+	// an error, mailbox history can overlap since the API simply returns the last
+	// N notifications received.  so this isn't a one-producer-one-consumer scenario
+	// since the producer can increment both read and write indices.  so we have to
+	// use a lock to be safe...
+	//
+	// caller is responsible for grabbing the lock...
+	//
+	list[*w] = (uint64_t)(((uint64_t)hi)<<32) | (uint64_t)lo;
+
+	*w = (*w + 1) & (MAILBOX_HISTORY_LEN - 1);
+	if (*w == *r) {
+		// overlap...increment read index
+		*r = (*r + 1) & (MAILBOX_HISTORY_LEN - 1);
+	}
+}
+#endif
+
+	
+void
+s2h_mbx_handler(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	uint8_t hotbyte;
+
+#if defined(WINDOWS)
+	uint32_t data[2];
+
+	data[0] = hi;
+	data[1] = lo;
+#endif
+
+	if (!ctx)
+		return;
+
+#if defined(ENABLE_MFG)	
+	spin_lock_bh(&ctx->ssp_mbxhist_lock);
+	add_mailbox_history(ctx->ssp_mbxhist_list, &ctx->ssp_mbxhist_r_idx, &ctx->ssp_mbxhist_w_idx, hi, lo);
+	spin_unlock_bh(&ctx->ssp_mbxhist_lock);
+#endif
+
+	hotbyte = ((hi & 0x0000FF00) >> 8);
+
+	if (hotbyte == MBX_HOTBYTE_SAME_AS_LAST)
+		hotbyte = ctx->last_mbx_agentid_ssp;
+
+	switch (hotbyte) {
+		case MBX_HOTBYTE_POST0:
+		case MBX_HOTBYTE_POST0_MFG:
+			handle_post0_mbx(ctx, hotbyte, hi, lo);
+			break;
+	
+		case MBX_HOTBYTE_MB0:
+			handle_mb0_mbx(ctx, hi, lo);
+			break;
+
+		case MBX_HOTBYTE_POST1:
+		case MBX_HOTBYTE_POST1_MFG:
+			handle_post1_mbx(ctx, hotbyte, hi, lo);
+			break;
+
+		case MBX_HOTBYTE_MB1:
+			handle_mb1_mbx(ctx, hi, lo);
+			break;
+
+#if defined(ENABLE_MFG)
+		case MBX_HOTBYTE_IBM_TAS_SSP:
+			handle_ssp_tas_mbx(ctx, hi, lo);
+			break;
+
+		case MBX_HOTBYTE_IBM_TAPP_SSP:
+			handle_ssp_tapp_mbx(ctx, hi, lo);
+			break;
+#endif
+
+		default:
+			PDEBUG(2, "Device %d: S2H MBX Unknown: %08x:%08x\n", ctx->dev_index, hi, lo);
+	}
+			
+	ctx->last_mbx_agentid_ssp = hotbyte;
+}
+
+
+void
+m2h_mbx_handler(ibm4767_ctx_t  *ctx, uint32_t hi, uint32_t lo)
+{
+	uint8_t hotbyte;
+
+	if (!ctx)
+		return;
+
+	//PDEBUG(1, "Device %d: Enter %s... %08x:%08x\n", ctx->dev_index, __FUNCTION__, hi, lo);
+
+#if defined(ENABLE_MFG)
+	spin_lock_bh(&ctx->mcpu_mbxhist_lock);
+	add_mailbox_history(ctx->mcpu_mbxhist_list, &ctx->mcpu_mbxhist_r_idx, &ctx->mcpu_mbxhist_w_idx, hi, lo);
+	spin_unlock_bh(&ctx->mcpu_mbxhist_lock);
+#endif
+
+	hotbyte = ((hi & 0x0000FF00) >> 8);
+
+	switch (hotbyte) {
+		case MBX_HOTBYTE_POST2:
+		case MBX_HOTBYTE_POST2_MFG:
+			handle_post2_mbx(ctx, hotbyte, hi, lo);
+			break;
+
+#if defined(ENABLE_MFG)
+		case MBX_HOTBYTE_IBM_TAS_MCPU:
+			handle_mcpu_tas_mbx(ctx, hi, lo);
+			break;
+
+		case MBX_HOTBYTE_IBM_TAPP_MCPU:
+			handle_mcpu_tapp_mbx(ctx, hi, lo);
+			break;
+#endif
+
+		default: 
+			m2h_mbx_comm_mgr_handler(ctx, hi, lo);
+	}
+}
+
+
+void
+m2h_mbx_comm_mgr_handler(ibm4767_ctx_t *ctx, uint32_t hi, uint32_t lo)
+{
+	if (!ctx)
+		return;
+	
+	//PDEBUG(1, "Device %d: Enter %s... %08x:%08x\n", ctx->dev_index, __FUNCTION__, hi, lo);
+	
+
+	switch (hi & 0x0000FF00) {
+		case MBX_GOODMORNINGHOST:
+			PDEBUG(1, "Device %d: GOOD MORNING recv\n", ctx->dev_index);
+
+			ctx->status_mcpu = MCPU_INITIALIZED;
+			
+			dma_enable(ctx, HBMCR_BMEN_MCPU);
+			if (ctx->htb_active == 0xFF)
+				htb_swap(ctx);
+
+			schedule_work(&ctx->hra_pool_work);
+			break;
+
+		case MBX_EMERGENCY_DUMP:
+			PRINTKW("M2H MBX: Recv emergency dump from card %d\n", ctx->dev_index);
+
+			// race condition:  FFDC_WITH_DUMP sets up a 2-second reset timer.
+			// if the timer fires at the same moment the edump begins, then we might 
+			// try to drain the FIFOs while the card is reseting...
+			//
+			spin_lock_bh(&ctx->counter_lock);
+			if (ctx->status_flags & FLAG_MAIN_RESET_ACTIVE) {
+				spin_unlock_bh(&ctx->counter_lock);
+				PRINTKW("Card %d: main reset already active...not retreiving MCPU emergency dump...\n", ctx->dev_index);
+				break;
+			}
+			spin_unlock_bh(&ctx->counter_lock);
+
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce |= TIMER_QUIESCE_DEFERRED_MAIN_RESET;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			cancel_deferred_main_reset(ctx);
+
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce &= ~TIMER_QUIESCE_DEFERRED_MAIN_RESET;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			// in contrast with the SSP emergency dump notification, there are several
+			// reasons why the MCPU might generate an emergency dump.  so we just disable
+			// the MCPU DMA channels
+			//
+			//mark_fp_offline(ctx, HOSTAborted);
+			mark_mcpu_offline(ctx, HOSTAborted);
+
+			schedule_work(&ctx->seg2_edump_work);
+
+			// the Comm Mgr forces a kernel panic after every emergency dump 
+			// so we don't kick off the reset timer for an MCPU emergency 
+			// dump.  We'll do that when we receive the ensuing kernel 
+			// panic notificatino.
+			//
+			// there's still a race condition with SSP emergency dumps.  in
+			// principle, an emergency dump due to critical error might be
+			// generated by both the SSP and MCPU or by only one of them.
+			// if both processors generate a dump, there's no guarantee which
+			// one we'll see first.  So when the driver sees an SSP e-dump,
+			// it sets the reset timer for 5 seconds.  this -should- be 
+			// enough time to retrieve the MCPU e-dump if one is in fact waiting...
+			//
+			break;
+
+		case MBX_REQBUFUPDATE:
+			PDEBUG(1, "Device %d: Request Buffer Update\n", ctx->dev_index);
+			// when we mark the MCPU offline, we disable DMA so it's reasonable for the HDD to
+			// stop responding to buffer update requests...
+			//
+			if (ctx->status_mcpu != MCPU_OFFLINE)
+				asym_hra_pool_update(ctx);
+			break;
+
+		case MBX_SYMPROTOCOL_ACK:
+			PDEBUG(1, "Device %d: SYMPROCOL_ACK\n", ctx->dev_index);
+			break;
+		
+		case MBX_REQRESET: 
+			PDEBUG(1, "Device %d: M2H MBX: Request RESET\n", ctx->dev_index);
+			// I don't think we need to do anything here.  If I'm reading the Fence correctly,
+			// a GOOD MORNING will follow REQRESET at which point the driver will resend the 
+			// HRA Load Packet
+			//
+			//asym_hra_pool_load_packet_send(ctx);
+			break;
+
+		case MBX_INVALID_MB_CMD:
+			PRINTKW("M2H MBX: Invalid mailbox command: %d\n", (hi & 0x00FF0000) >> 16);
+			break;
+    
+		case MBX_CEASE_EVENT_LOG_ACK:
+			PRINTKW("M2H MBX: CEASE_EVENT_LOG ACK (VFID %d)\n", (hi >> 16) & 0xFF);
+			break;
+    
+		case MBX_SW_ERROR: 
+			switch((hi & 0x00FF0000) >> 16) {
+				case TOO_MANY_IRQ3S: 
+					PRINTKW("Device %d: Software ERROR - too many nested IRQ3s\n", ctx->dev_index);
+					break;
+        
+				case TOO_MANY_IRQ5S: 
+					PRINTKW("Device %d: Software ERROR - too many nested IRQ5s\n", ctx->dev_index);
+					break;
+        
+				case NO_MRB_SPACE: 
+					PRINTKW("Device %d: Software ERROR - IRQ5 when MRB full\n", ctx->dev_index);
+					break;
+        
+				case LEN_NOT_MULT_OF_8: 
+					PRINTKW("Device %d: Software ERROR - HRA pool len (%d) not mult of 8\n", ctx->dev_index, lo);
+					break;
+        
+				case HRA_LEN_TOO_SMALL: 
+					PRINTKW("Device %d: Software ERROR - HRA pool len (%d) too small\n", ctx->dev_index, lo);
+					break;
+        
+				case NO_FREE_QES: 
+					PRINTKW("Device %d: Software ERROR - no free H2C queues\n", ctx->dev_index);
+					break;
+        
+				case ALIGNMENT_ERROR: 
+					PRINTKW("Device %d: Software ERROR - HRB misaligned - " 
+						"Aux data = 0x%08X\n", ctx->dev_index, lo);
+					// FIXME dump_all_xcrbs(ctx);
+					break;
+        
+				case NO_TAG_OCPRB: 
+					PRINTKW("Device %d: Software ERROR - no tag=01 in HRB\n", ctx->dev_index);
+					break;
+        
+				default: 
+					PRINTKW("Device %d: Software ERROR - unknown status %08X:%08X\n", ctx->dev_index, hi, lo);
+			} 
+
+			// these are unrecoverable comm mgr errors so clean up gracefully...
+			mark_mcpu_offline(ctx, HOSTFirmwareError);
+			break;
+    
+		case MBX_NONCRIT_SW_ERROR: 
+			switch ((hi & 0x00FF0000) >> 16) {
+				case HRA_MASK_INVALID:
+					PRINTKW("Device %d: HRA mask invalid\n", ctx->dev_index);
+					break;
+
+				case OUTPUT_CPRB_TOO_SMALL: 
+					PRINTKW("Device %d: Non-critical Software ERROR - OUTPUT_CPRB_TOO_SMALL - " 
+						"request ID %04X\n", ctx->dev_index, lo & 0xFFFF);
+					request_abort_by_requestid(ctx, lo & 0xFFFF, HOSTBufferTooSmall);
+					break;
+        
+				case REPLY_DATA_TOO_SMALL: 
+					PRINTKW("Device %d: Non-critical Software ERROR - REPLY_DATA_TOO_SMALL - " 
+						"request ID %04X\n", ctx->dev_index, lo & 0xFFFF);
+					request_abort_by_requestid(ctx, lo & 0xFFFF, HOSTBufferTooSmall);
+					break;
+
+				case FFDC_WITH_DUMP:
+					PRINTKW("Device %d: Kernel Err:  %08X:%08X.  Emergency dump expected...\n", ctx->dev_index, hi, lo);
+					defer_main_reset(ctx, 2, FALSE);
+					break;
+
+				case FFDC_NO_DUMP:
+					PRINTKW("Device %d: Kernel Err:  %08X:%08X.  No emergency dump expected...\n", ctx->dev_index, hi, lo);
+					mod_timer(&ctx->deferred_forced_edump_timer, jiffies + HZ*2);
+					break;
+
+				default: 
+					PRINTKW("Device %d: Non-critical Software ERROR - unknown 0x%08X : 0x%08x\n", ctx->dev_index, hi, lo);
+					break;
+			} 
+			break;
+    
+		case MBX_KERNEL_PANIC: 
+
+			// there could be lots of causes of an MCPU kernel panic including
+			// software problems.  we only disable the MCPU's DMA channels since
+			// SSP communications might be fine...
+			//
+			mark_fp_offline(ctx, HOSTAborted);
+			mark_mcpu_offline(ctx, HOSTAborted);
+
+			notify_observers(ctx, EVENT_MCPU_PANIC);
+
+			// we eventually want to reset the MCPU and the only way to do this is
+			// to reset the card.  the reset timer will wait until there are no active
+			// miniboot requests before it performs the reset...
+			//
+			// WARNING: this reset occurs without warning the host application(s).  if
+			// the embedded app is stateless, it should be seamless but if the embedded 
+			// app maintains some state information, bad things could happen...
+			//
+			if (reset_on_panic) {
+				PRINTKW("Device %d: Kernel panic.  Error code %08x.  Reseting card...\n", ctx->dev_index, lo);
+				defer_main_reset(ctx, 5, FALSE);
+			}
+			else {
+				PRINTKW("Device %d: Kernel panic.  Error code %08x.  reset_on_panic=0 --> not resetting card...\n", ctx->dev_index, lo);
+			}
+			break;
+    
+		case MBX_QUERYSTATUS_RETURN: 
+			PDEBUG(2, "Device %d: M2H MBX: MBX_QUERYSTATUS_RETURN - st: %02X, HRB cnt=%04x, GetReq cnt=%04x \n",
+					ctx->dev_index,
+					(uint32_t)((hi & 0x00FF0000) >> 16),
+					(uint16_t)((lo & 0xFFFF0000) >> 16),
+					(uint16_t)((lo & 0xFFFF)));
+			break;
+
+		case MBX_CDUFAILED:
+			{
+				// I'm going to assume that these errors can occur at any point during the CDU
+				// handshake so quiescing the timer is probably necessary...
+				//
+				spin_lock_bh(&ctx->timer_quiesce_lock);
+				ctx->timer_quiesce |= TIMER_QUIESCE_CDU;
+				spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+				del_timer_sync(&ctx->cdu_timeout_timer);
+
+				spin_lock_bh(&ctx->timer_quiesce_lock);
+				ctx->timer_quiesce &= ~TIMER_QUIESCE_CDU;
+				spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+				switch (hi) { 
+					case CDU_NOT_ATTACHED: 
+						PRINTKW("Device %d: CDU FAILURE:  seg3 daemon not attached\n", ctx->dev_index);
+						break;
+					case CDU_SEG3_NONEXISTENT: 
+						PRINTKW("Device %d: CDU FAILURE:  seg3 app non-existent\n", ctx->dev_index);
+						break;
+					case CDU_SEG3_OA_FAILURE: 
+						PRINTKW("Device %d: CDU FAILURE:  seg3 outbound authentication failure\n", ctx->dev_index);
+						break;
+					case CDU_SEG3_MOUNT_FAILURE: 
+						PRINTKW("Device %d: CDU FAILURE:  mount seg3 flash failed\n", ctx->dev_index);
+						break;
+					case CDU_SEG3_UMOUNT_FAILURE: 
+						PRINTKW("Device %d: CDU FAILURE:  un-mount seg3 flash failed\n", ctx->dev_index);
+						break;
+					case CDU_SEG3_LOCK_FAILURE: 
+						PRINTKW("Device %d: CDU FAILURE:  failed to obtain active lock\n", ctx->dev_index);
+						break;
+					case CDU_AGENTID_RUNNING: 
+						PRINTKW("Device %d: CDU FAILURE:  CDU daemon reports agentID already running\n", ctx->dev_index);
+						break;
+					default:
+						PRINTKW("Device %d: CDU FAILURE: Unknown reason.  MBX_HI = 0x%08x\n", ctx->dev_index, hi);
+						break;
+				}
+
+				ibm4767_cdu_finalize(ctx, (CDU_ERR | hi >> 16));
+			}
+			break;
+
+		case MBX_AGENTID_ATTACHED:
+			// shouldn't receive this anymore
+			//
+			//if (ctx->status_mcpu == MCPU_OFFLINE) {
+			//	PDEBUG(1, "Device %d: AGENTID_ATTACHED but MCPU_OFFLINE!  Ignoring\n", ctx->dev_index);
+			//}
+			//else {
+			//	PDEBUG(1, "Device %d: AGENTID_ATTACHED:  %04x (TGID %08x)\n", ctx->dev_index, (hi >> 16), lo);
+			//	ctx->status_mcpu = MCPU_ACTIVE;
+			//	ctx->status_fp   = FP_READY;
+			//}
+			break;
+
+		case MBX_AGENTID_LIST:
+			{
+				uint16_t agentid = (hi >> 16) & 0xFFFF;
+				uint32_t tgid    = lo;
+				uint8_t  x       = (hi >> 4) & 0x0F;
+				uint8_t  y       = (hi & 0x0F);
+
+				del_timer_sync(&ctx->agentid_list_timer);
+
+				if (x == 0) {
+					PRINTKW("Device %d: Number of AGENTIDS in list: %d\n", ctx->dev_index, y);
+					if (y > 0) {
+						write32(cpu_to_be32(MBX_LIST_ALL_AGENTIDS | (1 << 4)), H2M_MBX_H(ctx));
+						mod_timer(&ctx->agentid_list_timer, jiffies + HZ*2);
+					}
+				}
+				else {
+					PRINTKW("Device %d: AGENTID: 0x%04x, TGID: %d\n", ctx->dev_index, agentid, tgid);
+
+					if (x < y) {
+						x++;
+						write32(cpu_to_be32(MBX_LIST_ALL_AGENTIDS | (x << 4)), H2M_MBX_H(ctx));
+						mod_timer(&ctx->agentid_list_timer, jiffies + HZ*2);
+					}
+					else {
+						PDEBUG(1,"Device %d: AGENTID list complete...\n", ctx->dev_index);
+					}
+				}
+			}
+			break;
+
+		case MBX_NON_CDUABLE_APP:
+			// this is sent by CDU daemon in response to PROCEED_TO_CDU.
+			//
+			PDEBUG(1, "Device %d: CDU FAILURE:  non-CDUable app detected\n", ctx->dev_index);
+
+			// no need to quiesce the timer since WAIT_FOR_READY_TO_PROCEED doesn't self-schedule
+			//
+			del_timer_sync(&ctx->cdu_timeout_timer);
+			ibm4767_cdu_finalize(ctx, CDU_ERR_NOT_CDUABLE);
+			break;
+
+		case MBX_CDU_READY_TO_PROCEED:
+			// no need to quiesce the timer since WAIT_FOR_READY_TO_PROCEED doesn't self-schedule
+			//
+			del_timer_sync(&ctx->cdu_timeout_timer);
+			
+			if (ctx->status_cdu != CDU_WAIT_FOR_READY_TO_PROCEED) {
+				PRINTKW("Device %d: Received unexpected CDU_READY_TO_PROCEED!\n", ctx->dev_index);
+				break;
+			}
+
+			PRINTKW("Device %d: CDU_READY_TO_PROCEED.  AgentID: 0x%x, TGID: 0x%x\n", ctx->dev_index, (hi >> 16), lo);
+			
+			ctx->status_cdu = CDU_WAIT_FOR_PROCEED_ACK;
+			
+			// step 56 of the Fence document suggests that the driver wait until all IVs
+			// have been processed at this step.  since this incarnation of the driver
+			// does NOT stop incoming requests while CDU is active there is no need to
+			// pause here...
+			//
+			write32(cpu_to_be32(lo), H2M_MBX_L(ctx));
+			write32(cpu_to_be32(PROCEED_WITH_AGENTID | (hi & 0xFFFF0000)), H2M_MBX_H(ctx));
+
+			PRINTKW("Device %d: Sending PROCEED_WITH_AGENTID:  %08x:%08x\n",
+					ctx->dev_index,
+					cpu_to_be32(PROCEED_WITH_AGENTID | (hi & 0xFFFF0000)),
+					cpu_to_be32(lo));
+
+
+			mod_timer(&ctx->cdu_timeout_timer, jiffies + HZ*timeout_cdu);
+			break;
+
+		case MBX_CDU_PROCEED_ACK:
+			// we *do* need to quiesce the timer here...
+			//
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce |= TIMER_QUIESCE_CDU;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			del_timer_sync(&ctx->cdu_timeout_timer);
+			
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			ctx->timer_quiesce &= ~TIMER_QUIESCE_CDU;
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+
+			if (ctx->status_cdu != CDU_WAIT_FOR_PROCEED_ACK) {
+				PRINTKW("Device %d: Received unexpected CDU_PROCEED_ACK!\n", ctx->dev_index);
+				break;
+			}
+
+			PDEBUG(1, "Device %d: CDU_PROCEED_ACK\n", ctx->dev_index);
+			ibm4767_cdu_finalize(ctx, HOSTGood);
+			break;
+			
+		default: 
+			PDEBUG(2, "Device %d: MCPU MBX Unknown:  %08x:%08x\n", ctx->dev_index, hi, lo);
+
+	}
+}
+
diff --git drivers/misc/ibm4767/main.c drivers/misc/ibm4767/main.c
new file mode 100755
index 000000000000..1fc050b777e9
--- /dev/null
+++ drivers/misc/ibm4767/main.c
@@ -0,0 +1,1089 @@
+/*************************************************************************
+ *  Filename:main.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Visegrady, Tamas  IBM Poughkeepsie  <tamas@us.ibm.com>
+ *           Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#ifndef CONFIG_PCI
+#error "CONFIG_PCI is not defined - we cannot compile."
+#endif
+
+#include <linux/moduleparam.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_ioctl.h"
+#include "driver.h"
+#include "y_funcs.h"
+#include "host_err.h"
+
+
+ibm4767_ctx_t *ibm4767_list[MAX_DEV_COUNT];
+int          ibm4767_count;
+rwlock_t     ibm4767_lock;
+
+uint8_t     tasklet_shutdown[MAX_DEV_COUNT];
+
+int         allow_indirect = 0;
+int         auto_reset = 1;
+int         dbg_level = 1;
+int         enable_ecrc = 0;
+uint32_t    enable_fuzzer = 0;
+int         ignore_tamper = 0;
+int         num_msix = 1;
+int         pcie_link = 0;
+int         post1_jump = 1;
+int         postde_echo = 0;
+int         promote_ssp_reset = 0;
+int         reset_on_panic = 1;
+int         sev0_ignore = 0;
+uint32_t    sev0_limit = 5;
+uint32_t    sev0_timeframe = 30;
+uint32_t    timeout_cdu   = TIMEOUT_CDU;
+uint32_t    timeout_fpgaA = 30;
+uint32_t    timeout_fpgaB = 30;
+uint32_t    timeout_mb    = TIMEOUT_MINIBOOT;
+uint32_t    timeout_mcpu[NUM_PRI_WINDOWS] = {TIMEOUT_NORMAL,TIMEOUT_NORMAL_SLOW};
+uint32_t    timeout_pka   = 30;
+uint32_t    timeout_skch  = 30;
+uint32_t    timeout_ssp   = TIMEOUT_SSP_MB;
+int         use_msi  = 1;
+int         use_msix = 0;
+
+atomic_t    temp_soft_tamper_detected;
+
+static int  ibm4767_major;
+static int  timeout_count;
+
+MODULE_AUTHOR("Jimmie Mayfield");
+MODULE_DESCRIPTION("Device driver for IBM 4767 Cryptographic Adapter");
+MODULE_LICENSE("GPL");
+MODULE_INFO(supported, "external");
+
+module_param_array(timeout_mcpu, uint, &timeout_count, S_IRUGO);
+module_param(timeout_cdu,      uint, S_IRUGO | S_IWUSR );
+module_param(timeout_fpgaA,    uint, S_IRUGO | S_IWUSR );
+module_param(timeout_fpgaB,    uint, S_IRUGO | S_IWUSR );
+module_param(timeout_mb,       uint, S_IRUGO | S_IWUSR );
+module_param(timeout_pka,      uint, S_IRUGO | S_IWUSR );
+module_param(timeout_skch,     uint, S_IRUGO | S_IWUSR );
+module_param(timeout_ssp,      uint, S_IRUGO | S_IWUSR );
+
+module_param(allow_indirect,   uint,  S_IRUGO | S_IWUSR );
+module_param(auto_reset,       uint,  S_IRUGO | S_IWUSR );
+module_param(dbg_level,        uint,  S_IRUGO | S_IWUSR );
+module_param(enable_ecrc,      uint,  S_IRUGO | S_IWUSR );
+module_param(enable_fuzzer,    uint,  S_IRUGO | S_IWUSR );
+module_param(ignore_tamper,    uint,  S_IRUGO | S_IWUSR );
+module_param(num_msix,         uint,  S_IRUGO | S_IWUSR );
+module_param(pcie_link,        uint,  S_IRUGO | S_IWUSR );
+module_param(post1_jump,       uint,  S_IRUGO | S_IWUSR );
+module_param(postde_echo,      uint,  S_IRUGO | S_IWUSR );
+module_param(promote_ssp_reset,uint,  S_IRUGO | S_IWUSR );
+module_param(reset_on_panic,   uint,  S_IRUGO | S_IWUSR );
+module_param(sev0_ignore,      uint,  S_IRUGO | S_IWUSR );
+module_param(sev0_limit,       uint,  S_IRUGO | S_IWUSR );
+module_param(sev0_timeframe,   uint,  S_IRUGO | S_IWUSR );
+module_param(use_msi,          uint,  S_IRUGO);
+module_param(use_msix,         uint,  S_IRUGO);
+
+MODULE_PARM_DESC(allow_indirect,"allow indirect register operations (DS3645)");
+MODULE_PARM_DESC(auto_reset,    "automatically reset IBM 4767 upon serious error");
+MODULE_PARM_DESC(dbg_level,     "verbosity of debug logs (0=minimum, 5=verbose)");
+MODULE_PARM_DESC(enable_ecrc,   "enable PCI-E ECRC");
+MODULE_PARM_DESC(enable_fuzzer, "enable SAK-style fuzzer");
+MODULE_PARM_DESC(ignore_tamper, "do not disable device following tamper event");
+MODULE_PARM_DESC(num_msix,      "number of MSI-X to request (1, 2 or 4)");
+MODULE_PARM_DESC(pcie_link,     "expected PCIe link generation (1, 2 or 3)");
+MODULE_PARM_DESC(post1_jump,    "automatically jump to MB1 from POST1");
+MODULE_PARM_DESC(postde_echo,   "echo POST 0xDE.  do *NOT* reset afterwards");
+MODULE_PARM_DESC(promote_ssp_reset, "always promote SSP resets to full card resets");
+MODULE_PARM_DESC(reset_on_panic,"reset adapter upon kernel panic");
+MODULE_PARM_DESC(sev0_ignore,   "take no action on sev0 log messages");
+MODULE_PARM_DESC(sev0_limit,    "maximum # sev0-sev2 log messages allowed in <sev0_timeframe> minute span");
+MODULE_PARM_DESC(sev0_timeframe,"size of time window (in minutes) used for sev0 accounting");
+MODULE_PARM_DESC(timeout_cdu,   "timeout (secs) for CDU operations");
+MODULE_PARM_DESC(timeout_fpgaA, "timeout (secs) for FPGA-A FP operations");
+MODULE_PARM_DESC(timeout_fpgaB, "timeout (secs) for FPGA-B FP operations");
+MODULE_PARM_DESC(timeout_mb,    "timeout (secs) for Miniboot requests");
+MODULE_PARM_DESC(timeout_mcpu,  "per-window timeout for MCPU requests (in seconds)");
+MODULE_PARM_DESC(timeout_pka,   "timeout (secs) for PKA FP operations");
+MODULE_PARM_DESC(timeout_skch,  "timeout (secs) for SKCH FP operations");
+MODULE_PARM_DESC(timeout_ssp,   "timeout (secs) for SSP to boot to MB0");
+MODULE_PARM_DESC(use_msi,       "enable MSI");
+MODULE_PARM_DESC(use_msix,      "non-functional.  MSI interrupts are always used.");
+
+static int  ibm4767_init(void);
+static void ibm4767_exit(void);
+static int  ibm4767_probe(struct pci_dev *dev, const struct pci_device_id *id);
+static void ibm4767_remove(struct pci_dev *dev);
+
+#ifdef ENABLE_EEH
+static pci_ers_result_t ibm4767_error_detected(struct pci_dev *dev,
+                                               pci_channel_state_t state);
+#endif
+static pci_ers_result_t ibm4767_slot_reset(struct pci_dev *dev);
+#ifdef ENABLE_EEH
+static void ibm4767_eeh_resume(struct_pci_dev *dev);
+#endif
+
+#if defined(CONFIG_PM)
+static int ibm4767_resume(struct pci_dev *dev);
+static int ibm4767_suspend(struct pci_dev *dev, pm_message_t state);
+#else
+#define ibm4767_suspend NULL
+#define ibm4767_resume  NULL
+#endif
+
+module_init(ibm4767_init);
+module_exit(ibm4767_exit);
+
+static const struct pci_device_id ibm4767_ids[] = {
+	{ PCI_DEVICE(IBM4767_VENDOR_ID, IBM4767_DEVID_4767) },
+	{ PCI_DEVICE(IBM4767_VENDOR_ID, IBM4767_DEVID_4768) },
+	{ 0 }
+};
+
+
+#if defined(ENABLE_MODULE_DEVICE_TABLE)
+MODULE_DEVICE_TABLE(pci, ibm4767_ids);
+#endif
+
+static struct pci_error_handlers  ibm4767_err_handler = {
+#ifdef ENABLE_EEH
+        .error_detected = ibm4767_error_detected,
+#endif
+	.slot_reset     = ibm4767_slot_reset,
+#ifdef ENABLE_EEH
+        .resume         = ibm4767_eeh_resume,
+#endif
+};
+
+static struct pci_driver  pci_driver = {
+	.name         = DEV_NAME,
+	.err_handler  = &ibm4767_err_handler,
+	.id_table     = ibm4767_ids,
+	.probe        = ibm4767_probe,
+	.remove       = ibm4767_remove,
+	.resume       = ibm4767_resume,
+	.suspend      = ibm4767_suspend
+};
+
+struct file_operations ibm4767_fops =
+{ 
+	.owner 		= THIS_MODULE, 
+	.open 		= ibm4767_open, 
+	.release 	= ibm4767_release,
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)) 
+	.ioctl 		= ibm4767_ioctl,
+#else 
+	.unlocked_ioctl = ibm4767_ioctl,
+#endif
+
+#if defined(YC_USE_COMPAT_IOCTL) 
+	.compat_ioctl 	= ibm4767_compat_ioctl,
+#endif
+	.llseek 	= ibm4767_llseek     // Needed, but only to return a failure.
+};
+
+
+static uint32_t audit = 0;
+
+
+static int
+validate_module_params(void)
+{
+	char    *badtxt = NULL;
+	uint32_t badval = 0;
+
+	int i;
+
+	if (promote_ssp_reset != 0) {
+		PRINTKE("WARNING: promote_ssp_reset != 0 meaning that SSP resets will be promoted to\n"
+			"main card resets.  No sanity checks will be performed to make sure that the MCPU\n"
+			"is idle before such a reset.  YOU MUST NOT RUN CLU WHILE THE MCPU IS IN USE!\n");
+	}
+
+	for (i = 0; i < NUM_PRI_WINDOWS; i++) { 
+		if (timeout_mcpu[i] < MIN_TIMEOUT) { 
+			PRINTKE("ERROR: bad global timeout value (%d) specified for window %d " 
+					"(value should be at least %d seconds)\n", 
+					timeout_mcpu[i], i, MIN_TIMEOUT); 
+			return -EINVAL; 
+		} 
+	}
+        
+	if (timeout_cdu < MIN_TIMEOUT) {
+		badtxt = "CDU";
+		badval = timeout_cdu;
+	}
+
+        if (timeout_mb < MIN_TIMEOUT) {
+		badtxt = "MINIBOOT";
+		badval = timeout_mb;
+        }
+
+	if (timeout_ssp < MIN_TIMEOUT) {
+		badtxt = "SSP";
+		badval = timeout_ssp;
+        }
+
+	if (timeout_fpgaA < MIN_TIMEOUT) {
+		badtxt = "FPGA:A";
+		badval = timeout_fpgaA;
+        }
+
+	if (timeout_fpgaB < MIN_TIMEOUT) {
+		badtxt = "FPGA:B";
+		badval = timeout_fpgaB;
+        }
+
+	if (timeout_pka < MIN_TIMEOUT) {
+		badtxt = "PKA";
+		badval = timeout_pka;
+        }
+
+	if (timeout_skch < MIN_TIMEOUT) {
+		badtxt = "SKCH";
+		badval = timeout_skch;
+        }
+
+	if (badtxt) { 
+		PRINTKE("ERROR: bad global %s timeout value (%d) specified.  Must be at least %d seconds\n", 
+				badtxt, badval, MIN_TIMEOUT);
+		return -EINVAL;
+	}
+
+       
+	PRINTKW("Timeout values: CDU:%d MB:%d MCPU:%d,%d SSP:%d FPGA:%d,%d PKA:%d SKCH:%d\n",
+		       timeout_cdu, timeout_mb, timeout_mcpu[0], timeout_mcpu[1], timeout_ssp,
+		       timeout_fpgaA, timeout_fpgaB, timeout_pka, timeout_skch);
+
+
+        PRINTK("System HZ timer:  %d\n", HZ);
+
+        if ((dbg_level < 0) || (dbg_level > 5)) {
+                PRINTKE("ERROR: bad debug level specified (value should be between 0 and 5)\n");
+                return -EINVAL;
+        }
+
+#if 0
+	if (use_msi && use_msix) {
+		PRINTKE("ERROR: choose either use_msix=1 or use_msi=1.  not both.\n");
+		return -EINVAL;
+	}
+
+	if (use_msix) {
+		switch (num_msix) {
+			case 1:
+			case 2:
+			case 4: break;
+	
+			default: PRINTKE("ERROR: invalid num_msix (must be 1, 2 or 4).\n");
+				 return -EINVAL;
+		}
+	}
+#endif
+
+	return 0;
+}
+
+
+static __init int
+ibm4767_init(void)
+{
+	dev_t  dev;
+	int    i, rc;
+
+	PRINTKE("%s, version %d.%d.%d, build %d initializing\n",
+			DEV_NAME,
+			DRIVER_VERSION, DRIVER_RELEASE, DRIVER_VARIANT, DRIVER_BUILD);
+
+	rc = validate_module_params();
+	if (rc)
+		return rc;
+
+	rwlock_init(&ibm4767_lock);
+
+	for (i=0; i < MAX_DEV_COUNT; i++) {
+		ibm4767_list[i] = NULL;
+		tasklet_shutdown[i] = FALSE;
+	}
+	ibm4767_count = 0;
+
+	// obtain a major number
+	rc = alloc_chrdev_region(&dev, 0, MAX_DEV_COUNT, DEV_NAME);
+	if (rc < 0) {
+		PRINTKE("alloc_chrdev_region failed with %d\n", rc);
+		return rc;
+	}
+	audit |= AUDIT_REGISTER_CHRDEV;
+
+	ibm4767_major = MAJOR(dev);
+
+	atomic_set(&temp_soft_tamper_detected, 0);
+
+	// must setup procfs before registering the driver...
+	ibm4767_procfs_setup();
+	audit |= AUDIT_PROCFS;
+
+	rc = pci_register_driver(&pci_driver);
+	if (rc < 0) {
+		PRINTKE("pci_register_driver failed with %d\n", rc);
+		ibm4767_exit();
+		return rc;
+	}
+	audit |= AUDIT_REGISTER_DRIVER;
+	
+#if defined(YC_USE_OLD_IOCTL32)
+	rc = ibm4767_register_ioctl32_handlers();
+	if (rc) {
+		PRINTKW(">>> register_ioctl32_conversion failed           <<<\n");
+		PRINTKW(">>> DRIVER WILL NOT SUPPORT 32-BIT APPS.  SORRY. <<<\n");
+		ibm4767_unregister_ioctl32_handlers();
+	}
+	audit |= AUDIT_REGISTER_IOCTL32;
+#endif
+
+	return 0;
+}
+
+
+static void
+ibm4767_exit(void)
+{
+	PDEBUG(1, "Enter %s\n", __FUNCTION__);
+
+#if defined(YC_USE_OLD_IOCTL32)
+	if (audit & AUDIT_REGISTER_IOCTL32)
+		ibm4767_unregister_ioctl32_handlers();
+#endif
+
+	if (audit & AUDIT_REGISTER_DRIVER)
+		pci_unregister_driver(&pci_driver);
+
+	if (audit & AUDIT_PROCFS)
+		ibm4767_procfs_cleanup();
+
+	if (audit & AUDIT_REGISTER_CHRDEV)
+		unregister_chrdev_region(MKDEV(ibm4767_major, 0), MAX_DEV_COUNT);
+
+	PDEBUG(1, "Leave %s\n", __FUNCTION__);
+}
+
+
+int
+ibm4767_setup_msix(ibm4767_ctx_t *ctx)
+{
+	int rc, i, j;
+
+	PDEBUG(1, "Enter %s\n", __FUNCTION__);
+
+	ctx->msi_enabled = 0;
+	ctx->msix_enabled = 0;
+
+	switch (num_msix) {
+		case 4:   write32(cpu_to_be32(HMVMC_4_VEC), HMVMC(ctx));
+			  break;
+		case 2:   write32(cpu_to_be32(HMVMC_2_VEC), HMVMC(ctx));
+			  break;
+		default:  write32(cpu_to_be32(HMVMC_1_VEC), HMVMC(ctx));
+			  break;
+	}
+
+	PDEBUG(1, "Asking for %d MSI-X\n", num_msix);
+
+	PDEBUG(1, "%s: sanity check 1: HIER = 0x%08x\n", __FUNCTION__, cpu_to_be32(read32(ctx)));
+
+	do {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,13,0))
+		// I don't know exactly when pci_enable_msi() was removed.
+		// but it's been deprecated in the official kernel source trees
+		// since at least 3.10.  unfortunately, it looks like SLES 12.1
+		// (kernel 3.12.xx) does not define pci_enable_msix_exact()...
+		//
+		rc = pci_enable_msix(ctx->pci_dev, ctx->msix_entries, num_msix);
+#else
+		rc = pci_enable_msix_exact(ctx->pci_dev, ctx->msix_entries, num_msix);
+#endif
+		if (rc < 0) {
+			PRINTKW("enable msix failed rc 0x%08x\n", rc);
+			pci_disable_msix(ctx->pci_dev);
+			return -EIRQFAIL;
+		}
+		if (rc > 0) {
+			// if our attempt to grab 'num_msix' MSI-Xes failed...just ask for 1
+			num_msix = 1;
+		}
+	} while (rc > 0);
+		
+	PDEBUG(1, "%s: sanity check 2: HIER = 0x%08x\n", __FUNCTION__, cpu_to_be32(read32(ctx)));
+
+	for (i=0; i < num_msix; i++) {
+		rc = request_irq(ctx->msix_entries[i].vector,
+				 ibm4767_isr, 
+				0, 
+				DEV_NAME, 
+				ctx); 
+		if (rc) { 
+			PRINTKW("request_irq failed for MSIX i: %08x  rc: %08x\n", i, rc); 
+			for (j=0; j < i; j++) 
+				free_irq(ctx->msix_entries[j].vector, ctx); 
+			pci_disable_msix(ctx->pci_dev); 
+			return -EIRQFAIL; 
+		}
+	}
+	
+	PDEBUG(1, "%s: sanity check 3: HIER = 0x%08x\n", __FUNCTION__, cpu_to_be32(read32(ctx)));
+
+	ctx->msix_enabled = 1;
+	return 0;
+}
+
+
+int
+ibm4767_setup_msi(ibm4767_ctx_t *ctx)
+{
+	int rc;
+
+	PDEBUG(1, "Enter %s\n", __FUNCTION__);
+
+	ctx->msi_enabled  = 0;
+	ctx->msix_enabled = 0;
+
+	rc = pci_enable_msi(ctx->pci_dev);
+	if (rc) {
+		PRINTKW("pci_enable_msi failed rc %08x\n", rc);
+		pci_disable_msi(ctx->pci_dev);
+		return -EIRQFAIL;
+	}
+
+	rc = request_irq(ctx->pci_dev->irq, ibm4767_isr, IRQF_SHARED, DEV_NAME, ctx); 
+	if (rc) { 
+		PRINTKW("request_irq failed for MSI.  rc %08x\n", rc); 
+		return -EIRQFAIL;
+	}
+	ctx->msi_enabled = 1;
+	return 0;
+}
+
+
+void
+ibm4767_free_msix(ibm4767_ctx_t *ctx)
+{
+	int i;
+
+	for (i=0; i < num_msix; i++) 
+		free_irq(ctx->msix_entries[i].vector, ctx);
+	pci_disable_msix(ctx->pci_dev);
+
+	ctx->msix_enabled = 0;
+}
+
+
+void
+ibm4767_free_msi(ibm4767_ctx_t *ctx)
+{
+	free_irq(ctx->pci_dev->irq, ctx);
+	pci_disable_msi(ctx->pci_dev);
+
+	ctx->msi_enabled = 0;
+}
+
+
+static int
+ibm4767_probe(struct pci_dev *device, const struct pci_device_id *id)
+{
+	ibm4767_ctx_t  *ctx = NULL;
+	uint64_t      mask32, mask64;
+	uint32_t      hisr;
+	int           idx, rc;
+
+
+	PDEBUG(1, "Enter %s\n", __FUNCTION__);
+
+	if (!device) {
+		PRINTKW("%s: device == NULL!\n", __FUNCTION__);
+		return HOST_DD_NoDevice;
+	}
+
+	if (!device->bus) {
+		PRINTKW("%s: device->bus == NULL!\n", __FUNCTION__);
+		return HOST_DD_NoDevice;
+	}
+
+	// if running in a VM via PCI pass-through, it's possible or even likely that the
+	// bridge device won't be present...
+	if (!device->bus->self) {
+		PRINTKW("%s: No bridge detected...(running virtualized mabye?)\n", __FUNCTION__);
+	}
+	else {
+		PDEBUG(1, "IBM 4767 Bridge vendor number = 0x%04x\n", (u_int)(device->bus->self->vendor));
+		PDEBUG(1, "IBM 4767 Bridge device number = 0x%04x\n", (u_int)(device->bus->self->device));
+	}
+
+	PDEBUG(1, "IBM 4767 vendor number = 0x%04x\n", (u_int)(device->vendor));
+	PDEBUG(1, "IBM 4767 device number = 0x%04x\n", (u_int)(device->device));
+
+	mask32 = DMA_BIT_MASK(32);
+	mask64 = DMA_BIT_MASK(64);
+        if (!pci_set_dma_mask(device, mask64)) {
+                PDEBUG(1, "64-bit DAC enabled...\n");
+                pci_set_consistent_dma_mask(device, mask64);
+        }
+        else {
+                PDEBUG(1, "32-bit SAC enabled...\n");
+                pci_set_dma_mask(device, mask32);
+                pci_set_consistent_dma_mask(device, mask32);
+        }
+
+
+	ctx = kmalloc(sizeof(ibm4767_ctx_t), GFP_KERNEL);
+	if (!ctx) {
+		PRINTKE("can't allocate device context\n");
+		return HOST_DD_NoMemory;
+	}
+	memset(ctx, 0, sizeof(ibm4767_ctx_t));
+	audit |= AUDIT_CTX_ALLOC;
+
+	pci_set_drvdata(device, ctx);
+
+	write_lock(&ibm4767_lock);
+	
+	// it might appear that assigning to ibm4767_list[ibm4767_count++] is simpler but
+	// that won't work if we eventually support hotplug...
+	//
+	for (idx=0; (idx < MAX_DEV_COUNT) && (ibm4767_list[idx] != NULL); idx++)
+		;
+	if (idx == MAX_DEV_COUNT) {
+		write_unlock(&ibm4767_lock);
+		PRINTKE("no more device slots available\n");
+		rc = HOST_DD_NoMemory;
+		goto error;
+	}
+	
+	ibm4767_list[idx] = ctx;
+	ibm4767_count++;
+	write_unlock(&ibm4767_lock);
+
+	ctx->dev        = &device->dev;
+	ctx->pci_dev    = device;
+	ctx->bridge_dev = device->bus->self;
+	ctx->dev_index  = idx;
+	
+	ctx->status_mcpu = MCPU_UNINITIALIZED;
+	ctx->status_ssp  = SSP_UNINITIALIZED;
+	ctx->status_cdu  = CDU_INACTIVE;
+	
+	tasklet_init(&ctx->interrupt_dpc, ibm4767_interrupt_tasklet, (unsigned long)idx);
+
+	spin_lock_init(&ctx->fetch_lock_mcpu);
+	spin_lock_init(&ctx->fetch_lock_ssp);
+	spin_lock_init(&ctx->fetch_lock_skch);
+	spin_lock_init(&ctx->fetch_lock_pka);
+	spin_lock_init(&ctx->fetch_lock_fpgaA);
+	spin_lock_init(&ctx->fetch_lock_fpgaB);
+	spin_lock_init(&ctx->counter_lock);
+	spin_lock_init(&ctx->timeout_lock);
+	spin_lock_init(&ctx->indirect_lock);
+
+	ctx->active_opens = 0;
+
+	if ((rc = pci_enable_device(ctx->pci_dev))) {
+		PRINTKE("pci_enable_device failed: rc = 0x%08x\n", rc);
+		goto error;
+	}
+	audit |= AUDIT_PCI_ENABLE;
+
+	PDEBUG(1, "start is 0x%016llX\n", (u64)ctx->pci_dev->resource[0].start);
+	PDEBUG(1, "end   is 0x%016llX\n", (u64)ctx->pci_dev->resource[0].end);
+	PDEBUG(1, "request_mem_region(0x%016llX, 0x%016llX)\n", 
+			(u64)pci_resource_start(ctx->pci_dev, 0), 
+			(u64)pci_resource_len(ctx->pci_dev, 0));
+
+	if (!request_mem_region(pci_resource_start(ctx->pci_dev, 0), pci_resource_len(ctx->pci_dev, 0), DEV_NAME)) { 
+		PRINTKE("request_mem_region failed\n"); 
+		rc = HOST_DD_NoMemory; 
+		goto error; 
+	}
+	audit |= AUDIT_MEM_REGION;
+        
+	ctx->io_va = (u_char *)ioremap(pci_resource_start(ctx->pci_dev, 0), pci_resource_len(ctx->pci_dev, 0));
+	if (!ctx->io_va) {
+		PRINTKE("ioremap failed!\n");
+		rc = -EIO;
+		goto error;
+	}
+
+	audit |= AUDIT_IO_REMAP;
+
+	if (pcie_link != 0) {
+		uint64_t tmp64 = cpu_to_be64(read64(PCIE_TRAINING_CTRL(ctx)));
+		int link_gen = ((tmp64 >> 24) & 0xF); // link speed are bits 36:39
+		if (link_gen != pcie_link) {
+			PRINTKE(">>>>>\n");
+			PRINTKE(">>>>> ERROR!! Device %d:  PCIe link trained at 0x%02x,  module parameter 'pcie_link' specifies 0x%02x\n", ctx->dev_index, link_gen, pcie_link);
+			PRINTKE(">>>>>\n");
+			rc = -EIO;
+			goto error;
+		}
+	}
+
+	if ((rc = request_pool_initialize(ctx)) != HOST_DD_Good)
+		goto error;
+	audit |= AUDIT_REQUEST_POOL;
+
+	if ((rc = htb_allocate(ctx)) != HOST_DD_Good)
+		goto error;
+	audit |= AUDIT_HTB_ALLOC;
+
+	if ((rc = dt_initialize(ctx)) != HOST_DD_Good)
+		goto error;
+	audit |= AUDIT_DT_INIT;
+
+	if ((rc = asym_hra_pool_initialize(ctx)) != HOST_DD_Good)
+		goto error;
+	audit |= AUDIT_HRA_INIT;
+
+	memset(&ctx->cdev, 0x0, sizeof(struct cdev));
+	cdev_init(&ctx->cdev, &ibm4767_fops);
+	ctx->cdev.owner = THIS_MODULE;
+
+	rc = cdev_add(&(ctx->cdev), MKDEV(ibm4767_major, idx), 1); 
+	if (rc) { 
+		PDEBUG(1, "cdev_add error code is 0x%08x\n", rc); 
+		rc = HOST_DD_NoMemory; 
+		goto error; 
+	}
+	audit |= AUDIT_CDEV_ADD;
+
+	// if a card in the system has a temperature soft tamper, we shut down all cards to try to 
+	// prevent other cards from continuing to heat the tampered card and turning it into a permanent
+	// temperature tamper.  this is tricky during driver startup since the probe() routine is called 
+	// by the kernel as devices are enumerated so 'ibm4767_list' is not fully-populated 
+	// at this point.
+	//
+	if (atomic_read(&temp_soft_tamper_detected) != 0) {
+		// pre-existing soft tamper from another card?
+		ctx->status_mcpu   = MCPU_TEMP_SHUTDOWN;
+		ctx->status_ssp    = SSP_TEMP_SHUTDOWN;
+		ctx->status_tamper = TAMPER_SOFT_TEMP;
+		dma_disable(ctx, HBMCR_BMEN_ALL);
+	}
+	else {
+		// no, then check soft tamper for this card
+		uint32_t hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+		if (hrcsr & HRCSR_SOFT_TEMP) {
+			atomic_set(&temp_soft_tamper_detected, 1);
+			PRINTKE("Soft tamper (temp) active for device %d (S/N %s) during driver initialization.  Disabling all 4767 devices\n", ctx->dev_index, ctx->hwinfo.serial_num);
+			ibm4767_do_temperature_shutdown(NULL);
+		}
+	}
+
+	INIT_LIST_HEAD(&ctx->sev0_list);
+
+	spin_lock_init(&ctx->edump_lock);
+
+	ctx->edump_mcpu = (char *)vmalloc(MCPU_EMERGENCY_DUMP_BUFSIZE);
+	if (!ctx->edump_mcpu) {
+		PRINTKW("Can't allocate MCPU emergency dump buffer\n");
+		rc = HOST_DD_NoMemory;
+		goto error;
+	}
+
+	ctx->edump_ssp = (char *)vmalloc(SSP_EMERGENCY_DUMP_BUFSIZE);
+	if (!ctx->edump_ssp) {
+		PRINTKW("Can't allocate SSP emergency dump buffer\n");
+		rc = HOST_DD_NoMemory;
+		goto error;
+	}
+
+#if defined(ENABLE_MFG)
+	ctx->edump_ssp_tas = (char *)vmalloc(TAS_EMERGENCY_DUMP_SIZE);
+	if (!ctx->edump_ssp_tas) {
+		PRINTKW("Can't allocate TAS SSP emergency dump buffer\n");
+		rc = HOST_DD_NoMemory;
+		goto error;
+	}
+
+	ctx->ssp_mbxhist_list = (uint64_t *)vmalloc(sizeof(uint64_t) * MAILBOX_HISTORY_LEN);
+	if (!ctx->ssp_mbxhist_list) {
+		PRINTKW("Can't allocate SSP mailbox history\n");
+		rc = HOST_DD_NoMemory;
+		goto error;
+	}
+
+	ctx->mcpu_mbxhist_list = (uint64_t *)vmalloc(sizeof(uint64_t) * MAILBOX_HISTORY_LEN);
+	if (!ctx->mcpu_mbxhist_list) {
+		PRINTKW("Can't allocate MCPU mailbox history\n");
+		rc = HOST_DD_NoMemory;
+		goto error;
+	}
+
+	ctx->ssp_mbxhist_r_idx = 0;
+	ctx->ssp_mbxhist_w_idx = 0;
+	ctx->mcpu_mbxhist_r_idx = 0;
+	ctx->mcpu_mbxhist_w_idx = 0;
+	
+	spin_lock_init(&ctx->ssp_mbxhist_lock);
+	spin_lock_init(&ctx->mcpu_mbxhist_lock);
+#endif
+	
+	ibm4767_procfs_add_device(ctx->dev_index);
+
+	// the following must come before the first main reset
+	ibm4767_timers_init(ctx);
+	ibm4767_workqueues_init(ctx);
+
+	hisr = cpu_to_be32(read32(HISR(ctx)));
+	write32(cpu_to_be32(HISR_HW_ERR), HISR(ctx));
+	rc = ibm4767_main_reset(ctx, TRUE);
+	if (rc != HOST_DD_Good)
+		goto error;
+
+	rc = ibm4767_setup_msi(ctx);
+	if (rc < 0) {
+		rc = -EIRQFAIL;
+		goto error;
+	}
+	audit |= AUDIT_IRQ;
+
+    
+	PDEBUG(1, "cdev_add success\n");
+	return rc;
+
+error:
+	if (ctx->edump_mcpu) vfree(ctx->edump_mcpu);
+	if (ctx->edump_ssp)  vfree(ctx->edump_ssp);
+
+#if defined(ENABLE_MFG)
+	if (ctx->edump_ssp_tas) vfree(ctx->edump_ssp_tas);
+#endif
+
+	if (audit & AUDIT_HRA_INIT)
+		asym_hra_pool_unload(ctx);
+
+	if (audit & AUDIT_DT_INIT)
+		dt_unload(ctx);
+
+	if (audit & AUDIT_HTB_ALLOC)
+		htb_unload(ctx);
+	
+	if (audit & AUDIT_REQUEST_POOL)
+		request_pool_unload(ctx);
+
+	if (audit & AUDIT_IRQ)
+		ibm4767_free_msi(ctx);
+	
+	if (audit & AUDIT_IO_REMAP)
+		iounmap((void *)ctx->io_va);
+	
+	if (audit & AUDIT_MEM_REGION) {
+		release_mem_region(pci_resource_start(ctx->pci_dev, 0), 
+				   pci_resource_len(ctx->pci_dev, 0));
+	}
+
+	if (audit & AUDIT_PCI_ENABLE)
+		pci_disable_device(ctx->pci_dev);
+	
+	if (audit & AUDIT_CDEV_ADD)
+		cdev_del(&ctx->cdev);
+
+	if (audit & AUDIT_CTX_ALLOC)
+		kfree(ctx);
+
+
+	PDEBUG(1, "probe failure\n");
+	return rc;
+}
+
+
+static void
+ibm4767_remove(struct pci_dev *dev)
+{
+	ibm4767_ctx_t *ctx;
+
+
+	ctx = pci_get_drvdata(dev);
+	tasklet_shutdown[ctx->dev_index] = TRUE;
+	
+	PDEBUG(1, "Enter %s for device %d...\n", __FUNCTION__, ctx->dev_index);
+
+	dma_disable(ctx, HBMCR_BMEN_ALL); 
+	write32(0L, HIER(ctx));
+
+	ctx->status_mcpu = MCPU_UNINITIALIZED;
+	ctx->status_ssp  = SSP_UNINITIALIZED;
+	ctx->status_cdu  = CDU_INACTIVE;
+
+	ibm4767_timers_unload(ctx);
+	ibm4767_workqueues_unload(ctx);
+	asym_hra_pool_unload(ctx);
+	htb_unload(ctx);
+	dt_unload(ctx);
+	request_pool_unload(ctx);
+
+	sev0_purge(ctx);
+
+	ibm4767_free_msi(ctx);
+
+	ibm4767_procfs_remove_device(ctx->dev_index);
+
+	iounmap((void *)ctx->io_va);
+
+	release_mem_region(pci_resource_start(ctx->pci_dev, 0),
+			   pci_resource_len(ctx->pci_dev, 0));
+
+	pci_disable_device(ctx->pci_dev);
+	
+	write_lock(&ibm4767_lock);
+	ibm4767_list[ctx->dev_index] = NULL;
+	ibm4767_count--;
+	write_unlock(&ibm4767_lock);
+
+	cdev_del(&ctx->cdev);
+
+	if (ctx->edump_mcpu) vfree(ctx->edump_mcpu);
+	if (ctx->edump_ssp)  vfree(ctx->edump_ssp);
+#if defined(ENABLE_MFG)
+	if (ctx->edump_ssp_tas)  vfree(ctx->edump_ssp_tas);
+#endif
+
+	kfree(ctx);
+}
+
+
+// PCI advanced error recovery is dicey or non-functional under x86 but in the
+// future someone may decide to assume ownership of that part of the PCI code.  if that
+// happens, then this routine will get called when a slot reset occurs following
+// a PCI error...
+//
+
+#ifdef ENABLE_EEH
+// Callback when PCI error is detected
+static pci_ers_result_t
+ibm4767_error_detected(struct pci_dev *dev,pci_channel_state_t state)
+{
+  ibm4767_ctx_t *ctx = pci_get_drvdata(dev);
+
+  PRINTKW("Device %d: PCI error %d detected.  Marking card offline...\n",
+          ctx->dev_index,state);
+
+  mark_device_offline(ctx,HOSTHardwareError);
+
+  if (state == pci_channel_io_perm_failure)
+    return PCI_ERS_RESULT_DISCONNECT;
+
+  pci_disable_device(dev);
+  return PCI_ERS_RESULT_NEED_RESET;
+}
+#endif
+
+static pci_ers_result_t 
+ibm4767_slot_reset(struct pci_dev *dev)
+{
+	ibm4767_ctx_t *ctx = pci_get_drvdata(dev);
+
+	PRINTKW("Device %d: slot reset detected.  Attempting to recover...\n", ctx->dev_index);
+
+	if (pci_enable_device(ctx->pci_dev)) {
+		PRINTKE("Device %d: can't re-enable device after slot reset\n", ctx->dev_index);
+		return PCI_ERS_RESULT_DISCONNECT;
+	}
+
+	pci_set_master(ctx->pci_dev);
+
+	ibm4767_main_reset(ctx, TRUE);
+
+	return PCI_ERS_RESULT_RECOVERED;
+}
+
+#ifdef ENABLE_EEH
+static void
+ibm4767_eeh_resume(struct pci_dev *dev)
+{
+  // Do nothing - card remains offline until driver is reloaded
+}
+#endif
+
+static int
+ibm4767_resume(struct pci_dev *dev)
+{
+	ibm4767_ctx_t *ctx = pci_get_drvdata(dev);
+
+	if (!ctx)
+		return 0;
+
+	PRINTKW("Device %d: ibm4767_resume invoked!\n", ctx->dev_index);
+	return 0;
+}
+
+
+static int
+ibm4767_suspend(struct pci_dev *dev, pm_message_t state)
+{
+	ibm4767_ctx_t *ctx = pci_get_drvdata(dev);
+
+	if (!ctx)
+		return HOST_DD_DeviceBusy;
+
+	PRINTKW("Device %d: ibm4767_suspend invoked\n", ctx->dev_index);
+	return HOST_DD_DeviceBusy;
+}
+
+
+int
+ibm4767_open(struct inode *inode, struct file *fp)
+{
+	struct priv_data_t *priv  = NULL; 
+	int  minor = iminor(inode); 
+	int  rc = 0;
+
+
+	if (minor >= MAX_DEV_COUNT) {
+		PRINTK("ERROR: open attempt for invalid device minor %d\n", minor);
+		return HOST_DD_NoDevice;
+	}
+
+	read_lock(&ibm4767_lock);
+	if (ibm4767_list[minor] == NULL) {
+		PRINTK("ERROR: open attempt for non-existent device minor %d\n", minor);
+		rc = HOST_DD_NoDevice;
+		goto done;
+	}
+
+	if (!(priv = kmalloc(sizeof(struct priv_data_t), GFP_ATOMIC))) {
+		rc = HOST_DD_NoMemory;
+		goto done;
+	}
+
+	memset(priv, 0, sizeof(struct priv_data_t));
+	
+	priv->ctx = ibm4767_list[minor];
+	fp->private_data = priv;
+
+	spin_lock_bh(&priv->ctx->counter_lock);
+	priv->ctx->active_opens++;
+	spin_unlock_bh(&priv->ctx->counter_lock);
+
+done:
+	read_unlock(&ibm4767_lock);
+	return rc;
+}
+
+
+// 'release' is invoked after a file handle is closed
+//
+int
+ibm4767_release(struct inode *inode, struct file *fp)
+{
+	struct priv_data_t *priv = fp->private_data;
+	ibm4767_ctx_t      *ctx = priv->ctx;
+	uint32_t            hrcsr;
+
+
+	if (ctx) {
+		PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+	
+		// if we've performed any SSP operations that left the MCPU in reset, then we need
+		// to wake the MCPU.  
+		//
+		// miniboot no longer drops the MCPU into reset.  instead it puts it in debug mode.
+		// unfortunately, this means that HRCSR will indicate MCPU is awake.  so we try to
+		// keep track of the expected MCPU state changes based on the miniboot commands that
+		// have been issued.
+		//
+		// unfortunately, POST1 is probably not smart enough to bring the MCPU out of
+		// debug mode so we need to reset the device at this point.
+		//
+
+		spin_lock_bh(&ctx->counter_lock);
+		ctx->active_opens--;
+		ctx->status_flags |= FLAG_LOCKED;
+		spin_unlock_bh(&ctx->counter_lock);
+
+		// is there ever a situation where we'd want to defer a main reset (or reset the SSP) when
+		// another process has an open session?  I can't think of one...
+		//
+		if (!ctx->active_opens && priv->req_sent) {
+			hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+			if ((hrcsr & HRCSR_MCPU_RstS) || (ctx->status_mcpu == MCPU_IN_RESET)) {
+				defer_main_reset(ctx, 1, FALSE);
+			}
+			else {
+				// if we entered miniboot during bootup following a main reset then the MCPU
+				// might be awake at this point sitting in POST2 waiting for miniboot to give
+				// the proceed-to-seg2 command.  we reset the SSP at this point so that if the
+				// MCPU is in this state, MB1 will give the go-ahead to proceed to seg2...
+				//
+				if (priv->need_ssp_reset) {
+					ibm4767_reset_ssp(ctx, FALSE);
+				}
+			}
+		}
+
+		spin_lock_bh(&ctx->counter_lock);
+		ctx->status_flags &= ~FLAG_LOCKED;
+		spin_unlock_bh(&ctx->counter_lock);
+
+	}
+	else {
+		PDEBUG(1, "Unknown device: %s\n", __FUNCTION__);
+	}
+
+	memset(priv, 0x0, sizeof(struct priv_data_t));
+	kfree(priv);
+	fp->private_data = NULL;
+
+	return 0;
+}
+
+
+loff_t
+ibm4767_llseek(struct file *fp, loff_t off, int whence)
+{ return -ESPIPE; }
+
+
diff --git drivers/misc/ibm4767/procfs.c drivers/misc/ibm4767/procfs.c
new file mode 100755
index 000000000000..3df332b36b09
--- /dev/null
+++ drivers/misc/ibm4767/procfs.c
@@ -0,0 +1,1769 @@
+/*************************************************************************
+ *  Filename:procfs.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  proc file system interface.                                        
+ *                                         
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+//
+// FIXME - we should probably migrate towards using sysfs instead of procfs
+//
+
+// this file is responsible for maintaining the following /proc hierarchy:
+//
+// /proc/drivers/ibm4767/driver         - R/O global driver status
+// /proc/drivers/ibm4767/timeout        - R/W default symmetric request timeout
+// /proc/drivers/ibm4767/dbg_level      - R/W global debug log level
+// /proc/drivers/ibm4767/<device #>     - R/W device status
+//     write 'E' or 'D' to this file to manually enable/disable device
+//
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#ifndef CONFIG_PROC_FS
+#error "Kernel must be built with CONFIG_PROC_FS enabled"
+#endif
+
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/proc_fs.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_mailbox.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+
+struct proc_dir_entry *ibm4767_dir;
+int    proc_initialized = FALSE;
+int    dev_entries[MAX_DEV_COUNT];
+
+#define Y_PROCFS_IBM4767	0x00000001
+#define Y_PROCFS_DBGLVL 	0x00000002
+#define Y_PROCFS_DRIVER		0x00000004
+#define Y_PROCFS_TIMEOUT	0x00000008
+#define Y_PROCFS_EDUMP  	0x00000010
+#define Y_PROCFS_HWERR  	0x00000020
+#define Y_PROCFS_RESET  	0x00000040
+#define Y_PROCFS_CRITERR	0x00000080
+#define Y_PROCFS_AGENTIDS       0x00000100
+#define Y_PROCFS_ENABLEFP       0x00000200
+#define Y_PROCFS_FIFOS          0x00000400
+#define Y_PROCFS_BARREG         0x00001000
+#define Y_PROCFS_DS3645         0x00002000
+#define Y_PROCFS_LOGGER         0x00004000
+
+static uint32_t procfs_audit = 0;
+
+static uint32_t procfs_ds3645_target = 0;
+
+
+//--------------------------------------------------------------------------------------------------
+// to force driver to send LIST_ALL_AGENTIDS command to a specific card...
+//    echo <card #> > /proc/driver/ibm4767/agentids
+//
+#if defined(DEBUG)
+static int 
+procfs_agentids_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t
+procfs_agentids_write(struct file *file, const char *buffer, 
+		      size_t count, loff_t *pos)
+{
+	ibm4767_ctx_t *ctx = NULL;
+	unsigned char buf[80];
+	uint32_t len, num;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d", &num) != 1)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	read_lock(&ibm4767_lock); 
+	ctx = ibm4767_list[num];
+	if (ctx) {
+		mod_timer(&ctx->agentid_list_timer, jiffies + HZ*2);
+
+		PRINTKW("User-invoked LIST_ALL_AGENTIDS sent to device %d\n", num);
+		write32(cpu_to_be32(MBX_LIST_ALL_AGENTIDS), H2M_MBX_H(ctx));
+	}
+	else 
+		PRINTKW("%s - agentids to nonexistent device %d\n", __FUNCTION__, num);
+
+	read_unlock(&ibm4767_lock); 
+	return count;
+}
+
+	
+static int
+procfs_agentids_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_agentids_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_agentids_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_agentids_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_agentids_write,
+};
+#else
+static const struct proc_ops procfs_agentids_fops = {
+	.proc_open = procfs_agentids_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_agentids_write
+};
+#endif
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// nihad requested a routine to read 8 bytes at the specified offset from BAR address space...
+//   echo "R <device #> [hex offset]"             > /proc/driver/ibm4767/barreg
+//   echo "W <device #> [hex offset] [hex value]" > /proc/driver/ibm4767/barreg
+//
+//   offset is in hex, device # and # bytes are in dec
+//
+static int 
+procfs_barreg_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t
+procfs_barreg_write(struct file *file, const char *buffer, 
+		    size_t count, loff_t *pos)
+{
+	ibm4767_ctx_t *ctx = NULL;
+	unsigned char str[256], junk;
+	uint64_t      val;
+	unsigned int  len, num, offset;
+
+	len = count;
+	if (len > sizeof(str)-1)
+		len = sizeof(str)-1;
+
+	if (copy_from_user(str, buffer, len))
+		return -EFAULT;
+
+	str[len] = 0;
+
+	if (str[0] == 'R' || str[0] == 'r') {
+		if (sscanf(str, "%c %d %x", &junk, &num, &offset) != 3)
+			return -EINVAL;
+
+		if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+			PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+			return -EFAULT;
+		}
+		ctx = ibm4767_list[num];
+
+		val = read64(ctx->io_va + offset);
+		
+		sprintf(str, "Device %d: Dump 8 bytes at BAR offset 0x%x:", ctx->dev_index, offset);
+		hex_dump(ctx, 1, str, (void *)&val, 8);
+	}
+	else if (str[0] == 'W' || str[0] == 'w') {
+		if (sscanf(str, "%c %d %x %llx", &junk, &num, &offset, &val) != 4)
+			return -EINVAL;
+
+		if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+			PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+			return -EFAULT;
+		}
+		ctx = ibm4767_list[num];
+
+		write64(cpu_to_be64(val), (ctx->io_va + offset));
+	}
+	else {
+		PRINTKW("%s:  Invalid args\n", __FUNCTION__);
+		return -EFAULT; 
+	}
+
+	return count;
+}
+
+
+static int
+procfs_barreg_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_barreg_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_barreg_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_barreg_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_barreg_write,
+};
+#else
+static const struct proc_ops procfs_barreg_fops = {
+	.proc_open = procfs_barreg_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_barreg_write
+};
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// to force a critical error dump for a specific card:
+//    echo <card#> > /proc/drivers/ibm4767/criterr
+//
+#if defined(DEBUG)
+static int 
+procfs_criterr_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t
+procfs_criterr_write(struct file *file, const char *buffer, 
+		     size_t count, loff_t *pos)
+{
+	unsigned char buf[80];
+	uint32_t len, num;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d", &num) != 1)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	read_lock(&ibm4767_lock);
+	if (ibm4767_list[num]) {
+		PRINTKW("User-invoked FORCE_CRIT_ERR sent to device %d\n", num);
+		write32(cpu_to_be32(MBX_FORCE_CRIT_ERR), H2M_MBX_H(ibm4767_list[num]));
+	}
+	else 
+		PRINTKW("%s - criterr to nonexistent device %d\n", __FUNCTION__, num);
+
+	read_unlock(&ibm4767_lock);
+	return count;
+}
+
+	
+static int
+procfs_criterr_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_criterr_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_criterr_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_criterr_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_criterr_write,
+};
+#else
+static const struct proc_ops procfs_criterr_fops = {
+	.proc_open = procfs_criterr_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_criterr_write
+};
+#endif
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+#if defined(DEBUG)
+static int 
+procfs_dbglvl_show(struct seq_file *sf, void *data)
+{
+	seq_printf(sf, "%d\n", dbg_level);
+	return 0;
+}
+
+
+static ssize_t
+procfs_dbglvl_write(struct file *file, const char *buffer, 
+		    size_t count, loff_t *pos)
+{
+	unsigned char buf[80];
+	int tmp;
+
+
+	if (count > sizeof(buf)-1)
+		count = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	buf[count] = 0;
+
+	if (sscanf(buf, "%d", &tmp) != 1)
+		return -EINVAL;
+
+	if ((tmp < 0) || (tmp > 5))
+		return -EINVAL;
+
+	dbg_level = tmp;
+	return count;
+}
+
+
+static int
+procfs_dbglvl_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_dbglvl_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_dbglvl_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_dbglvl_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_dbglvl_write
+};
+#else
+static const struct proc_ops procfs_dbglvl_fops = {
+	.proc_open = procfs_dbglvl_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_dbglvl_write
+};
+#endif
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// to dump general driver statistics:
+//    cat /proc/drivers/ibm4767/driver
+//
+static int 
+procfs_driver_show(struct seq_file *sf, void *data)
+{
+	uint32_t i, queued = 0, busy = 0, reqs = 0;
+
+	read_lock(&ibm4767_lock);
+
+	seq_printf(sf, "Driver status:\n");
+	seq_printf(sf, "   ibm4767 driver version %d.%d.%d build %d\n",
+			DRIVER_VERSION, DRIVER_RELEASE, DRIVER_VARIANT, DRIVER_BUILD);
+	seq_printf(sf, "   Device count:  %d\n", ibm4767_count);
+	for (i=0; i < MAX_DEV_COUNT; i++) {
+
+		ibm4767_ctx_t *ctx = ibm4767_list[i];
+
+		if (ctx) {
+			uint16_t domain;
+			uint8_t bus;
+			union {
+				struct { int dev:5; int fn:3; } devst;
+				uint8_t val;
+			} devfn;
+
+			// if we're running in a VM via PCIe pass-thru, then the 
+			// underlying PCI bus may not have a parent...
+			//
+			if (ctx->pci_dev && ctx->pci_dev->bus && ctx->pci_dev->bus->parent) {
+				devfn.val = ctx->pci_dev->devfn;
+				bus       = ctx->pci_dev->bus->number;
+				domain    = ctx->pci_dev->bus->parent->number;
+
+				seq_printf(sf, "      Device %d: serial number: %s  PCI address:  %04x:%02x:%02x.%x\n", ctx->dev_index, ctx->hwinfo.serial_num, domain, bus, devfn.devst.dev, devfn.devst.fn);
+			}
+			else {
+				seq_printf(sf, "      Device %d: serial number: %s  PCI address:  UNKNOWN (running inside a VM?)\n", ctx->dev_index, ctx->hwinfo.serial_num);
+			}
+		}
+	}
+
+	//seq_printf(sf, "   Open handles:  %d\n", (int)module_refcount(&__this_module));
+
+	seq_printf(sf, "   Request Activity\n");
+	seq_printf(sf, "      %8s\t%8s\t%8s\t%8s\t%8s\t%8s\t%8s\t%8s\n", "Device", "A:SSP", "A:MCPU", "A:FP", "EBUSY", "T:SSP", "T:MCPU", "T:FP");
+	
+	for (i=0; i < MAX_DEV_COUNT; i++) {
+		ibm4767_ctx_t *ctx = ibm4767_list[i];
+		uint32_t tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+
+		if (ctx) {
+			// the counters are protected by the 'counter_lock' spinlock.  but since this
+			// this routine only spits out informational data, I'm not going to grab it here
+			tmp1 = ctx->active_mb   + ctx->active_p1 + ctx->active_ssp_tas  + ctx->active_ssp_tapp;
+			tmp2 = ctx->active_norm + ctx->active_p2 + ctx->active_mcpu_tas + ctx->active_mcpu_tapp;
+			tmp3 = ctx->active_fp;
+			tmp4 = atomic_read(&ctx->ebusy_counter);
+			tmp5 = atomic_read(&ctx->ssp_counter);
+			tmp6 = atomic_read(&ctx->mcpu_counter);
+			tmp7 = atomic_read(&ctx->fp_counter);
+		
+			seq_printf(sf, "      %8d\t%8u\t%8u\t%8u\t%8u\t%8u\t%8u\t%8u\n", i, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+		
+			queued  += (tmp1 + tmp2 + tmp3);
+			busy    += tmp4; 
+			reqs    += tmp5 + tmp6 + tmp7; 
+		}
+	}
+
+	seq_printf(sf, "   Total queued requests:  %u\n", queued);
+	seq_printf(sf, "   Total EBUSY responses:  %u\n", busy);
+	seq_printf(sf, "   Total request attempts: %u\n", reqs);
+
+	read_unlock(&ibm4767_lock);
+	return 0;
+}
+
+
+static int
+procfs_driver_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_driver_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_driver_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_driver_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release
+};
+#else
+static const struct proc_ops procfs_driver_fops = {
+	.proc_open = procfs_driver_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release
+};
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// this routine can spit out a fair amount of data.  be sure to keep it below PAGE_SIZE because we're 
+// not implementing seg_file iterators for this.  
+//
+static int 
+procfs_devstat_show(struct seq_file *sf, void *data)
+{
+	ibm4767_ctx_t  *ctx = NULL;
+	const char     *txt = NULL;
+	long num = (long)sf->private;
+	int i;
+	uint64_t  hcsr;
+	uint8_t   val8  = 0;
+	uint64_t  val64 = 0;
+
+
+	if (num >= MAX_DEV_COUNT) {
+		PRINTKW("%s - device #%ld out of range!\n", __FUNCTION__, num);
+		return 0;
+	}
+
+	read_lock(&ibm4767_lock);
+	ctx = ibm4767_list[num];
+
+	if (ctx->status_flags & FLAG_MANUAL_DISABLED) {
+		seq_printf(sf, "SSP:  DISABLED MANUALLY\n");
+		seq_printf(sf, "MCPU: DISABLED MANUALLY\n");
+		read_unlock(&ibm4767_lock);
+		return 0;
+	}
+
+	hcsr  = cpu_to_be64(read64(HCSR(ctx)));
+
+	seq_printf(sf, "Serial number %s\n", ctx->hwinfo.serial_num);
+	seq_printf(sf, "Part number   %s\n", ctx->hwinfo.part_num);
+	seq_printf(sf, "FRU number    %s\n", ctx->hwinfo.fru_num);
+	seq_printf(sf, "EC level      %s\n", ctx->hwinfo.ec_level);
+	seq_printf(sf, "Manufacturing loc %s\n", ctx->hwinfo.mf_loc);
+
+	seq_printf(sf, "FPGA Rev ID %04x\n", (int)((hcsr & HCSR_FPGA_REV)  >> 16));
+	seq_printf(sf, "ASIC Rev ID %02x\n", (int)((hcsr & HCSR_ASIC_REV)  >> 8));
+	seq_printf(sf, "Card Rev ID %02x\n", (int)(hcsr & HCSR_CARD_REV));
+	
+	seq_printf(sf, "POST 0 rev %08x\n", ctx->hwinfo.post0_ver);
+	seq_printf(sf, "POST 1 rev %08x\n", ctx->hwinfo.post1_ver);
+	seq_printf(sf, "POST 2 rev %08x\n", ctx->hwinfo.post2_ver);
+	seq_printf(sf, "MB 0   rev %08x\n", ctx->hwinfo.mb0_ver);
+	seq_printf(sf, "MB 1   rev %08x\n", ctx->hwinfo.mb1_ver);
+
+	seq_printf(sf, "Tamper state:  ");
+	txt = tamper_state_to_str(ctx->status_tamper);
+	if (txt)
+		seq_printf(sf, "%s\n", txt);
+	else
+		seq_printf(sf, "UNKNOWN (%d)\n", ctx->status_ssp);
+		
+	seq_printf(sf, "SSP state:  ");
+	txt = ssp_state_to_str(ctx->status_ssp);
+	if (txt)
+		seq_printf(sf, "%s\n", txt);
+	else
+		seq_printf(sf, "UNKNOWN (%d)\n", ctx->status_ssp);
+
+	seq_printf(sf, "MCPU state: ");
+	txt = mcpu_state_to_str(ctx->status_mcpu);
+	if (txt)
+		seq_printf(sf, "%s\n", txt);
+	else
+		seq_printf(sf, "UNKNOWN (%d)\n", ctx->status_ssp);
+
+	seq_printf(sf, "SSP  Reset: %s\n", (ctx->status_flags & FLAG_SSP_RESET_PENDING) ? "Pending" : "None pending");
+	seq_printf(sf, "SSP  WUR:   %s\n", (ctx->status_flags & FLAG_SSP_WUR_PENDING) ? "Pending" : "None pending");
+	seq_printf(sf, "Main Reset: %s\n",
+		((ctx->status_flags & FLAG_MAIN_RESET_PENDING) ? "Pending" :
+	 	((ctx->status_flags & FLAG_MAIN_RESET_ACTIVE) ? "Active" : "Idle")));
+
+	if (allow_indirect) {
+		uint8_t tmprA, tmprB, tmprC;
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_TMPR_LATCH_A, &tmprA)) {
+			if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_TMPR_LATCH_B, &tmprB))
+				if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_TMPR_LATCH_C, &tmprC))
+					seq_printf(sf, "Tamper Latches:  0x%02X  0x%02X  0x%02X\n", tmprA, tmprB, tmprC);
+		}
+		else {
+			seq_printf(sf, "Tamper Latches:  Error reading DS3645...\n");
+		}
+
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_VCCI, &val8)) {
+			// nihad requested that we convert to mV but FTE/FTC guys parse this field and
+			// can't afford the time needed to retest everything right now...
+			//
+			//seq_printf(sf, "ADC Vcci:  %04lld mV (0x%02X)\n", ADC2MV(val8), val8);
+			seq_printf(sf, "ADC Vcci:  0x%02X\n", val8);
+		}
+		else {
+			seq_printf(sf, "ADC Vcci:  N/A\n");
+		}
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_VBAT, &val8)) {
+			//seq_printf(sf, "ADC VBAT:  %04lld mV (0x%02X)\n", ADC2MV(val8), val8);
+			seq_printf(sf, "ADC VBAT:  0x%02X\n", val8);
+		}
+		else {
+			seq_printf(sf, "ADC VBAT:  N/A\n");
+		}
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_VREF, &val8)) {
+			//seq_printf(sf, "ADC VREF:  %04lld mV (0x%02X)\n", ADC2MV(val8), val8);
+			seq_printf(sf, "ADC VREF:  0x%02X\n", val8);
+		}
+		else {
+			seq_printf(sf, "ADC VREF:  N/A\n");
+		}
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_TEMP, &val8)) {
+			//seq_printf(sf, "ADC Temp:  %d C (0x%02X)\n", ADC2TEMP(val8), val8);
+			seq_printf(sf, "ADC Temp:  0x%02X\n", val8);
+		}
+		else {
+			seq_printf(sf, "ADC Temp:  N/A\n");
+		}
+	}
+
+	// in addition, mike debole wanted a full ds3645 dump for manufacturing
+	if (allow_indirect) {
+		uint32_t  val, tmp;
+		uint8_t   off;
+		uint8_t   buf[INDIRECT_DS3645_LAST_VISIBLE_REG+1];
+
+		memset(buf, 0, sizeof(buf));
+
+		seq_printf(sf, "DS3645 contents:  ");
+	
+		for (off=0; off < INDIRECT_DS3645_LAST_VISIBLE_REG; off += 4) {
+			if (!ibm4767_read32_indirect(ctx, off, &val)) {
+				*(uint32_t *)(&buf[off]) = val;
+			}
+			else {
+				seq_printf(sf, "\n*** Failed to read registers at offset 0x%02x ***\n", off);
+			}
+		}
+
+		for (off=0; off < INDIRECT_DS3645_LAST_VISIBLE_REG; off += 4) {
+			tmp = *(uint32_t *)(&buf[off]);
+			val = cpu_to_be32(tmp);
+			if ((off % 16) == 0) 
+				seq_printf(sf, "\n   0x%03x: %08x ", off, val);
+			else 
+				seq_printf(sf, "%08x ", val);
+		}
+		seq_printf(sf, "\n");
+	}
+		
+
+	val64 = cpu_to_be64(read64(HCSR(ctx)));
+	seq_printf(sf, "Battery Status: 0x%02x\n", (uint8_t)((val64 & HCSR_BATTERY)   >> 54));
+	//seq_printf(sf, "PM Temp:        %d C\n", ADC2TEMP((uint8_t)((val64 & HCSR_CURR_TEMP) >> 40)));
+	seq_printf(sf, "PM Temp:        0x%02x\n", (uint8_t)((val64 & HCSR_CURR_TEMP) >> 40));
+	seq_printf(sf, "Throttle level: %d\n", ctx->throttle_lvl);
+
+	// do a quick register dump...
+	//
+#if defined(ENABLE_REALTIME_REGS)
+	seq_printf(sf, "HISR        0x%08x (realtime)\n", cpu_to_be32(read32(HISR(ctx))));
+	seq_printf(sf, "HTB_WA      0x%016llx (realtime)\n", cpu_to_be64(read64(HTB_WA(ctx))));
+	seq_printf(sf, "S2H_MBX     0x%08x : %08x (realtime)\n", cpu_to_be32(read32(S2H_MBX_H(ctx))), cpu_to_be32(read32(S2H_MBX_L(ctx))));
+	seq_printf(sf, "M2H_MBX     0x%08x : %08x (realtime)\n", cpu_to_be32(read32(M2H_MBX_H(ctx))), cpu_to_be32(read32(M2H_MBX_L(ctx))));
+#else
+	seq_printf(sf, "HISR        0x%08x (last intr)\n", ctx->last_hisr);
+	seq_printf(sf, "HTB_WA      0x%016llx (last intr)\n", ctx->htb_last_wa);
+	seq_printf(sf, "S2H_MBX     0x%08x : %08x (last intr)\n", ctx->last_s2h_h, ctx->last_s2h_l);
+	seq_printf(sf, "M2H_MBX     0x%08x : %08x (last intr)\n", ctx->last_m2h_h, ctx->last_m2h_l);
+#endif
+	seq_printf(sf, "HTB_TC      0x%016llx\n", cpu_to_be64(read64(HTB_TC(ctx))));
+	seq_printf(sf, "HCR         0x%08x\n",    cpu_to_be32(read32(HCR(ctx))));
+	seq_printf(sf, "HCSR        0x%016llx\n", cpu_to_be64(read64(HCSR(ctx))));
+	seq_printf(sf, "HRCSR       0x%08x\n",    cpu_to_be32(read32(HRCSR(ctx))));
+	seq_printf(sf, "HBMCR       0x%08x\n",    cpu_to_be32(read32(HBMCR(ctx))));
+	seq_printf(sf, "HBMSR       0x%016llx\n", cpu_to_be64(read64(HBMSR(ctx))));
+	seq_printf(sf, "HIER        0x%08x\n",    cpu_to_be32(read32(HIER(ctx))));
+	seq_printf(sf, "HMVMC       0x%08x\n",    cpu_to_be32(read32(HMVMC(ctx))));
+	seq_printf(sf, "HDMAERR     0x%08x\n",    cpu_to_be32(read32(H_CPU_DMAERR(ctx))));
+	
+	seq_printf(sf, "SSP  CHKPT  %04x\n", cpu_to_be16(read16(S2H_MBX_H(ctx))));
+	seq_printf(sf, "MCPU CHKPT  %04x\n", cpu_to_be16(read16(M2H_MBX_H(ctx))));
+
+	seq_printf(sf, "PCIe Link Reg:   0x%016llx\n", (cpu_to_be64(read64(PCIE_TRAINING_CTRL(ctx)))));
+	seq_printf(sf, "PCIe Link Speed: 0x%02llx\n", ((cpu_to_be64(read64(PCIE_TRAINING_CTRL(ctx)))) >> 24) & 0xF);
+
+	spin_lock_bh(&ctx->req_pool_lock);
+	for (i=0, num=0; i < REQ_POOL_COUNT; i++) {
+		if (ctx->req_array[i].active)
+			num++;
+	}
+	spin_unlock_bh(&ctx->req_pool_lock);
+
+	if (num == 0) {
+		seq_printf(sf, "Request Queue:  empty\n");
+	}
+	else {
+		seq_printf(sf, "Request Queue:  %ld entries\n", num);
+	}
+
+	seq_printf(sf, "Active opens:  %u\n", (uint32_t)ctx->active_opens);
+	seq_printf(sf, "MCPU requests: %u\n", (uint32_t)atomic_read(&ctx->mcpu_counter));
+	seq_printf(sf, "   MRB0:       %u\n", (uint32_t)atomic_read(&ctx->mrb0_counter));
+	seq_printf(sf, "   MRB1:       %u\n", (uint32_t)atomic_read(&ctx->mrb1_counter));
+	seq_printf(sf, "SSP  requests: %u\n", (uint32_t)atomic_read(&ctx->ssp_counter));
+	seq_printf(sf, "FP   requests: %u\n", (uint32_t)atomic_read(&ctx->fp_counter));
+	seq_printf(sf, "BUSY results:  %u\n", (uint32_t)atomic_read(&ctx->ebusy_counter));
+	
+	spin_lock_bh(&ctx->counter_lock);
+	seq_printf(sf, "Sev0 monitor:  %d (limit %d) %s\n", sev0_count(ctx), sev0_limit, (sev0_ignore ? "(ignoring)" : ""));
+	sev0_dump(ctx, sf);
+	spin_unlock_bh(&ctx->counter_lock);
+
+	read_unlock(&ibm4767_lock);
+	return 0;
+}
+
+
+// to disable a particular device:
+//    echo D > /proc/driver/ibm4767/<device #>
+//
+// to re-enable the device:
+//    echo E > /proc/driver/ibm4767/<device #>
+//
+static ssize_t
+procfs_devstat_write(struct file *file, const char *buffer, 
+		     size_t count, loff_t *pos)
+{
+	ibm4767_ctx_t *ctx = NULL;
+	unsigned char buf[80];
+	unsigned int  len, rc;
+	char tmp;
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0))
+	void *data = PDE(file->f_path.dentry->d_inode)->data;
+#else
+	void *data = PDE_DATA(file_inode(file));
+#endif
+	long num = (long)data;
+	
+
+	if (num >= MAX_DEV_COUNT) {
+		PRINTKW("%s - device #%ld out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	read_lock(&ibm4767_lock);
+	ctx = ibm4767_list[num];
+
+	rc = len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len)) {
+		rc = -EFAULT;
+		goto done;
+	}
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%c", &tmp) != 1) {
+		rc = -EINVAL;
+		goto done;
+	}
+
+	spin_lock_bh(&ctx->counter_lock);
+	if ((tmp == 'E') || (tmp == 'e')) 
+		ctx->status_flags &= ~FLAG_MANUAL_DISABLED;
+	else if ((tmp == 'D') || (tmp == 'd'))
+		ctx->status_flags |= FLAG_MANUAL_DISABLED;
+	else
+		rc = -EINVAL;
+	spin_unlock_bh(&ctx->counter_lock);
+
+done:
+	read_unlock(&ibm4767_lock);
+	return rc;
+}
+
+
+static int
+procfs_devstat_open(struct inode *inode, struct file *file)
+{
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0))
+	return single_open(file, procfs_devstat_show, PDE(inode)->data);
+#else
+	return single_open(file, procfs_devstat_show, PDE_DATA(inode));
+#endif
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_devstat_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_devstat_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_devstat_write
+};
+#else
+static const struct proc_ops procfs_devstat_fops = {
+	.proc_open = procfs_devstat_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_devstat_write
+};
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// nihad requested a routine to dump the entire host-readable range of DS3645...
+// since output needs to go to stdout, specifying which device to query is a
+// little tricky.  Initially the driver will query device #0.  This can be changed
+// by writing the device # to /proc/driver/ibm4767/ds3645:
+//
+//    echo <device #>  >  /proc/driver/ibm4767/ds3645
+//
+// From this point on, the driver will query the specified device when ds3645 is
+// examined:
+//
+//    cat /proc/driver/ibm4767/ds3645
+//
+static int 
+procfs_ds3645_show(struct seq_file *sf, void *data)
+{
+	ibm4767_ctx_t *ctx = NULL;
+	uint32_t  val, tmp;
+	uint8_t   off;
+	uint8_t   str[64], buf[INDIRECT_DS3645_LAST_VISIBLE_REG+1];
+
+	ctx = ibm4767_list[procfs_ds3645_target];
+	if (!ctx) {
+		PRINTKW("%s - no target context\n", __FUNCTION__);
+		return -EFAULT;
+	}
+
+	memset(buf, 0, sizeof(buf));
+	
+	for (off=0; off < INDIRECT_DS3645_LAST_VISIBLE_REG; off += 4) {
+		if (!ibm4767_read32_indirect(ctx, off, &val)) {
+			*(uint32_t *)(&buf[off]) = val;
+		}
+		else {
+			seq_printf(sf, "\n*** Failed to read registers at offset 0x%02x ***\n", off);
+			PRINTKW("%s: failed to read registers at offset 0x%02x\n", __FUNCTION__, off);
+		}
+	}
+
+	// Nihad wants to dump to stdout and syslog...
+
+	sprintf(str, "DS3645 for Device #%d", procfs_ds3645_target);
+	seq_printf(sf, "%s", str);
+
+	hex_dump(ctx, 0, str, buf, sizeof(buf));
+
+	for (off=0; off < INDIRECT_DS3645_LAST_VISIBLE_REG; off += 4) {
+		tmp = *(uint32_t *)(&buf[off]);
+		val = cpu_to_be32(tmp);
+		if ((off % 16) == 0) 
+			seq_printf(sf, "\n0x%03x: %08x ", off, val);
+		else 
+			seq_printf(sf, "%08x ", val);
+	}
+	seq_printf(sf, "\n");
+
+
+	return 0;
+}
+
+
+static ssize_t
+procfs_ds3645_write(struct file *file, const char *buffer, 
+		    size_t count, loff_t *pos)
+{
+	unsigned char buf[256];
+	unsigned int  len, num;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d", &num) != 1)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EINVAL;
+	}
+
+	read_lock(&ibm4767_lock);
+
+	if (ibm4767_list[num]) {
+		PRINTKW("%s - setting target DS3645 to device %d\n", __FUNCTION__, num);
+		procfs_ds3645_target = num;
+	}
+	else {
+		read_unlock(&ibm4767_lock);
+		PRINTKW("%s - device #%d doesn't exist\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+	read_unlock(&ibm4767_lock);
+
+	return count;
+}
+
+
+static int
+procfs_ds3645_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_ds3645_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_ds3645_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_ds3645_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_ds3645_write,
+};
+#else
+static const struct proc_ops procfs_ds3645_fops = {
+	.proc_open = procfs_ds3645_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_ds3645_write
+};
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// to force an emergency dump for a specic card:
+//    echo card# > /proc/drivres/ibm4767/edump
+//
+#if defined(DEBUG)
+static int 
+procfs_edump_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t
+procfs_edump_write(struct file *file, const char *buffer, 
+		   size_t count, loff_t *pos)
+{
+	unsigned char buf[80];
+	unsigned int  len, num;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d", &num) != 1)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	read_lock(&ibm4767_lock);
+	if (ibm4767_list[num]) {
+		PRINTKW("User-invoked FORCE_EDUMP sent to device %d\n", num);
+		write32(cpu_to_be32(MBX_FORCE_EDUMP), H2M_MBX_H(ibm4767_list[num]));
+	}
+	else 
+		PRINTKW("%s - edump to nonexistent device %d\n", __FUNCTION__, num);
+		
+	read_unlock(&ibm4767_lock);
+
+	return count;
+}
+
+
+static int
+procfs_edump_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_edump_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_edump_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_edump_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_edump_write,
+};
+#else
+static const struct proc_ops procfs_edump_fops = {
+	.proc_open = procfs_edump_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_edump_write
+};
+#endif
+#endif
+
+
+
+//--------------------------------------------------------------------------------------------------
+// normally the driver doesn't enable DMA channels until it receives a HELLO notification
+// from an embedded entity (POST, MB, etc).  until we get a functional POST, this routine
+// will let us explicitly enable DMA so that FP testing can be done.
+//
+// note: you must enable DMA via RISCWatch before using this!
+//
+#if defined(DEBUG)
+static int 
+procfs_enablefp_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t 
+procfs_enablefp_write(struct file *file, const char *buffer, 
+		      size_t count, loff_t *pos)
+{
+	ibm4767_ctx_t *ctx = NULL;
+	uint64_t hbmsr;
+	unsigned char buf[80];
+	unsigned int  len, num;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d", &num) != 1)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+	ctx = ibm4767_list[num];
+	if (!ctx) {
+		PRINTKW("%s - device #%d doesn't exist!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	if (hbmsr & (HBMSR_H2MM_MASTER_EN|HBMSR_H2SK_MASTER_EN|HBMSR_H2FA_MASTER_EN|HBMSR_H2FB_MASTER_EN)) {
+		dma_enable(ctx, HBMCR_BMEN_ALL);
+		if (ctx->htb_active == 0xFF)
+			htb_swap(ctx);
+		ctx->status_fp = FP_READY;
+		PRINTKW("%s - enable all channels for device %d\n", __FUNCTION__, num);
+	}
+	else {
+		PRINTKW("%s - device %d FP DMA channels are disabled on card side (HBMSR: %016llx)\n", __FUNCTION__, num, hbmsr);
+	}
+
+	return count;
+}
+
+
+static int
+procfs_enablefp_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_enablefp_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_enablefp_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_enablefp_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_enablefp_write,
+};
+#else
+static const struct proc_ops procfs_enablefp_fops = {
+	.proc_open = procfs_enablefp_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_enablefp_write
+};
+#endif
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// force the driver to attempt to read 32 bytes from the FIFO.  this will help mark debug his
+// seg2 emergency dump stuff...
+//
+#if defined(DEBUG)
+static int 
+procfs_fifos_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t 
+procfs_fifos_write(struct file *file, const char *buffer, 
+		   size_t count, loff_t *pos)
+{
+	ibm4767_ctx_t *ctx = NULL;
+	unsigned char buf[128];
+	unsigned int  len, num;
+	int i, total=0, rc;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d", &num) != 1)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+	ctx = ibm4767_list[num];
+
+	for (i=0, total=0; i < 4; i++) {
+		rc = target_read8_mcpu_fifo(ctx, buf+total, FALSE);
+		if (rc < 0) {
+			PDEBUG(2, "FIFO timeout (i=%d)\n", i);
+			goto done;
+		}
+
+		total += 8;
+	}
+
+done:
+	hex_dump(ctx, 1, "FIFO dump", buf, total);
+	return count;
+}
+
+
+static int
+procfs_fifos_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_fifos_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_fifos_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_fifos_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_fifos_write,
+};
+#else
+static const struct proc_ops procfs_fifos_fops = {
+	.proc_open = procfs_fifos_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_fifos_write
+};
+#endif
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// to force a hardware error dump for a specific card:
+//    echo "<card#> <32bit hex value>" > /proc/drivers/ibm4767/hwerr
+//
+#if defined(DEBUG)
+static int 
+procfs_hwerr_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t
+procfs_hwerr_write(struct file *file, const char *buffer, 
+		   size_t count, loff_t *pos)
+{
+	unsigned char buf[80];
+	uint32_t len, num, val;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d %x", &num, &val) != 2)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	read_lock(&ibm4767_lock); 
+	if (ibm4767_list[num]) {
+		// we must write the low MBX first since byte 2 results in
+		// the MCPU interrupt...
+		//
+		PRINTKW("User-invoked FORCE_HW_ERR sent to device %d\n", num);
+		write32(cpu_to_be32(val),              H2M_MBX_L(ibm4767_list[num])); 
+		write32(cpu_to_be32(MBX_FORCE_HW_ERR), H2M_MBX_H(ibm4767_list[num]));
+	}
+	else 
+		PRINTKW("%s - hwerr to nonexistent device %d\n", __FUNCTION__, num);
+		
+	read_unlock(&ibm4767_lock);
+
+	return count;
+}
+
+	
+static int
+procfs_hwerr_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_hwerr_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_hwerr_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_hwerr_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_hwerr_write,
+};
+#else
+static const struct proc_ops procfs_hwerr_fops = {
+	.proc_open = procfs_hwerr_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_hwerr_write
+};
+#endif
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// tell the driver to begin asym log monitoring and analysis
+//    echo <card#> [0|1] > /proc/drivers/ibm4767/logger
+//
+#if defined(DEBUG)
+static int 
+procfs_logger_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t
+procfs_logger_write(struct file *file, const char *buffer, 
+		   size_t count, loff_t *pos)
+{
+	ibm4767_ctx_t *ctx;
+	unsigned char buf[80];
+	uint32_t len, num, val;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d %d", &num, &val) != 2)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	read_lock(&ibm4767_lock); 
+
+	ctx = ibm4767_list[num];
+	if (ctx) {
+		ctx->analyze_logs = val;
+		PRINTKW("Log analysis %s\n", val ? "enabled" : "disabled");
+	}
+	else {
+		PRINTKW("%s - enable logger to nonexistent device %d\n", __FUNCTION__, num);
+	}
+	
+	read_unlock(&ibm4767_lock);
+
+	return count;
+}
+
+	
+static int
+procfs_logger_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_logger_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_logger_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_logger_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_logger_write,
+};
+#else
+static const struct proc_ops procfs_logger_fops = {
+	.proc_open = procfs_logger_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_logger_write
+};
+#endif
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+// to force an immediate reset for a specific card (aborting any requests in progress):
+//    echo card# > /proc/drivers/ibm4767/reset
+//
+static int 
+procfs_reset_show(struct seq_file *sf, void *data)
+{
+	return -EFAULT;
+}
+
+
+static ssize_t
+procfs_reset_write(struct file *file, const char *buffer, 
+		   size_t count, loff_t *pos)
+{
+	unsigned char buf[80];
+	unsigned int  len, num;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d", &num) != 1)
+		return -EINVAL;
+
+	if ((num < 0) || (num >= MAX_DEV_COUNT)) {
+		PRINTKW("%s - device #%d out of range!\n", __FUNCTION__, num);
+		return -EFAULT;
+	}
+
+	read_lock(&ibm4767_lock);
+
+	if (ibm4767_list[num]) {
+		// this is one of the few places where we call main_reset() 
+		// directly instead of using the deferred reset timer.  we
+		// ignore the state of the tamper reset registers and blindly
+		// performs an HCR reset.  we assume that the user knows
+		// what she is doing...
+		//
+		PRINTKW("Device %d: forced reset via procfs\n", num);
+		ibm4767_main_reset(ibm4767_list[num], TRUE);
+	}
+	else 
+		PRINTKW("%s - reset sent to nonexistent device %d\n", __FUNCTION__, num);
+
+	read_unlock(&ibm4767_lock);
+	return count;
+}
+
+
+static int
+procfs_reset_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_reset_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_reset_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_reset_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_reset_write,
+};
+#else
+static const struct proc_ops procfs_reset_fops = {
+	.proc_open = procfs_reset_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_reset_write
+};
+#endif
+
+
+//--------------------------------------------------------------------------------------------------
+static int 
+procfs_timeout_show(struct seq_file *sf, void *data)
+{
+	seq_printf(sf, "Timeout values: CDU:%d MB:%d MCPU:%d,%d SSP:%d FPGA:%d,%d PKA:%d SKCH:%d\n", 
+			timeout_cdu, timeout_mb, timeout_mcpu[0], timeout_mcpu[1], timeout_ssp, 
+			timeout_fpgaA, timeout_fpgaB, timeout_pka, timeout_skch);
+	return 0;
+}
+
+
+// only one timeout window can be specified at a time.  the format is
+// <window #> <timeout value>
+//
+static ssize_t
+procfs_timeout_write(struct file *file, const char *buffer, 
+		     size_t count, loff_t *pos)
+{
+	unsigned char buf[80];
+	unsigned int  len, rc;
+	int window, tmp;
+
+	len = count;
+	if (len > sizeof(buf)-1)
+		len = sizeof(buf)-1;
+
+	rc = copy_from_user(buf, buffer, len);
+	if (rc)
+		return -EFAULT;
+	buf[len] = 0;
+
+	if (sscanf(buf, "%d %d", &window, &tmp) != 2)
+		return -EINVAL;
+
+	if (window < 0 || window >= NUM_PRI_WINDOWS)
+		return -EINVAL;
+
+	if (tmp < 0)
+		return -EINVAL;
+
+	timeout_mcpu[window] = tmp;
+	return count;
+}
+
+
+static int
+procfs_timeout_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, procfs_timeout_show, NULL);
+}
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,6,0)
+static const struct file_operations procfs_timeout_fops = {
+	.owner   = THIS_MODULE,
+	.open    = procfs_timeout_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = procfs_timeout_write,
+};
+#else
+static const struct proc_ops procfs_timeout_fops = {
+	.proc_open = procfs_timeout_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+	.proc_write = procfs_timeout_write
+};
+#endif
+
+
+//
+// setup/cleanup routines
+//
+
+void 
+ibm4767_procfs_add_device(long num)
+{
+	struct proc_dir_entry *entry;
+	char   buf[80];
+
+	if (proc_initialized == FALSE) {
+		ibm4767_procfs_setup();
+		if (proc_initialized == FALSE)
+			return;
+	}
+
+	sprintf(buf, "%ld", num);
+	entry = proc_create_data(buf, 0644, ibm4767_dir, &procfs_devstat_fops, (void *)num);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/%s\n", buf);
+		goto fail;
+	}
+
+	dev_entries[num] = 1;
+	return;
+
+fail:
+	remove_proc_entry(buf, ibm4767_dir);
+}
+
+
+void 
+ibm4767_procfs_remove_device(int num)
+{
+	char buf[80];
+
+	if (proc_initialized == FALSE)
+		return;
+
+	if (dev_entries[num]) {
+		sprintf(buf, "%d", num);
+		remove_proc_entry(buf, ibm4767_dir);
+		dev_entries[num] = 0;
+	}
+}
+
+
+static void 
+procfs_remove_entries(void)
+{
+	if (procfs_audit & Y_PROCFS_AGENTIDS)
+		remove_proc_entry("agentids", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_BARREG)
+		remove_proc_entry("barreg", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_CRITERR)
+		remove_proc_entry("criterr", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_DBGLVL)	
+		remove_proc_entry("dbg_level", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_DRIVER)
+		remove_proc_entry("driver",  ibm4767_dir);
+	
+	if (procfs_audit & Y_PROCFS_DS3645)
+		remove_proc_entry("ds3645", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_EDUMP)
+		remove_proc_entry("edump", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_ENABLEFP)
+		remove_proc_entry("enablefp", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_FIFOS)
+		remove_proc_entry("fifos", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_HWERR)
+		remove_proc_entry("hwerr", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_LOGGER)
+		remove_proc_entry("logger", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_RESET)
+		remove_proc_entry("reset", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_TIMEOUT)
+		remove_proc_entry("timeout", ibm4767_dir);
+
+	if (procfs_audit & Y_PROCFS_IBM4767)
+		remove_proc_entry("driver/ibm4767", NULL);
+}
+
+
+void 
+ibm4767_procfs_setup(void)
+{
+	struct proc_dir_entry *entry;
+
+	procfs_audit = 0;
+
+	memset(dev_entries, 0, sizeof(dev_entries));
+
+	ibm4767_dir = proc_mkdir("driver/ibm4767", NULL);
+	if (!ibm4767_dir) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767\n");
+		return;
+	}
+	procfs_audit |= Y_PROCFS_IBM4767;
+
+#if defined(DEBUG)	
+	entry = proc_create_data("agentids", 0666, ibm4767_dir, &procfs_agentids_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/agentids\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_AGENTIDS;
+#endif
+#if defined(DEBUG)	
+	entry = proc_create_data("barreg", 0666, ibm4767_dir, &procfs_barreg_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/barreg\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_BARREG;
+#endif
+#if defined(DEBUG)
+	entry = proc_create_data("criterr", 0666, ibm4767_dir, &procfs_criterr_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/criterr\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_CRITERR;
+#endif
+#if defined(DEBUG)
+	entry = proc_create_data("dbg_level", 0666, ibm4767_dir, &procfs_dbglvl_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/dbg_level\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_DBGLVL;
+#endif
+	
+	entry = proc_create_data("driver", 0644, ibm4767_dir, &procfs_driver_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/driver\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_DRIVER;
+
+#if defined(DEBUG)	
+	entry = proc_create_data("ds3645", 0666, ibm4767_dir, &procfs_ds3645_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/ds3645\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_DS3645;
+#endif
+#if defined(DEBUG)	
+	entry = proc_create_data("edump", 0666, ibm4767_dir, &procfs_edump_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/edump\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_EDUMP;
+#endif
+#if defined(DEBUG)
+	entry = proc_create_data("enablefp", 0666, ibm4767_dir, &procfs_enablefp_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/enablefp\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_ENABLEFP;
+#endif
+#if defined(DEBUG)	
+	entry = proc_create_data("fifos", 0666, ibm4767_dir, &procfs_fifos_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/fifos\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_FIFOS;
+#endif
+#if defined(DEBUG)
+	entry = proc_create_data("hwerr", 0666, ibm4767_dir, &procfs_hwerr_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/hwerr\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_HWERR;
+#endif
+#if defined(DEBUG)
+	entry = proc_create_data("logger", 0666, ibm4767_dir, &procfs_logger_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/logger\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_LOGGER;
+#endif
+	
+	entry = proc_create_data("reset", 0666, ibm4767_dir, &procfs_reset_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/reset\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_RESET;
+
+	entry = proc_create_data("timeout", 0644, ibm4767_dir, &procfs_timeout_fops, NULL);
+	if (!entry) {
+		PRINTKW("Couldn't create /proc/drivers/ibm4767/timeout\n");
+		goto error;
+	}
+	procfs_audit |= Y_PROCFS_TIMEOUT;
+
+
+	proc_initialized = TRUE;
+	return;
+
+error:
+	procfs_remove_entries();
+	proc_initialized = FALSE;
+}
+
+
+void 
+ibm4767_procfs_cleanup()
+{
+	int  i;
+
+	for (i=0; i < MAX_DEV_COUNT; i++) 
+		ibm4767_procfs_remove_device(i);
+
+	procfs_remove_entries();
+}
+
diff --git drivers/misc/ibm4767/request.c drivers/misc/ibm4767/request.c
new file mode 100755
index 000000000000..a52673a32910
--- /dev/null
+++ drivers/misc/ibm4767/request.c
@@ -0,0 +1,1901 @@
+/*************************************************************************
+ *  Filename:request.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Visegrady, Tamas  IBM Poughkeepsie  <tamas@us.ibm.com>
+ *           Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+
+	
+int
+request_pool_initialize(ibm4767_ctx_t *ctx)
+{ 
+	int i, rc;
+
+	PDEBUG(1, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	spin_lock_init(&ctx->req_pool_lock);
+	spin_lock_init(&ctx->observe_pool_lock);
+
+	// unlike the DT pool, we do not keep requests in a 'free' list.  keeping them in an array
+	// makes abort-processing much easier...
+	//
+	for (i=0; i < REQ_POOL_COUNT; i++) {
+		ctx->req_array[i].active = REQUEST_INACTIVE;
+		ctx->req_array[i].request_id = i;
+		ctx->req_array[i].header = kmalloc(PAGE_SIZE, GFP_KERNEL);
+		if (!ctx->req_array[i].header) {
+			PRINTKW("Device %d:  kmalloc failed for request pool header %d\n", ctx->dev_index, i);
+			rc = HOST_DD_NoMemory;
+			goto error;
+		}
+
+		init_waitqueue_head(&ctx->req_array[i].waitQ);
+	}
+
+	for (i=0; i < OBS_POOL_COUNT; i++) {
+		init_waitqueue_head(&ctx->observe_pool[i].waitQ);
+		ctx->observe_pool[i].active = OBSERVER_INACTIVE;
+	}
+
+	INIT_LIST_HEAD(&ctx->req_pool);
+	for (i=0; i < REQ_POOL_COUNT; i++) {
+		INIT_LIST_HEAD(&ctx->req_array[i].list_head);
+		list_add(&ctx->req_array[i].list_head, &ctx->req_pool);
+	}
+
+	INIT_LIST_HEAD(&ctx->req_wait_list);
+
+	return HOST_DD_Good; 
+
+error:
+	for (i=0; i < REQ_POOL_COUNT; i++) {
+		if (ctx->req_array[i].header)
+			kfree(ctx->req_array[i].header);
+	}
+
+	return rc;
+}
+
+
+void
+request_pool_unload(ibm4767_ctx_t *ctx)
+{ 
+	int i;
+
+	spin_lock_bh(&ctx->req_pool_lock);
+	for (i=0; i < REQ_POOL_COUNT; i++) {
+		if (ctx->req_array[i].header)
+			kfree(ctx->req_array[i].header);
+	}
+	spin_unlock_bh(&ctx->req_pool_lock);
+}
+
+
+// this routine is deprecated.  don't use it.
+//
+request_t *
+request_allocate_old(ibm4767_ctx_t *ctx, char mode)
+{
+	struct list_head *pLH;
+	request_t *pReq= NULL;
+
+	spin_lock_bh(&ctx->req_pool_lock);
+
+	if (list_empty(&ctx->req_pool)) {
+		spin_unlock_bh(&ctx->req_pool_lock);
+		//PRINTKW("Device %d: %s - no more requests!\n", ctx->dev_index, __FUNCTION__);
+		goto done;
+	}
+
+	pLH = list_remove_head(&ctx->req_pool);
+	spin_unlock_bh(&ctx->req_pool_lock);
+
+	pReq = list_entry(pLH, request_t, list_head);
+
+	INIT_LIST_HEAD(&pReq->DT_req);
+	INIT_LIST_HEAD(&pReq->DT_hra1);
+	INIT_LIST_HEAD(&pReq->DT_hra2);
+			
+	INIT_LIST_HEAD(&pReq->hdr_slist.frags);
+	INIT_LIST_HEAD(&pReq->req_data_slist.frags);
+	INIT_LIST_HEAD(&pReq->req_slist.frags);
+	INIT_LIST_HEAD(&pReq->hra1_slist.frags);
+	INIT_LIST_HEAD(&pReq->hra2_slist.frags);
+			
+	memset(pReq->header, 0, PAGE_SIZE);
+
+	atomic_set(&pReq->waitEvent, 0);
+			
+	atomic_set(&pReq->num_hras, 0);
+	pReq->retcode  = 0;
+	pReq->reqd_state = 0;
+	pReq->status = 0;
+	pReq->active = mode;
+	// fall through...
+
+done:
+	return pReq;
+}
+
+
+
+int
+request_allocate(ibm4767_ctx_t *ctx, char mode, request_wait_t *rw)
+{
+	request_t *pReq= NULL;
+
+	spin_lock_bh(&ctx->req_pool_lock);
+
+	if (list_empty(&ctx->req_pool)) {
+		// put ourselves to sleep until a request becomes available.  threads on this queue take priority 
+		// over new incoming requests so we don't have starvation issues...
+		//
+		atomic_set(&rw->waitEvent, 0);
+		init_waitqueue_head(&rw->waitQ);
+		INIT_LIST_HEAD(&rw->list_head);
+		rw->pReq = NULL;
+		rw->type = mode;
+		rw->rc   = HOST_DD_Good;
+
+		list_add_tail(&rw->list_head, &ctx->req_wait_list);
+		
+		spin_unlock_bh(&ctx->req_pool_lock);
+		wait_event(rw->waitQ, atomic_read(&rw->waitEvent));
+
+		if (rw->rc != HOST_DD_Good)
+			return rw->rc;
+
+		pReq = rw->pReq;
+		// fall thru
+	}
+	else {
+		struct list_head *pLH = list_remove_head(&ctx->req_pool);
+		spin_unlock_bh(&ctx->req_pool_lock);
+
+		rw->pReq = pReq = list_entry(pLH, request_t, list_head);
+		rw->rc = HOST_DD_Good;
+		// fall thru
+	}
+
+	INIT_LIST_HEAD(&pReq->DT_req);
+	INIT_LIST_HEAD(&pReq->DT_hra1);
+	INIT_LIST_HEAD(&pReq->DT_hra2);
+			
+	INIT_LIST_HEAD(&pReq->hdr_slist.frags);
+	INIT_LIST_HEAD(&pReq->req_data_slist.frags);
+	INIT_LIST_HEAD(&pReq->req_slist.frags);
+	INIT_LIST_HEAD(&pReq->hra1_slist.frags);
+	INIT_LIST_HEAD(&pReq->hra2_slist.frags);
+			
+	memset(pReq->header, 0, PAGE_SIZE);
+
+	atomic_set(&pReq->waitEvent, 0);
+			
+	atomic_set(&pReq->num_hras, 0);
+	pReq->retcode  = 0;
+	pReq->reqd_state = 0;
+	pReq->status = 0;
+	pReq->active = mode;
+
+	return HOST_DD_Good;
+}
+
+
+
+void
+request_release_old(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	atomic_set(&pReq->waitEvent, 0);
+	
+	pReq->active = REQUEST_INACTIVE;
+	pReq->retcode = 0;
+	pReq->reqd_state = 0;
+	atomic_set(&pReq->num_hras, 0);
+
+	spin_lock_bh(&ctx->req_pool_lock);
+	list_add_tail(&pReq->list_head, &ctx->req_pool);
+	spin_unlock_bh(&ctx->req_pool_lock);
+
+	// note that we don't free any memory associated with the request's
+	// DT chains or scatterlists.  that is the caller's responsibility.
+
+	// we're not using a mutex here.  as soon as we mark it inactive,
+	// it's fair game to other threads.  so we need the memory barrier.
+	//
+}
+
+
+void
+request_release(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	pReq->active = REQUEST_INACTIVE;
+
+	spin_lock_bh(&ctx->req_pool_lock);
+	if (list_empty(&ctx->req_wait_list)) {
+		list_add_tail(&pReq->list_head, &ctx->req_pool);
+		spin_unlock_bh(&ctx->req_pool_lock);
+	}
+	else {
+		request_wait_t *rw;
+		struct list_head *pLH = list_remove_head(&ctx->req_wait_list);
+
+		spin_unlock_bh(&ctx->req_pool_lock);
+
+		rw = list_entry(pLH, request_wait_t, list_head);
+		rw->pReq = pReq;
+		rw->rc   = HOST_DD_Good;
+
+		atomic_set(&rw->waitEvent, 1);
+		wake_up(&rw->waitQ);
+	}
+
+	// note that we don't free any memory associated with the request's
+	// DT chains or scatterlists.  that is the caller's responsibility.
+
+	// we're not using a mutex here.  as soon as we mark it inactive,
+	// it's fair game to other threads.  so we need the memory barrier.
+	//
+}
+
+
+
+// locate a request given the physical address of a DT.  2 situations
+// where this routine might be used:
+//
+// 1. trying to map an HRA back to its request.  might be a normal reply
+//    or maybe an asym log message.  in either case, only need to check
+//    the first DT of the HRA chains...
+//
+// 2. trying to map an RMRI back to a request.  in this case, the RMRI
+//    bit will have been set on the *last* DT in the request chain
+//    so we need to check the last DT of the request chains...
+//
+// since bus_to_virt() and phys_to_virt() have been deprecated, we can't just
+// convert the phys addr to a DT pointer and use the '->pReq' to get
+// the owner request.  we'll have to search each active request.  bleh.
+//
+request_t *
+request_locate_by_DT(ibm4767_ctx_t *ctx, uint64_t phys)
+{
+	request_t *pReq = NULL;
+	int i;
+
+
+	// assume 'phys' has already been adjusted for endianness
+
+	spin_lock_bh(&ctx->req_pool_lock);
+
+#if defined(ENABLE_REQ_TRACKING)
+	spin_lock_bh(&ctx->fetch_lock_skch);
+	if (ctx->last_DT_SKCH && ctx->last_DT_SKCH->this_dma == phys) {
+		pReq = ctx->last_DT_SKCH->pReq;
+		spin_unlock_bh(&ctx->fetch_lock_skch);
+		goto done;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_skch);
+
+	spin_lock_bh(&ctx->fetch_lock_pka);
+	if (ctx->last_DT_PKA && ctx->last_DT_PKA->this_dma == phys) {
+		pReq = ctx->last_DT_PKA->pReq;
+		spin_unlock_bh(&ctx->fetch_lock_pka);
+		goto done;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_pka);
+
+	spin_lock_bh(&ctx->fetch_lock_ssp);
+	if (ctx->last_DT_SSP && ctx->last_DT_SSP->this_dma == phys) {
+		pReq = ctx->last_DT_SSP->pReq;
+		spin_unlock_bh(&ctx->fetch_lock_ssp);
+		goto done;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_ssp);
+
+	spin_lock_bh(&ctx->fetch_lock_mcpu);
+	if (ctx->last_DT_MCPU && ctx->last_DT_MCPU->this_dma == phys) {
+		pReq = ctx->last_DT_MCPU->pReq;
+		spin_unlock_bh(&ctx->fetch_lock_mcpu);
+		goto done;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_mcpu);
+#endif
+
+	for (i=0; i < REQ_POOL_COUNT; i++) {
+		if (ctx->req_array[i].active) {
+			struct dt_t *pDT;
+
+			// typically this routine will be called to map an HRA back to 
+			// its request so we'll check those first.  only need to check
+			// the first DT of the HRA chains...
+			//
+			if (!list_empty(&ctx->req_array[i].DT_hra1)) {
+				pDT = list_entry(ctx->req_array[i].DT_hra1.next, struct dt_t, list_head);
+				if (pDT->this_dma == phys) {
+					pReq = pDT->pReq;
+					goto done;
+				}
+			}
+
+			if (!list_empty(&ctx->req_array[i].DT_hra2)) {
+				pDT = list_entry(ctx->req_array[i].DT_hra2.next, struct dt_t, list_head);
+				if (pDT->this_dma == phys) {
+					pReq = pDT->pReq;
+					goto done;
+				}
+			}
+
+#if defined(ENABLE_REQ_TRACKING)
+			// due to the way the driver moves the last DT of a request into purgatory and then prepends it
+			// onto a subsequent request, request tracking is tedious at best and sometimes flat-out doesn't work.
+			// enable at your own risk...
+
+			if (!list_empty(&ctx->req_array[i].DT_req)) {
+				pDT = list_entry(ctx->req_array[i].DT_req.next, struct dt_t, list_head);
+				if (pDT->this_dma == phys)  {
+					pReq = pDT->pReq;
+					goto done;
+				}
+
+				pDT = list_entry(ctx->req_array[i].DT_req.prev, struct dt_t, list_head);
+				if (pDT->this_dma == phys) {
+					pReq = pDT->pReq;
+					goto done;
+				}
+			}
+#endif
+		}
+	}
+
+
+#if 0
+	for (i=0; i < REQ_POOL_COUNT; i++) {
+		if (ctx->req_array[i].active) {
+			struct dt_t *pDT;
+			struct list_head *pLH, *tmp;
+			int n = 0;
+
+			list_for_each_safe(pLH, tmp, &ctx->req_array[i].DT_req) {
+				pDT = list_entry(pLH, struct dt_t, list_head);
+				if (pDT->this_dma == phys)  {
+					pReq = &ctx->req_array[i];
+					PRINTKW("Found DT at element %d\n", n);
+					goto done;
+				}
+				n++;
+			}
+		}
+	}
+#endif
+
+
+done:
+	spin_unlock_bh(&ctx->req_pool_lock);
+	return pReq;
+}
+
+
+observer_t *
+event_observer_allocate(ibm4767_ctx_t *ctx)
+{
+	observer_t *pObs = NULL;
+	int i;
+
+	// this is the only place where we use the observer pool spinlock...
+	//
+	spin_lock_bh(&ctx->observe_pool_lock);
+
+	for (i=0; i < OBS_POOL_COUNT; i++) {
+		if (ctx->observe_pool[i].active == OBSERVER_INACTIVE) {
+			ctx->observe_pool[i].active = OBSERVER_INUSE;
+
+			pObs = &ctx->observe_pool[i];
+			atomic_set(&pObs->waitEvent, 0);
+			pObs->event_code = 0;
+			goto done;
+		}
+	}
+
+	PDEBUG(1, "Device %d: %s: no more observers\n", ctx->dev_index, __FUNCTION__);
+
+done:
+	spin_unlock_bh(&ctx->observe_pool_lock);
+	return pObs;
+}
+
+
+void
+event_observer_release(observer_t *pObs)
+{
+	atomic_set(&pObs->waitEvent, 0);
+	pObs->event_code = 0;
+	mb();
+
+	pObs->active = OBSERVER_INACTIVE;
+}
+
+
+void
+notify_observers(ibm4767_ctx_t *ctx, uint32_t event)
+{
+	int i;
+
+	PDEBUG(2, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	for (i=0; i < OBS_POOL_COUNT; i++) {
+		observer_t *pObs = &ctx->observe_pool[i];
+
+		if (pObs->active) {
+			PDEBUG(2, "Device %d: %s: Waking up observer %d\n", ctx->dev_index, __FUNCTION__, i);
+			pObs->event_code = event;
+			atomic_set(&pObs->waitEvent, 1);
+			wake_up(&pObs->waitQ);
+		}
+	}
+}
+
+
+int
+mcpu_request(ibm4767_ctx_t *ctx, xcRB_t *pRB)
+{ 
+	request_t     *pReq = NULL;
+	char          *failtxt = NULL;
+	unsigned long  timeout;
+	uint32_t       hrb_len, hdr_len, hra1_len, hra2_len = 0;
+	request_wait_t rwt;
+#if defined(DEBUG)
+	int            hra1_fuzzed = FALSE;
+#endif
+	int            rc = HOST_DD_Good;
+
+
+	PDEBUG(2, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	switch (ctx->status_mcpu) {
+		case MCPU_UNINITIALIZED:
+			failtxt = "REQUEST failed (MCPU_UNINITIALIZED)";
+			rc = HOST_DD_NoDevice;
+			break;
+
+		case MCPU_IN_RESET:
+			failtxt = "REQUEST failed (MCPU_IN_RESET)";
+			rc = HOST_DD_NoDevice;
+			break;
+
+		case MCPU_BOOTING:
+			failtxt = "REQUEST failed (MCPU_BOOTING)";
+			rc = HOST_DD_NoDevice;
+			break;
+
+		case MCPU_POST2_START:
+			failtxt = "REQUEST failed (POST2_START)";
+			rc = HOST_DD_NoDevice;
+			break;
+		
+		case MCPU_POST2_HELLO:
+			failtxt = "REQUEST failed (POST2_HELLO)";
+			rc = HOST_DD_NoDevice;
+			break;
+
+		case MCPU_INITIALIZED:
+			failtxt = "REQUEST failed (MCPU_INITIALIZED)";
+			rc = HOST_DD_NoDevice;
+			break;
+		
+		case MCPU_ACTIVE:
+			rc = HOST_DD_Good;
+			break;
+
+		case MCPU_TEMP_SHUTDOWN:
+			failtxt = "REQUEST failed (MCPU_TEMP_SHUTDOWN)";
+			rc = HOST_DD_Temperature;
+			break;
+
+		case MCPU_OFFLINE:
+			failtxt = "REQUEST failed (MCPU_OFFLINE)";
+			rc = HOST_DD_NoDevice;
+			break;
+
+		default:
+			failtxt = "REQUEST failed (UNKNOWN STATE)";
+			rc = HOST_DD_NoDevice;
+	}
+
+	// some pathalogical user apps might ignore an error code and keep hitting xcRequest.
+	// so don't fill the syslog with "REQUEST failed" messages.  allow REQFAIL_MSG_COUNT
+	// messages per device boot cycle...
+	//
+	if (rc != HOST_DD_Good) {
+		spin_lock_bh(&ctx->counter_lock);
+		if (ctx->reqfail_msg_count < REQFAIL_MSG_COUNT) {
+			PRINTKW("Device %d: %s\n", ctx->dev_index, failtxt);
+
+			if (++ctx->reqfail_msg_count == REQFAIL_MSG_COUNT) {
+				PRINTKW("Device %d: Fail msg limit reached.  No more until next reset\n", ctx->dev_index);
+			}
+		}
+		spin_unlock_bh(&ctx->counter_lock);
+		return rc;
+	}
+
+
+	rc = request_allocate(ctx, REQUEST_MCPU, &rwt);
+	if (rc != HOST_DD_Good)
+		return rc;
+	pReq = rwt.pReq;
+
+	PDEBUG(3, "AgentID:                  %04x\n", pRB->AgentID);
+	PDEBUG(3, "UserDefined:              %08x\n", pRB->UserDefined);
+	PDEBUG(3, "RequestControlBlkLength:  %08x\n", pRB->RequestControlBlkLength);
+	PDEBUG(3, "RequestDataLength:        %08x\n", pRB->RequestDataLength);
+	PDEBUG(3, "ReplyControlBlkLength:    %08x\n", pRB->ReplyControlBlkLength);
+	PDEBUG(3, "ReplyDataLength:          %08x\n", pRB->ReplyDataLength);
+	PDEBUG(3, "PriorityWindow:           %04x\n", pRB->PriorityWindow);
+	PDEBUG(3, "Status:                   %08x\n", pRB->Status);
+
+	// sanity checks
+	//
+	if (pRB->PriorityWindow >= NUM_PRI_WINDOWS) {
+		PRINTKW("Device %d: REQUEST contained invalid priority window (%d)\n", ctx->dev_index, pRB->PriorityWindow);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	// sanity-check the request
+	//
+	if (!pRB->RequestControlBlkAddr) {
+		PRINTKW("Device %d: REQUEST contained a NULL RequestControlBlkAddr\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (pRB->RequestControlBlkLength == 0) {
+		PRINTKW("Device %d: REQUEST contained null RequestControlBlkLength\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if ((pRB->RequestDataAddress == NULL) && (pRB->RequestDataLength != 0)) {
+		PRINTKW("Device %d: REQUEST contained NULL RequestDataAddress and non-zero RequestDataLength\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if ((pRB->RequestDataAddress != NULL) && (pRB->RequestDataLength == 0)) {
+		PRINTKW("Device %d: REQUEST contained non-NULL RequestDataAddress but zero RequestDataLength\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if ((pRB->RequestControlBlkLength + pRB->RequestDataLength) > MAX_REQUEST_SIZE) {
+		PRINTKW("Device %d: total request size exceeds 0x%x bytes\n", ctx->dev_index, MAX_REQUEST_SIZE);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	// sanity-check the HRAs
+	//
+	if (!pRB->ReplyControlBlkAddr) {
+		PRINTKW("Device %d: REQUEST contained NULL ReplyControlBlkAddr\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (pRB->ReplyControlBlkLength == 0) {
+		PRINTKW("Device %d: REQUEST with ReplyControlBlkLength == 0\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (pRB->ReplyControlBlkLength > MAX_REPLY_CPRB_SIZE) {
+		PRINTKW("Device %d: REQUEST where ReplyControlBlkLength exceeds 0x%x bytes\n", ctx->dev_index, MAX_REPLY_CPRB_SIZE);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if ((pRB->ReplyDataAddr == NULL) && (pRB->ReplyDataLength != 0)) {
+		PRINTKW("Device %d: REQUEST with NULL ReplyDataAddr and non-zero ReplyDataLength\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+	
+	if ((pRB->ReplyDataAddr != NULL) && (pRB->ReplyDataLength == 0)) {
+		PRINTKW("Device %d: REQUEST with non-NULL ReplyDataAddr and null ReplyDataLength\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (pRB->ReplyDataLength > MAX_REPLY_DATA_SIZE) {
+		PRINTKW("Device %d: REQUEST where ReplyDataLength exceeds 0x%x bytes\n", ctx->dev_index, MAX_REPLY_DATA_SIZE);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	// in the Z universe, the destination MRB is determined based on fast/slow op status.  that's fine
+	// for them because their cards will only run CCA or EP11.  in the x86 universe, we might have 
+	// a custom seg3 app.  so we have to allow the app to specify its MRB.
+	//
+	switch (ibm4767_opcode_type(pRB->AgentID, pRB->UserDefined)) { 
+		case SLOW_OP: 
+			pRB->PriorityWindow = 1;
+			pReq->timeout = timeout_mcpu[pRB->PriorityWindow];
+			PDEBUG(2, "Device %d: SLOW op. Timeout in %d\n", ctx->dev_index, pReq->timeout);
+			break;
+
+		default: 
+			if (pRB->AgentID == CCAAGENT)
+				pRB->PriorityWindow = 0;
+			pReq->timeout = timeout_mcpu[pRB->PriorityWindow];
+			PDEBUG(2, "Device %d: NORMAL op. Timeout in %d\n", ctx->dev_index, pReq->timeout);
+	}
+
+	spin_lock_bh(&ctx->counter_lock);
+	atomic_inc(&ctx->mcpu_counter);
+	ctx->active_norm++;
+	if (pRB->PriorityWindow == 0)
+		atomic_inc(&ctx->mrb0_counter);
+	else
+		atomic_inc(&ctx->mrb1_counter);
+	spin_unlock_bh(&ctx->counter_lock);
+
+	// HRAs.  Need to account for HW requirement that DMA buffer lengths be
+	// a multiple of 8 bytes...
+	//
+	// HRA #1 needs to take into account mrm_type1_header_t
+	//
+	hra1_len = pRB->ReplyControlBlkLength + sizeof(mrm_type1_header_t);
+	hra1_len = (hra1_len + 7) & ~7;
+
+	rc = alloc_scatterlist(ctx,
+			       NULL, 0,
+			       hra1_len,
+			       FALSE,
+			       &pReq->hra1_slist,
+			       DMA_FROM_DEVICE);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist hra1 failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_hra1, &pReq->hra1_slist);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist hra1 failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	atomic_inc(&pReq->num_hras);
+
+
+	// HRA #2 needs to take into account mrm_type2_header_t...
+	//
+	if (pRB->ReplyDataAddr) {
+		hra2_len = pRB->ReplyDataLength + sizeof(mrm_type2_header_t);
+		hra2_len = (hra2_len + 7) & ~7;
+
+		rc = alloc_scatterlist(ctx,
+				       NULL, 0,
+				       hra2_len,
+				       FALSE,
+				       &pReq->hra2_slist,
+				       DMA_FROM_DEVICE);
+		if (rc != HOST_DD_Good) {
+			PRINTKW("Device %d: alloc_scatterlist hra2 failed\n", ctx->dev_index); 
+			goto cleanup; 
+		}
+
+		rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_hra2, &pReq->hra2_slist); 
+		if (rc != HOST_DD_Good) {
+			PRINTKW("Device %d: dt_append_scatterlist hra2 failed\n", ctx->dev_index); 
+			goto cleanup; 
+		}
+	
+		atomic_inc(&pReq->num_hras);
+	}
+
+#if defined(DEBUG)
+	// use request fuzzing with caution.  you've been warned.
+	if (enable_fuzzer) 
+		hra1_fuzzed = request_fuzzer(ctx, &hra1_len);
+#endif
+
+	// we set up the request chain last since we need the HRA bus addrs...
+	//
+	// things are more complicated because of comm mgr's variable-padding requirement
+	// (see Fence doc section 6.0).
+	//    1. RequestData block must immediately follow RequestControl block
+	//    2. RequestData block must be on an 8byte boundary
+	//
+	// for MCPU communications, the assembled request blob looks like:
+	//    hrb_type1_header_t or hrb_type3_header_t (depending on # HRAs)
+	//    [0-7 bytes padding]  -- this padding ensures RequestData alignment
+	//    hrb_header_part2_t
+	//    RequestControlBlock data
+	//    RequestDataBlock data
+	//    [0-7 bytes padding]  -- this ensures overall DMA xfer is multiple of 8
+	//
+
+	if (!pRB->ReplyDataAddr) {
+		// request has one HRA...
+		//
+		dma_header16_t    *pDMAhdr;
+		hrb_type3_header_t *pHdr1;
+		hrb_header_part2_t *pHdr2;
+		struct dt_t     *pDT;
+		char            *pBuf;
+
+		uint32_t   tmp;
+		uint32_t   pad1, pad2;
+		uint32_t   len1, len2, len3;
+
+		hdr_len = sizeof(hrb_type3_header_t) + sizeof(hrb_header_part2_t);
+		tmp  = (hdr_len + pRB->RequestControlBlkLength + 7) & ~7;
+		pad1 = tmp - (hdr_len + pRB->RequestControlBlkLength);
+		hdr_len += pad1;
+
+		tmp  = (pRB->RequestDataLength + 7) & ~7;
+		pad2 = tmp - pRB->RequestDataLength;
+
+		pBuf     = pReq->header;
+		pHdr1    = (hrb_type3_header_t *)pBuf;  pBuf += sizeof(hrb_type3_header_t);
+		pBuf    += pad1;
+		pHdr2    = (hrb_header_part2_t *)pBuf;  pBuf += sizeof(hrb_header_part2_t);
+
+		pDMAhdr  = (dma_header16_t *)pHdr1;
+
+		len3 = sizeof(hrb_type3_header_t) 
+			- sizeof(pHdr1->len3) 
+			- sizeof(pHdr1->len2) 
+			- sizeof(dma_header16_t) 
+			+ pad1 + sizeof(hrb_header_part2_t);
+
+		len2 = len3 + sizeof(pHdr1->len3) 
+			+ pRB->RequestControlBlkLength 
+			+ pRB->RequestDataLength 
+			+ pad2;
+
+		hrb_len = len1 = (len2 
+			+ sizeof(pHdr1->len2) 
+			+ sizeof(dma_header8_t)) & 0xFFFFF;
+
+		pDMAhdr->hdr8.bytes[0] = DMA_HTYPE_H2M;
+		pDMAhdr->hdr8.bytes[1] = DMA_RQSTR_HOST | DMA_CW_NORM;
+		pDMAhdr->hdr8.bytes[2] = DMA_CW_TX | DMA_CW_INT_NOIV;
+		if (pRB->PriorityWindow == 0)	
+			pDMAhdr->hdr8.bytes[2] |= DMA_CW_MRB0;
+		else
+			pDMAhdr->hdr8.bytes[2] |= DMA_CW_MRB1;
+		pDMAhdr->hdr8.bytes[3] = 0;  // UF (reserved)
+		pDMAhdr->hdr8.bytes[4] = ctx->vfid;
+		// for host->card DMA, the DMA header length is ignored by hardware
+		//pDMAhdr->hdr8.bytes[5] = (len1 & 0x0F0000) >> 16;
+		//pDMAhdr->hdr8.bytes[6] = (len1 & 0x00FF00) >>  8;
+		//pDMAhdr->hdr8.bytes[7] = (len1 & 0x0000FF);
+		pDMAhdr->sig           = cpu_to_be16(DMA_SIG_H2M);
+
+		pHdr1->len2          = cpu_to_be16(len2);
+		pHdr1->len3          = cpu_to_be16(len3);
+		pHdr1->ep_len        = pad2;
+		pHdr1->flags         = HRB_FLAGS_NORMAL;
+		pHdr1->agent_id      = cpu_to_be16(pRB->AgentID);
+		pHdr1->num_hras      = cpu_to_be16(1);
+		pHdr1->tag_len_hra.tag    = 0x1;
+		pHdr1->tag_len_hra.len[0] = (hra1_len & 0x0F0000) >> 16;
+		pHdr1->tag_len_hra.len[1] = (hra1_len & 0x00FF00) >>  8;
+		pHdr1->tag_len_hra.len[2] = (hra1_len & 0x0000FF);
+#if defined(WINDOWS)
+		pDT = CONTAINING_RECORD(pReq->DT_hra1.Flink, struct dt_t, list_entry);
+		pHdr1->tag_len_hra.hra = cpu_to_be64(pDT->this_LA);
+#endif
+#if defined(LINUX)
+		pDT = list_entry(pReq->DT_hra1.next, struct dt_t, list_head);
+		pHdr1->tag_len_hra.hra = cpu_to_be64(pDT->this_dma);
+#endif
+		pHdr2->frag_idx    = 0;
+		pHdr2->frag_total  = 0;
+		pHdr2->request_id  = cpu_to_be16(pReq->request_id);
+		pHdr2->user_def    = cpu_to_be32(pRB->UserDefined);
+		pHdr2->status      = cpu_to_be32(pRB->Status);
+	}
+	else {
+		// request has two HRAs...
+		//
+		dma_header16_t    *pDMAhdr;
+		hrb_type1_header_t    *pHdr1;
+		hrb_header_part2_t     *pHdr2;
+		struct dt_t     *pDT;
+		char            *pBuf;
+
+		uint32_t   tmp;
+		uint32_t   pad1, pad2;
+		uint32_t   len1, len2, len3;
+
+		hdr_len = sizeof(hrb_type1_header_t) + sizeof(hrb_header_part2_t);
+		tmp  = (hdr_len + pRB->RequestControlBlkLength + 7) & ~7;
+		pad1 = tmp - (hdr_len + pRB->RequestControlBlkLength);
+		hdr_len += pad1;
+
+		tmp  = (pRB->RequestDataLength + 7) & ~7;
+		pad2 = tmp - pRB->RequestDataLength;
+
+		pBuf     = pReq->header;
+		pHdr1    = (hrb_type1_header_t *)pBuf;  pBuf += sizeof(hrb_type1_header_t);
+		pBuf    += pad1;
+		pHdr2    = (hrb_header_part2_t  *)pBuf;  pBuf += sizeof(hrb_header_part2_t);
+		
+		pDMAhdr  = (dma_header16_t *)pHdr1;
+
+		len3 = sizeof(hrb_type1_header_t) 
+			- sizeof(pHdr1->len3) 
+			- sizeof(pHdr1->len2) 
+			- sizeof(dma_header16_t) 
+			+ pad1 + sizeof(hrb_header_part2_t);
+
+		len2 = len3 + sizeof(pHdr1->len3) 
+			+ pRB->RequestControlBlkLength 
+			+ pRB->RequestDataLength 
+			+ pad2;
+
+		hrb_len = len1 = (len2 
+			+ sizeof(pHdr1->len2) 
+			+ sizeof(dma_header8_t)) & 0xFFFFF;  
+		
+		pDMAhdr->hdr8.bytes[0] = DMA_HTYPE_H2M;
+		pDMAhdr->hdr8.bytes[1] = DMA_RQSTR_HOST | DMA_CW_NORM;
+		pDMAhdr->hdr8.bytes[2] = DMA_CW_TX | DMA_CW_INT_NOIV;
+		if (pRB->PriorityWindow == 0)	
+			pDMAhdr->hdr8.bytes[2] |= DMA_CW_MRB0;
+		else
+			pDMAhdr->hdr8.bytes[2] |= DMA_CW_MRB1;
+		pDMAhdr->hdr8.bytes[3] = 0;  // UF (reserved)
+		pDMAhdr->hdr8.bytes[4] = ctx->vfid;
+		// for host->card DMA, the DMA header length is ignored by hardware
+		//pDMAhdr->hdr8.bytes[5] = (len1 & 0x0F0000) >> 16;
+		//pDMAhdr->hdr8.bytes[6] = (len1 & 0x00FF00) >>  8;
+		//pDMAhdr->hdr8.bytes[7] = (len1 & 0x0000FF);
+		pDMAhdr->sig           = cpu_to_be16(DMA_SIG_H2M);
+
+		pHdr1->len2          = cpu_to_be16(len2);
+		pHdr1->len3          = cpu_to_be16(len3);
+		pHdr1->ep_len        = pad2;
+		pHdr1->flags         = HRB_FLAGS_NORMAL;
+		pHdr1->agent_id      = cpu_to_be16(pRB->AgentID);
+		pHdr1->num_hras      = cpu_to_be16(2);
+		pHdr1->tag_len_hra[0].tag    = 0x1;
+		pHdr1->tag_len_hra[0].len[0] = (hra1_len & 0x0F0000) >> 16;
+		pHdr1->tag_len_hra[0].len[1] = (hra1_len & 0x00FF00) >>  8;
+		pHdr1->tag_len_hra[0].len[2] = (hra1_len & 0x0000FF);
+#if defined(WINDOWS)
+		pDT = CONTAINING_RECORD(pReq->DT_hra1.Flink, struct dt_t, list_entry);
+		pHdr1->tag_len_hra[0].hra = cpu_to_be64(pDT->this_LA);
+#endif
+#if defined(LINUX)
+		pDT = list_entry(pReq->DT_hra1.next, struct dt_t, list_head);
+		pHdr1->tag_len_hra[0].hra = cpu_to_be64(pDT->this_dma);
+#endif
+		
+		pHdr1->tag_len_hra[1].tag    = 0x2;
+		pHdr1->tag_len_hra[1].len[0] = (hra2_len & 0x0F0000) >> 16;
+		pHdr1->tag_len_hra[1].len[1] = (hra2_len & 0x00FF00) >>  8;
+		pHdr1->tag_len_hra[1].len[2] = (hra2_len & 0x0000FF);
+#if defined(WINDOWS)
+		pDT = CONTAINING_RECORD(pReq->DT_hra2.Flink, struct dt_t, list_entry);
+		pHdr1->tag_len_hra[1].hra = cpu_to_be64(pDT->this_LA);
+#endif
+#if defined(LINUX)
+		pDT = list_entry(pReq->DT_hra2.next, struct dt_t, list_head);
+		pHdr1->tag_len_hra[1].hra = cpu_to_be64(pDT->this_dma);
+#endif
+		pHdr2->request_id  = cpu_to_be16(pReq->request_id);
+		pHdr2->user_def    = cpu_to_be32(pRB->UserDefined);
+		pHdr2->status      = cpu_to_be32(pRB->Status);
+	}
+		
+	rc = alloc_scatterlist(ctx, 
+			       pReq->header, hdr_len, 
+			       hdr_len, 
+			       TRUE,
+			       &pReq->hdr_slist,
+			       DMA_TO_DEVICE); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist hdr failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+		
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->hdr_slist); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist hdr failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+	dt_set_hrb_len(&pReq->DT_req, hrb_len);
+
+	rc = alloc_scatterlist(ctx, 
+			       pRB->RequestControlBlkAddr, 
+			       pRB->RequestControlBlkLength, 
+			       pRB->RequestControlBlkLength, 
+			       FALSE,
+			       &pReq->req_slist,
+			       DMA_TO_DEVICE); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist req failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->req_slist); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist req1 failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+
+	if (pRB->RequestDataAddress) {
+		// the Fence spec requires that RequestData start on an 8-byte boundary
+		// (hence the first padding block in the header).  the HRB length needs to be
+		// a multiple of 8 so we padd the RequestData...
+		//
+		rc = alloc_scatterlist(ctx,
+				       pRB->RequestDataAddress,
+				       pRB->RequestDataLength,
+				       ((pRB->RequestDataLength + 7) & ~7),
+				       FALSE,
+				       &pReq->req_data_slist,
+				       DMA_TO_DEVICE);
+	
+		if (rc != HOST_DD_Good) {
+			PRINTKW("Device %d: alloc_scatterlist req2 failed\n", ctx->dev_index); 
+			goto cleanup; 
+		}
+
+		rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->req_data_slist); 
+		if (rc != HOST_DD_Good) {
+			PRINTKW("Device %d: dt_append_scatterlist req2 failed\n", ctx->dev_index); 
+			goto cleanup; 
+		}
+
+		//PDEBUG(ctx, 1, "REQ chain\n");
+		//dump_chain(&pReq->DT_req, FALSE);
+	}
+
+#if defined(ENABLE_REQ_TRACKING)
+	dt_set_rmri_last(&pReq->DT_req, pReq);
+#endif
+
+	rc = dma_send_request_MCPU(ctx, pReq);
+	PDEBUG(2, "Device %d: rc from dma_send_request_MCPU: 0x%x\n", ctx->dev_index, rc);
+	if (rc != HOST_DD_Good) 
+		goto signal;
+
+	spin_lock_bh(&ctx->timeout_lock);
+	ctx->timeout_sum += pReq->timeout;
+	timeout = ctx->timeout_sum * HZ;
+	spin_unlock_bh(&ctx->timeout_lock);
+
+	rc = wait_event_timeout(pReq->waitQ, atomic_read(&pReq->waitEvent), timeout);
+
+	spin_lock_bh(&ctx->timeout_lock);
+	ctx->timeout_sum -= pReq->timeout;
+	spin_unlock_bh(&ctx->timeout_lock);
+
+	if (rc == 0) {
+		atomic_set(&pReq->waitEvent, 1);
+		wmb();
+
+		PRINTKW("Device %d: TIMEOUT (AgentID: %04x UserDef: %08x). Flushing queue\n", ctx->dev_index, pRB->AgentID, pRB->UserDefined);
+		mark_mcpu_offline(ctx, HOSTTimeout);
+		mod_timer(&ctx->deferred_forced_edump_timer, jiffies + HZ);
+	}
+	else {
+		rc = HOST_DD_Good;
+	}
+	
+		
+
+signal:
+	if (pReq->retcode != 0) {
+		PDEBUG(1, "Device %d: retcode = 0x%x\n", ctx->dev_index, pReq->retcode);
+		pRB->ReplyControlBlkLength = 0;
+
+		// out-of-band return codes are slowly being converted to 32-bit codes with the
+		// source prefix.  all negative POSIX codes should be gone by now but there still
+		// might be some 16-bit HOST_DD_* codes remaining...
+		//
+		if (pReq->retcode < 0xFFFF)
+			pRB->Status = HOST_DD_ERR | (pReq->retcode & 0xFFFF);
+		else
+			pRB->Status = pReq->retcode;
+		rc = pReq->retcode;
+		goto cleanup;
+	}
+
+
+	// now copy the reply HRAs back to userspace...
+	{
+		struct list_head *pLH;
+		scatter_frag_t     *sf;
+		char             *src, *dest;
+		uint32_t          size, i, remain;
+
+
+		i = 0;
+		dest   = pRB->ReplyControlBlkAddr;
+		remain = pRB->ReplyControlBlkLength;
+
+		list_for_each(pLH, &pReq->hra1_slist.frags) {
+			sf   = list_entry(pLH, scatter_frag_t, list_head);
+			src  = sf->vp.va;
+			size = sf->len;  // not the same as sf->vp.len!
+
+			dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+			
+			if (i++ == 0) {
+				mrm_type1_header_t *hdr = (mrm_type1_header_t *)src;
+				// the driver is responsible for proper byte ordering of pRB
+				pRB->Status      = cpu_to_be32(hdr->status);
+				pRB->UserDefined = cpu_to_be32(hdr->user_def);
+
+#if defined(DEBUG)
+				if (hra1_fuzzed && (pRB->Status != 0x8042200E)) {
+					PRINTKW("Device %d: fuzzed HRA1 length to 0x%x bytes.  Expected 0x8042200E.  Received 0x%08x\n", ctx->dev_index, hra1_len, pRB->Status);
+					pRB->Status = HOSTFirmwareError;
+				}
+#endif
+
+
+				src  += sizeof(mrm_type1_header_t);
+				size -= sizeof(mrm_type1_header_t);
+			}
+
+			size = min(size, remain);
+
+			PDEBUG(1, "Device %d: HRA1: Copying %d bytes to userspace...\n", ctx->dev_index, size);
+
+			if (copy_to_user(dest, src, size)) {
+				PRINTKW("Device %d: copy_to_user failed\n", ctx->dev_index);
+				rc = HOST_DD_BadAddress;
+				goto cleanup;
+			}
+
+			dest += size;
+			remain -= size;
+			if (remain == 0)
+				break;
+		}
+
+		// I do not believe the following sanity check is necessary but BEAM insists on it lest it
+		// complain about NULL pointers inside the loop that in reality cannot be NULL...
+		//
+		if (pRB->ReplyDataAddr) {
+			i = 0;
+			dest   = pRB->ReplyDataAddr;
+			remain = pRB->ReplyDataLength;
+
+			list_for_each(pLH, &pReq->hra2_slist.frags) {
+				sf   = list_entry(pLH, scatter_frag_t, list_head);
+				src  = sf->vp.va;
+				size = sf->len;  // not the same as sf->vp.len!
+
+				dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+			
+				if (i++ == 0) {
+					src  += sizeof(mrm_type2_header_t);
+					size -= sizeof(mrm_type2_header_t);
+				}
+
+				size = min(size, remain);
+
+				PDEBUG(1, "Device %d: HRA2: Copying %d bytes to userspace...\n", ctx->dev_index, size);
+
+				if (copy_to_user(dest, src, size)) {
+					PRINTKW("Device %d: copy_to_user failed\n", ctx->dev_index);
+					rc = HOST_DD_BadAddress;
+					goto cleanup;
+				}
+
+				dest += size;
+				remain -= size;
+				if (remain == 0)
+					break;
+			}
+		}
+	}
+
+cleanup:
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->active_norm--;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	if (pReq) {
+		free_scatterlist(&pReq->hdr_slist);
+		free_scatterlist(&pReq->req_slist);
+		free_scatterlist(&pReq->req_data_slist);
+		free_scatterlist(&pReq->hra1_slist);
+		free_scatterlist(&pReq->hra2_slist);
+
+		dt_free_chain(ctx, &pReq->DT_req);
+		dt_free_chain(ctx, &pReq->DT_hra1);
+		dt_free_chain(ctx, &pReq->DT_hra2);
+
+		// we probably ought to verify that the last DT is not freed
+		// until the next request
+		//
+		request_release(ctx, pReq);
+	}
+
+	return rc;
+}
+
+
+int
+fastpath_request(ibm4767_ctx_t *ctx, xcRB_t *pRB)
+{
+	request_t           *pReq = NULL;
+	unsigned long        timeout;
+	uint32_t            *timeout_sum = NULL;
+	uint32_t             hra1_len = 0, padded_hrb_len;
+	request_wait_t       rwt;
+	int                  rc = HOST_DD_Good;
+
+	if (ctx->status_fp == FP_OFFLINE)
+		return HOST_DD_NoDevice;
+
+#if 1
+	rc = request_allocate(ctx, REQUEST_FP, &rwt);
+	if (rc != HOST_DD_Good)
+		return rc;
+	pReq = rwt.pReq;
+#else
+	pReq = request_allocate_old(ctx, REQUEST_FP);
+	if (!pReq)
+		return HOST_DD_DeviceBusy;
+#endif
+
+	spin_lock_bh(&ctx->counter_lock);
+	atomic_inc(&ctx->fp_counter);
+	ctx->active_fp++;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	PDEBUG(1, "Device %d: Enter %s...\n", ctx->dev_index, __FUNCTION__);
+
+	PDEBUG(3, "AgentID:                  %04x\n", pRB->AgentID);
+	PDEBUG(3, "UserDefined:              %08x\n", pRB->UserDefined);
+	PDEBUG(3, "RequestControlBlkLength:  %08x\n", pRB->RequestControlBlkLength);
+	PDEBUG(3, "RequestDataLength:        %08x\n", pRB->RequestDataLength);
+	PDEBUG(3, "ReplyControlBlkLength:    %08x\n", pRB->ReplyControlBlkLength);
+	PDEBUG(3, "ReplyDataLength:          %08x\n", pRB->ReplyDataLength);
+	PDEBUG(3, "PriorityWindow:           %04x\n", pRB->PriorityWindow);
+	PDEBUG(3, "Status:                   %08x\n", pRB->Status);
+
+	// sanity-check the request
+	//
+	if ((pRB->AgentID != XC_AGENTID_SKCH_FASTPATH) && 
+	    (pRB->AgentID != XC_AGENTID_PKA_FASTPATH) &&
+	    (pRB->AgentID != XC_AGENTID_FPGAA_FASTPATH) &&
+	    (pRB->AgentID != XC_AGENTID_FPGAB_FASTPATH)) {
+		PRINTKW("Device %d: Invalid FP AgentID\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (!pRB->RequestControlBlkAddr) {
+		PRINTKW("Device %d: FP contained a NULL RequestControlBlkAddr\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (pRB->RequestControlBlkLength == 0) {
+		PRINTKW("Device %d: FP contained null RequestControlBlkLength\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if ((pRB->RequestControlBlkLength) > MAX_FP_REQUEST_SIZE) {
+		PRINTKW("Device %d: total request size exceeds 0x%x bytes\n", ctx->dev_index, MAX_FP_REQUEST_SIZE);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	// sanity-check the HRA
+	//
+	if (!pRB->ReplyControlBlkAddr) {
+		PRINTKW("Device %d: REQUEST contained NULL ReplyControlBlkAddr\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (pRB->ReplyControlBlkLength == 0) {
+		PRINTKW("Device %d: REQUEST with ReplyControlBlkLength == 0\n", ctx->dev_index);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	if (pRB->ReplyControlBlkLength > MAX_FP_REQUEST_SIZE) {
+		PRINTKW("Device %d: REQUEST where ReplyControlBlkLength exceeds 0x%x bytes\n", ctx->dev_index, MAX_REPLY_CPRB_SIZE);
+		rc = HOST_DD_InvalidParm;
+		goto cleanup;
+	}
+
+	// hardware limitation: the internal DMA channels must be enabled
+	// prior to the corresponding host-side channels.  in general, this
+	// happens just before an agent sends the _HELLO notification.  this
+	// doesn't meld well with the overall FTE design.  for reasons I don't
+	// fully understand, TAS cannot enable FP channels and by the time
+	// a TAPP is able to enable the channels, the _HELLO will have already
+	// been sent.
+	//
+	// so to workaround this, we try to enable FP channels on the first
+	// request...
+	//
+	switch (pRB->AgentID) {
+		case XC_AGENTID_SKCH_FASTPATH:
+			rc = dma_check_fp_enable(ctx, HBMSR_H2SK_MASTER_EN, HBMCR_BMEN_SKCH);
+			break;
+		case XC_AGENTID_PKA_FASTPATH:
+			rc = dma_check_fp_enable(ctx, HBMSR_H2MM_MASTER_EN, HBMCR_BMEN_MM);
+			break;
+		case XC_AGENTID_FPGAA_FASTPATH:
+			rc = dma_check_fp_enable(ctx, HBMSR_H2FA_MASTER_EN, HBMCR_BMEN_FPGAA);
+			break;
+		case XC_AGENTID_FPGAB_FASTPATH:
+			rc = dma_check_fp_enable(ctx, HBMSR_H2FB_MASTER_EN, HBMCR_BMEN_FPGAB);
+			break;
+	}
+
+	if (rc) {
+		atomic_inc(&ctx->ebusy_counter);
+		rc = HOST_DD_DeviceBusy;
+		goto cleanup;
+	}
+
+
+	// HRA.  Need to account for HW requirement that DMA buffer lengths be
+	// a multiple of 8 bytes...
+	//
+	// HRA #1 needs to account for the header also
+	//
+	hra1_len = pRB->ReplyControlBlkLength + sizeof(fp_reply_header_t);
+	hra1_len = (hra1_len + 7) & ~7;
+
+	rc = alloc_scatterlist(ctx,
+			       NULL, 0,
+			       hra1_len,
+			       FALSE,
+			       &pReq->hra1_slist,
+			       DMA_FROM_DEVICE);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist hra1 failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_hra1, &pReq->hra1_slist);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist hra1 failed\n", ctx->dev_index);
+		goto cleanup;
+	}
+
+	atomic_inc(&pReq->num_hras);
+
+	// it's the host library/application's responsibility to assemble a correct
+	// fastpath request block.  the driver takes this block, tacks on an DMA HRB header
+	// and sends it to the card...
+	//
+	{
+		fp_request_header_t *pDMAhdr;
+		struct dt_t       *pDT;
+		uint32_t           len;
+
+		len = sizeof(fp_request_header_t) 
+			- sizeof(dma_header8_t)
+			+ pRB->RequestControlBlkLength;
+
+		PDEBUG(2, "Device %d: len = %d (x%08x)\n", ctx->dev_index, len, len);
+
+		padded_hrb_len = ((len + 7) & ~7) & 0xFFFFF;  // 20 bits
+		
+		PDEBUG(2, "Device %d: padded len = %d (x%08x)\n", ctx->dev_index, padded_hrb_len, padded_hrb_len);
+
+		pDMAhdr = (fp_request_header_t *)pReq->header;
+
+		pDMAhdr->hdr8.bytes[1] = DMA_RQSTR_HOST | DMA_CW_NORM;
+		pDMAhdr->hdr8.bytes[2] = DMA_CW_TX | DMA_CW_MRB0 | DMA_CW_INT_IV;
+		pDMAhdr->hdr8.bytes[3] = 0;  // UF (reserved)
+		pDMAhdr->hdr8.bytes[4] = ctx->vfid;
+		// for host->card DMA, the DMA header length is ignored by hardware
+		//pDMAhdr->hdr8.bytes[5] = (padded_hrb_len & 0x0F0000) >> 16;
+		//pDMAhdr->hdr8.bytes[6] = (padded_hrb_len & 0x00FF00) >>  8;
+		//pDMAhdr->hdr8.bytes[7] = (padded_hrb_len & 0x0000FF);
+
+		if (pRB->AgentID == XC_AGENTID_SKCH_FASTPATH) {
+			pDMAhdr->hdr8.bytes[0] = DMA_HTYPE_H2SKA;
+			pDMAhdr->sig = cpu_to_be16(DMA_SIG_H2SKCH);
+		}
+		else if (pRB->AgentID == XC_AGENTID_PKA_FASTPATH) {
+			pDMAhdr->hdr8.bytes[0] = DMA_HTYPE_H2PKA;
+			pDMAhdr->sig = cpu_to_be16(DMA_SIG_H2PKA);
+		}
+		else if (pRB->AgentID == XC_AGENTID_FPGAA_FASTPATH) {
+			pDMAhdr->hdr8.bytes[0] = DMA_HTYPE_H2FA;
+			pDMAhdr->sig = cpu_to_be16(DMA_SIG_H2FPGA);
+		}
+		else if (pRB->AgentID == XC_AGENTID_FPGAB_FASTPATH) {
+			pDMAhdr->hdr8.bytes[0] = DMA_HTYPE_H2FB;
+			pDMAhdr->sig = cpu_to_be16(DMA_SIG_H2FPGA);
+		}
+
+#if defined(WINDOWS)
+		pDT = CONTAINING_RECORD(pReq->DT_hra1.Flink, struct dt_t, list_entry);
+		pDMAhdr->dra = cpu_to_be64(pDT->this_LA);
+		pDMAhdr->request_id = cpu_to_be64(pDT->this_LA);
+#endif
+#if defined(LINUX)
+		pDT = list_entry(pReq->DT_hra1.next, struct dt_t, list_head);
+		pDMAhdr->dra = cpu_to_be64(pDT->this_dma);
+		pDMAhdr->request_id = cpu_to_be64(pDT->this_dma);
+#endif
+	}
+
+	rc = alloc_scatterlist(ctx, 
+			       pReq->header, sizeof(fp_request_header_t), 
+			       sizeof(fp_request_header_t), 
+			       TRUE,
+			       &pReq->hdr_slist,
+			       DMA_TO_DEVICE); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist hdr failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+		
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->hdr_slist); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist hdr failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+
+	rc = alloc_scatterlist(ctx, 
+			       pRB->RequestControlBlkAddr, 
+			       pRB->RequestControlBlkLength, 
+			       pRB->RequestControlBlkLength, 
+			       FALSE,
+			       &pReq->req_slist,
+			       DMA_TO_DEVICE); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist req failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->req_slist); 
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist req1 failed\n", ctx->dev_index); 
+		goto cleanup; 
+	}
+
+	dt_set_hrb_len(&pReq->DT_req, padded_hrb_len);
+
+#if defined(ENABLE_REQ_TRACKING)
+	dt_set_rmri_last(&pReq->DT_req, pReq);
+#endif
+
+	if (pRB->AgentID == XC_AGENTID_SKCH_FASTPATH) {
+		rc = dma_send_request_SKCH(ctx, pReq);
+		PDEBUG(2, "Device %d: rc from dma_send_request_SKCH: 0x%x\n", ctx->dev_index, rc);
+		if (rc != HOST_DD_Good) 
+			goto signal;
+
+		pReq->timeout = timeout_skch; 
+		timeout_sum = &ctx->timeout_sum_skch;
+	}
+	else if (pRB->AgentID == XC_AGENTID_PKA_FASTPATH) {
+		rc = dma_send_request_PKA(ctx, pReq);
+		PDEBUG(2, "Device %d: rc from dma_send_request_PKA: 0x%x\n", ctx->dev_index, rc);
+		if (rc != HOST_DD_Good) 
+			goto signal;
+		pReq->timeout = timeout_pka; 
+		timeout_sum = &ctx->timeout_sum_pka;
+	}
+	else if (pRB->AgentID == XC_AGENTID_FPGAA_FASTPATH) {
+		rc = dma_send_request_FPGA_A(ctx, pReq);
+		PDEBUG(2, "Device %d: rc from dma_send_request_FPGA_A: 0x%x\n", ctx->dev_index, rc);
+		if (rc != HOST_DD_Good) 
+			goto signal;
+		pReq->timeout = timeout_fpgaA; 
+		timeout_sum = &ctx->timeout_sum_fpgaA;
+	}
+	else if (pRB->AgentID == XC_AGENTID_FPGAB_FASTPATH) {
+		rc = dma_send_request_FPGA_B(ctx, pReq);
+		PDEBUG(2, "Device %d: rc from dma_send_request_FPGA_B: 0x%x\n", ctx->dev_index, rc);
+		if (rc != HOST_DD_Good) 
+			goto signal;
+		pReq->timeout = timeout_fpgaB; 
+		timeout_sum = &ctx->timeout_sum_fpgaB;
+	}
+
+	spin_lock_bh(&ctx->timeout_lock); 
+	*timeout_sum += pReq->timeout; 
+	timeout = *timeout_sum * HZ;
+	spin_unlock_bh(&ctx->timeout_lock);
+
+	rc = wait_event_timeout(pReq->waitQ, atomic_read(&pReq->waitEvent), timeout);
+
+	spin_lock_bh(&ctx->timeout_lock);
+	*timeout_sum -= pReq->timeout;
+	spin_unlock_bh(&ctx->timeout_lock);
+	
+	if (rc == 0) {
+		PRINTKW("Device %d: FASTPATH TIMEOUT!  Flushing queue\n", ctx->dev_index);
+		PRINTKW("pReq->waitEvent:  %d\n", atomic_read(&pReq->waitEvent));
+        spin_lock_bh(&ctx->req_pool_lock);
+		flush_request(pReq, HOSTTimeout);
+        spin_unlock_bh(&ctx->req_pool_lock);
+
+#if defined(DEBUG)
+		// dump the HRA to see if it had been written-to.  just dump the FP reply header piece...
+		{
+			struct list_head *pLH;
+			scatter_frag_t   *sf;
+
+			pLH  = pReq->hra1_slist.frags.next;
+			sf   = list_entry(pLH, scatter_frag_t, list_head);
+
+			dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+			hex_dump(ctx, 0, "Dumping timed-out reply header", sf->vp.va, 24);
+		}
+#endif
+
+		// FIXME - should we mark the whole card offline?
+		//mark_fp_offline(ctx, HOSTTimeout);
+		mark_device_offline(ctx, HOSTTimeout);
+	}
+	else {
+		rc = HOST_DD_Good;
+	}
+
+signal:
+	if (pReq->retcode != 0) {
+		PDEBUG(1, "Device %d: retcode = 0x%x\n", ctx->dev_index, pReq->retcode);
+		pRB->ReplyControlBlkLength = 0;
+		
+		// out-of-band return codes are slowly being converted to 32-bit codes with the
+		// source prefix.  all negative POSIX codes should be gone by now but there still
+		// might be some 16-bit HOST_DD_* codes remaining...
+		//
+		if (pReq->retcode < 0xFFFF)
+			pRB->Status = HOST_DD_ERR | (pReq->retcode & 0xFFFF);
+		else
+			pRB->Status = pReq->retcode;
+		rc = pReq->retcode;
+		goto cleanup;
+	}
+
+
+	// copy the reply HRA back to userspace sans the reply header...
+	//
+	{
+		struct list_head *pLH;
+		scatter_frag_t     *sf;
+		char             *src, *dest;
+		uint32_t          size, i, remain = 0;
+		
+		i = 0;
+		dest   = pRB->ReplyControlBlkAddr;
+
+		list_for_each(pLH, &pReq->hra1_slist.frags) {
+			sf   = list_entry(pLH, scatter_frag_t, list_head);
+			src  = sf->vp.va;
+			size = sf->len;  // not the same as sf->vp.len!
+
+			dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+
+			//hex_dump(ctx, 0, "Dumping first 64 bytes FP reply", sf->vp.va, 64);
+			
+			if (i++ == 0) {
+				fp_reply_header_t *hdr = (fp_reply_header_t *)src;
+				remain = (hdr->hdr8.bytes[5] & 0x0F) << 16 |
+					 (hdr->hdr8.bytes[6]       ) <<  8 |
+					 (hdr->hdr8.bytes[7]       );
+				remain -= 16;  // strip DRA and RequestID
+
+				if (remain > pRB->ReplyControlBlkLength) {
+					PRINTKW("Device %d: reply buffer too small\n", ctx->dev_index);
+					rc = HOST_DD_BufferTooSmall;
+					goto cleanup;
+				}
+
+				pRB->ReplyControlBlkLength = remain;
+
+				src  += sizeof(fp_reply_header_t);
+				size -= sizeof(fp_reply_header_t);
+			}
+
+			size = min(size, remain);
+
+			PDEBUG(1, "Device %d: HRA1: Copying %d bytes to userspace...\n", ctx->dev_index, size);
+
+			if (copy_to_user(dest, src, size)) {
+				PRINTKW("Device %d: copy_to_user failed\n", ctx->dev_index);
+				rc = HOST_DD_BadAddress;
+				goto cleanup;
+			}
+
+#if defined(DEBUG)
+			hex_dump(ctx, 3, "contents:", src, size);
+#endif
+
+			dest += size;
+			remain -= size;
+			if (remain == 0)
+				break;
+		}
+	}
+
+
+cleanup:
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->active_fp--;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	if (pReq) {
+		free_scatterlist(&pReq->hdr_slist);
+		free_scatterlist(&pReq->req_slist);
+		free_scatterlist(&pReq->hra1_slist);
+
+		dt_free_chain(ctx, &pReq->DT_req);
+		dt_free_chain(ctx, &pReq->DT_hra1);
+
+		// we probably ought to verify that the last DT is not freed
+		// until the next request
+		//
+		request_release(ctx, pReq);
+	}
+
+	return rc;
+}
+
+
+//
+// flush routines (moved from util.c)
+//
+
+
+// this routine expects 'req_pool_lock' to be held by the caller...
+//
+void
+flush_request(request_t *req, int code)
+{
+#if defined(DEBUG)
+	unsigned char *txt = NULL;
+	struct dt_t   *dt  = NULL;
+#endif
+
+	if (req->active == REQUEST_INACTIVE)
+		return;
+
+#if defined(DEBUG)
+#if defined(ENABLE_REQ_TRACKING)
+
+	if (req->status & REQ_STATUS_XFERRED)
+		txt = "TXD";
+	else
+		txt = "NOT TXD";
+#else
+	txt = "XFER NOT TRACKED";
+#endif
+
+	dt = (struct dt_t *)list_entry(req->DT_req.next, struct dt_t, list_head);
+	//PDEBUG(1, "REQ:: DT dma: %16llx  BE_data_dma: %16llx   %s\n", dt->this_dma, dt->BE_data_dma, txt);
+	PRINTKW("REQ::  DT dma: %16llx  BE_data_dma: %16llx   %s\n", dt->this_dma, dt->BE_data_dma, txt);
+			
+	if (!list_empty(&req->DT_hra1)) { 
+		if (req->status & REQ_STATUS_HRA1_RECD) 
+			txt = "RX"; 
+		else 
+			txt = "NOT RX"; 
+
+		dt = (struct dt_t *)list_entry(req->DT_hra1.next, struct dt_t, list_head);
+		//PDEBUG(1, "HRA1:: DT dma: %16llx  BE_data_dma: %16llx   %s\n", dt->this_dma, dt->BE_data_dma, txt);
+		PRINTKW("HRA1:: DT dma: %16llx  BE_data_dma: %16llx   %s\n", dt->this_dma, dt->BE_data_dma, txt);
+	}
+	else {
+		//PDEBUG(1, "No HRA1\n");
+		PRINTKW("No HRA1\n");
+	}
+
+	if (!list_empty(&req->DT_hra2)) { 
+		if (req->status & REQ_STATUS_HRA2_RECD) 
+			txt = "RX"; 
+		else 
+			txt = "NOT RX"; 
+
+		dt = (struct dt_t *)list_entry(req->DT_hra2.next, struct dt_t, list_head);
+		//PDEBUG(1, "HRA2:: DT dma: %16llx  BE_data_dma: %16llx   %s\n", dt->this_dma, dt->BE_data_dma, txt);
+		PRINTKW("HRA2:: DT dma: %16llx  BE_data_dma: %16llx   %s\n", dt->this_dma, dt->BE_data_dma, txt);
+	}
+	else {
+		//PDEBUG(1, "No HRA2\n");
+		PRINTKW("No HRA2\n");
+	}
+#endif // DEBUG
+
+	req->retcode = code;
+	if (atomic_read(&req->waitEvent) == 0) {
+		// async requests do not sleep
+		atomic_set(&req->waitEvent, 1); 
+		wake_up(&req->waitQ);
+	}
+}
+
+
+void 
+flush_fp_request_queue(ibm4767_ctx_t *ctx, int code)
+{
+	struct list_head *pLH, *next;
+	request_wait_t *rw;
+
+        int i;
+
+	PRINTKW("Device %d: Flushing all FP requests\n", ctx->dev_index);
+
+	spin_lock_bh(&ctx->req_pool_lock);
+        for (i=0; i < REQ_POOL_COUNT; i++) {
+                request_t *req = &ctx->req_array[i];
+                
+		if (req->active == REQUEST_FP)
+			flush_request(req, code);
+	}
+
+	if (!list_empty(&ctx->req_wait_list)) {
+		pLH = ctx->req_wait_list.next;
+		do {
+			rw = list_entry(pLH, request_wait_t, list_head);
+
+			if (rw->type == REQUEST_FP) {
+				next = pLH->next;
+				list_del(pLH);
+				rw->rc = code;
+				rw->pReq = NULL;
+				atomic_set(&rw->waitEvent, 1);
+				wake_up(&rw->waitQ);
+
+				pLH = next;
+			}
+			else {
+				pLH = pLH->next;
+			}
+		} while (pLH != &ctx->req_wait_list);
+	}
+
+	spin_unlock_bh(&ctx->req_pool_lock);
+}
+
+
+void 
+flush_mcpu_request_queue(ibm4767_ctx_t *ctx, int code)
+{
+	struct list_head *pLH, *next;
+	request_wait_t *rw;
+        int i;
+
+	PRINTKW("Device %d: Flushing all MCPU requests\n", ctx->dev_index);
+
+	spin_lock_bh(&ctx->req_pool_lock);
+        for (i=0; i < REQ_POOL_COUNT; i++) {
+                request_t *req = &ctx->req_array[i];
+                
+		if (req->active == REQUEST_MCPU)
+			flush_request(req, code);
+	}
+	
+	if (!list_empty(&ctx->req_wait_list)) {
+		pLH = ctx->req_wait_list.next;
+		do {
+			rw = list_entry(pLH, request_wait_t, list_head);
+
+			if (rw->type == REQUEST_MCPU) {
+				next = pLH->next;
+				list_del(pLH);
+				rw->rc = code;
+				rw->pReq = NULL;
+				atomic_set(&rw->waitEvent, 1);
+				wake_up(&rw->waitQ);
+
+				pLH = next;
+			}
+			else {
+				pLH = pLH->next;
+			}
+		} while (pLH != &ctx->req_wait_list);
+	}
+
+	spin_unlock_bh(&ctx->req_pool_lock);
+}
+
+
+void 
+flush_ssp_request_queue(ibm4767_ctx_t *ctx, int code)
+{
+	struct list_head *pLH, *next;
+	request_wait_t *rw;
+	int i;
+
+	// in production mode, we can get by with checking ctx->mb_req
+	// but in mfg mode, there may be multiple outstanding SSP requests
+	//
+	spin_lock_bh(&ctx->req_pool_lock);
+        for (i=0; i < REQ_POOL_COUNT; i++) {
+                request_t *req = &ctx->req_array[i];
+                
+		if (req->active == REQUEST_SSP)
+			flush_request(req, code);
+	}
+	
+	if (!list_empty(&ctx->req_wait_list)) {
+		pLH = ctx->req_wait_list.next;
+		do {
+			rw = list_entry(pLH, request_wait_t, list_head);
+
+			if (rw->type == REQUEST_SSP) {
+				next = pLH->next;
+				list_del(pLH);
+				rw->rc = code;
+				rw->pReq = NULL;
+				atomic_set(&rw->waitEvent, 1);
+				wake_up(&rw->waitQ);
+
+				pLH = next;
+			}
+			else {
+				pLH = pLH->next;
+			}
+		} while (pLH != &ctx->req_wait_list);
+	}
+	spin_unlock_bh(&ctx->req_pool_lock);
+}
+
+
+void
+flush_all_requests(ibm4767_ctx_t *ctx, int code)
+{
+	uint32_t i;
+
+	spin_lock_bh(&ctx->req_pool_lock);
+	for (i=0; i < REQ_POOL_COUNT; i++) {
+		request_t *req = &ctx->req_array[i];
+		
+		if (req->active)
+			flush_request(req, code);
+	}
+
+	while (!list_empty(&ctx->req_wait_list)) {
+		struct list_head *pLH = list_remove_head(&ctx->req_wait_list);
+		request_wait_t *rw = list_entry(pLH, request_wait_t, list_head);
+
+		rw->rc = code;
+		rw->pReq = NULL;
+		atomic_set(&rw->waitEvent, 1);
+		wake_up(&rw->waitQ);
+	}
+
+	spin_unlock_bh(&ctx->req_pool_lock);
+}
+
+
+
+void
+request_abort_by_requestid(ibm4767_ctx_t *ctx, uint16_t requestid, int code)
+{
+	request_t *req = NULL;
+
+	if (requestid >= REQ_POOL_COUNT)
+		return;
+
+	spin_lock_bh(&ctx->req_pool_lock);
+
+	req = &ctx->req_array[requestid];
+	if (req->active)
+		flush_request(req, code);
+	
+	spin_unlock_bh(&ctx->req_pool_lock);
+}
+
+
+// the request fuzzer causes the driver to lie about big the reply buffers are.  this was originally
+// a quick hack to help the firmware guys track down some comm mgr problems.  it was sufficiently
+// useful that they asked that I make it a permanent part of the code...
+//
+// a true fuzzer doesn't belong in a DD since it's too easy to cause a hardware DMA
+// error and a DD can't know a priori what effect a fuzzed request might have on the
+// reply's contents or on state information maintained by card firmware or how the host app might
+// be affected.  so we limit our scope to modifying HRA lengths.  
+//
+// still, use with caution.  you've been warned.
+//
+// we only fuzz HRA #1 since that's sufficient to test the firmware...
+//
+int
+request_fuzzer(ibm4767_ctx_t *ctx, uint32_t *hra1_len)
+{
+	uint32_t count = atomic_read(&ctx->mcpu_counter);
+	static uint8_t active = 0;
+
+	// when fuzzing is enabled, the fuzzer toggles on/off every 1M requests
+
+	if (0 == (count & 0x00100000)) {
+		if (!active) {
+			active = 1;
+			PRINTKW("Device %d:  Fuzzer active for next 1M requests\n", ctx->dev_index);
+		}
+
+		// when the fuzzer is active, every 4th HRA #1 will have its length overwritten.  the
+		// value used cycles between 0x30/0x18 or 0x38/0x28 every 512K ops...
+		//
+		if (count & 0x80000) {
+			if (0 == (count & 0x3)) {
+				if (0 == (count & 0x7))
+					*hra1_len = 0x18;
+				else
+					*hra1_len = 0x30;
+
+				PDEBUG(3, "Device %d: fuzzing request HRA to 0x%02x\n", ctx->dev_index, *hra1_len);
+				return TRUE;
+			}
+		}
+		else {
+			if (0 == (count & 0x3)) {
+				if (0 == (count & 0x7))
+					*hra1_len = 0x28;
+				else
+					*hra1_len = 0x30;
+
+				PDEBUG(3, "Device %d: fuzzing request HRA to 0x%02x\n", ctx->dev_index, *hra1_len);
+				return TRUE;
+			}
+		}
+	}
+
+	return FALSE;
+}
+
+
diff --git drivers/misc/ibm4767/special.c drivers/misc/ibm4767/special.c
new file mode 100755
index 000000000000..bf4876166281
--- /dev/null
+++ drivers/misc/ibm4767/special.c
@@ -0,0 +1,1810 @@
+/*************************************************************************
+ *  Filename:special.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Routines for special requests
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0))
+#include <linux/signal.h>
+#else
+#include <linux/sched/signal.h>
+#endif
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_ioctl.h"
+#include "y_mailbox.h"
+#include "xc_host.h"   // FIXME - for xcHdwVer_t and xcHdwTmpr_t
+#include "driver.h"
+#include "y_funcs.h"
+#include "host_err.h"
+
+
+//
+//
+int
+special_request_mb(ibm4767_ctx_t *ctx, xcRB_t *pRB)
+{
+	request_t      *pReq = NULL;
+	uint32_t        repl_bytes = 0;
+	int             rc = HOST_DD_Good;
+	int             unsafe = FALSE;
+	request_wait_t  rwt;
+
+
+	PDEBUG(2, "Enter %s...\n", __FUNCTION__);
+
+	spin_lock_bh(&ctx->counter_lock);
+	if (ctx->active_mb) {
+		spin_unlock_bh(&ctx->counter_lock);
+		PRINTKW("Device %d: SPECIAL MB REQUEST already active\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+	ctx->active_mb = 1;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	del_timer_sync(&ctx->cdu_timeout_timer);
+	del_timer_sync(&ctx->agentid_list_timer);
+
+	switch (ctx->status_ssp) {
+		case SSP_OFFLINE:
+			// we'll allow a MB request to wake an OFFLINE SSP...
+			break;
+
+		case SSP_TAMPER:
+			PRINTKW("Device %d: SPECIAL MB REQUEST when SSP TAMPER\n", ctx->dev_index);
+			rc = HOST_DD_TamperDetected;
+			break;
+
+		case SSP_SOFT_TAMPER:
+			PRINTKW("Device %d: SPECIAL MB REQUEST when SSP SOFT TAMPER\n", ctx->dev_index);
+			rc = HOST_DD_TamperDetected;
+			break;
+
+		case SSP_TEMP_SHUTDOWN:
+			PRINTKW("Device %d: SPECIAL MB REQUEST when SSP TEMP SHUTDOWN\n", ctx->dev_index);
+			//return -ESHUTDOWN;
+			rc = HOST_DD_Temperature;
+			break;
+
+		default:
+			break;  // satisfy the compiler
+	}
+
+	if (rc != HOST_DD_Good)
+		goto done;
+
+#if defined(DEBUG)
+	switch (pRB->UserDefined) {
+		case CMD_MB0_CONTINUE:          PDEBUG(2, "MB REQUEST: CONTINUE\n"); break;
+		case CMD_MB0_IBMBURN:           PDEBUG(2, "MB REQUEST: IBMBURN\n"); break;
+		case CMD_MB0_RQ_STATUS:         PDEBUG(2, "MB REQUEST: RQ_STATUS\n"); break;
+		case CMD_MB0_RQ_STATUS_NORESET: PDEBUG(2, "MB REQUEST: RQ_STATUS_NORESET\n"); break;
+
+		case CMD_MB1_IBM_INITIALIZE:    PDEBUG(2, "MB REQUEST: IBM_INITIALIZE\n"); break;
+		case CMD_MB1_RECERTIFY:         PDEBUG(2, "MB REQUEST: RECERTIFY\n"); break;
+		case CMD_MB1_REMBURN1:          PDEBUG(2, "MB REQUEST: REMBURN1\n"); break;
+		case CMD_MB1_ESTOWN2:           PDEBUG(2, "MB REQUEST: ESTOWN2\n"); break;
+		case CMD_MB1_REMBURN2:          PDEBUG(2, "MB REQUEST: REMBURN2\n"); break;
+		case CMD_MB1_EMBURN2:           PDEBUG(2, "MB REQUEST: EMBURN2\n"); break;
+		case CMD_MB1_SUROWN2:           PDEBUG(2, "MB REQUEST: SUROWN2\n"); break;
+		case CMD_MB1_ESTOWN3:           PDEBUG(2, "MB REQUEST: ESTOWN3\n"); break;
+		case CMD_MB1_REMBURN3:          PDEBUG(2, "MB REQUEST: REMBURN3\n"); break;
+		case CMD_MB1_EMBURN3:           PDEBUG(2, "MB REQUEST: EMBURN3\n"); break;
+		case CMD_MB1_SUROWN3:           PDEBUG(2, "MB REQUEST: SUROWN3\n"); break;
+		case CMD_MB1_SELF_TAMPER:       PDEBUG(2, "MB REQUEST: SELF_TAMPER\n"); break;
+		case CMD_MB1_S1Q_HEALTH:        PDEBUG(2, "MB REQUEST: S1Q_HEALTH\n"); break;
+		case CMD_MB1_S1Q_CERTS:         PDEBUG(2, "MB REQUEST: S1Q_CERTS\n"); break;
+		case CMD_MB1_S1Q_FIRMWARE:      PDEBUG(2, "MB REQUEST: S1Q_FIRMWARE\n"); break;
+		case CMD_MB1_S1Q_FIPS_TEST:     PDEBUG(2, "MB REQUEST: S1Q_FIPS_TEST\n"); break;
+
+		default: PDEBUG(2, "MB REQUEST: Other (0x%x)\n", pRB->UserDefined);
+	}
+#endif
+
+	switch (pRB->UserDefined) {
+		case CMD_MB0_CONTINUE:
+		case CMD_MB0_RQ_STATUS_NORESET:
+		case CMD_MB0_RQ_S0_HASH:
+		case CMD_MB1_REMBURN3:
+		case CMD_MB1_S1Q_FIRMWARE:
+			// these commands do not affect the MCPU so they're safe to issue
+			// concurrently with Seg2/Seg3 requests
+			unsafe = FALSE;
+			break;
+
+		default:
+			spin_lock_bh(&ctx->counter_lock);
+
+			if (ctx->active_opens > 1) {
+				spin_unlock_bh(&ctx->counter_lock);
+				PRINTK("Device %d: This MB REQUEST requires exclusive access to the adapter\n", ctx->dev_index);
+				atomic_inc(&ctx->ebusy_counter);
+				rc = HOST_DD_DeviceBusy;
+				goto done;
+			}
+			else {
+				ctx->status_flags |= FLAG_LOCKED;
+				unsafe = TRUE;
+			}
+
+			spin_unlock_bh(&ctx->counter_lock);
+	}
+
+	ibm4767_request_throttle(ctx);
+	
+	rc = request_allocate(ctx, REQUEST_SSP, &rwt); 
+	if (rc != HOST_DD_Good)
+		goto done;
+	ctx->mb_req = pReq = rwt.pReq;
+
+	atomic_inc(&ctx->ssp_counter);
+
+	//
+	// from this point on, we have to clean up the request before leaving
+	//
+
+	// ensure the card is in the correct state.  these routines can sleep.
+	//
+	if (pRB->AgentID == XC_AGENTID_MB0) {
+		// MB0 on 4768 doesn't support any DMA commands so don't try to send one
+		if (ctx->pci_dev->device == IBM4767_DEVID_4768) {
+			PRINTKW("Device %d: Attempt to send MB0 DMA request to 4768!\n", ctx->dev_index);
+			rc = HOST_DD_BadRequest;
+			goto done;
+		} else {
+			rc = special_request_mb0_pre(ctx, pReq);
+		}
+	}
+	else {
+		rc = special_request_mb1_pre(ctx, pReq);
+	}
+
+	if (rc != HOST_DD_Good) {
+		if (pReq->retcode) {
+			pRB->ReplyControlBlkLength = 0;
+			pRB->Status = pReq->retcode;
+			PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+			rc = HOST_DD_Good;
+		}
+		goto done;
+	}
+
+	if (signal_pending(current)) {
+		const char *txt = ssp_state_to_str(ctx->status_ssp);
+
+		PRINTKW("Device %d: %s:%d woke up due to signal...aborting...\n", ctx->dev_index, __FUNCTION__, __LINE__);
+		if (txt) 
+			PRINTKW("Device %d: present SSP state: %s\n", ctx->dev_index, txt);
+		else
+			PRINTKW("Device %d: present SSP state: UNKNOWN (%d)\n", ctx->dev_index, ctx->status_ssp);
+
+		rc = HOST_DD_Interrupted;
+		goto done;
+	}
+
+	// did MB halt unexpectedly while we were waiting?  this condition *should* get caught
+	// by the _pre() routine above but for sanity, we check it again here...
+	//
+	if (ctx->status_ssp == SSP_OFFLINE) {
+		// here, the ioctl succeeds (rc = 0) but we report the unexpected
+		// halt as an error via xcRB_t::Status
+		pRB->ReplyControlBlkLength = 0;
+		pRB->Status = pReq->retcode;  // this will already be byte-reversed
+		PDEBUG(1, "%s:%d Setting pRB->Status = %08x\n", __FUNCTION__, __LINE__, pRB->Status);
+		rc = HOST_DD_Good;
+		goto done;
+	}
+
+	// at this point, the card is in the correct state.  now we need to assemble
+	// the request and dispatch to the SSP
+	//
+	rc = special_request_assemble(ctx, pReq, pRB, TRUE);
+	if (rc != HOST_DD_Good)
+		goto done;
+
+	if (pRB->UserDefined == CMD_MB1_REMBURN3) {
+		uint32_t hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+
+		// there are 3 possibilities:
+		// 1) The CDU daemon is running and a CDU should take place
+		// 2) The MCPU held in RST or is blocked in POST2 so CDU can't take place.  No user action
+		//    is required.  When the SSP resets following leaving miniboot mode, it will wake the
+		//    MCPU and tell POST2 to jump to Seg2 where it will "see" the new image...
+		// 3) The MCPU is active but the CDU daemon is not running.  The driver cannot safely
+		//    automaticaly reset the MCPU to activate the new embedded app.  The user must reset the 
+		//    card to activate...
+		// 
+		// Mark M assures me that the SEG3_CMPL bit will only be set by the CDU daemon...  
+		//
+		if (hrcsr & HRCSR_SEG3_CMPL) { 
+			PRINTKW("Device %d: HRCSR::SEG3_CMPL --> CDU\n", ctx->dev_index); 
+			ctx->status_cdu = CDU_REMBURN3_CDU; 
+		}
+		else { 
+			if (hrcsr & HRCSR_MCPU_RstS) { 
+				PRINTKW("Device %d: HRCSR::MCPU_RST --> NO CDU\n", ctx->dev_index);
+				ctx->status_cdu = CDU_REMBURN3_NOCDU_AUTORESET; 
+			} 
+			else if (ctx->status_mcpu != MCPU_ACTIVE) {
+				PRINTKW("Device %d: !MCPU_ACT --> NO CDU\n", ctx->dev_index);
+				ctx->status_cdu = CDU_REMBURN3_NOCDU_NEED_USER_RESET; 
+			} 
+			else {
+				PRINTKW("Device %d: MCPU_ACT --> NO CDU\n", ctx->dev_index);
+				ctx->status_cdu = CDU_REMBURN3_NOCDU_NEED_USER_RESET; 
+			} 
+		} 
+	}
+
+	rc = dma_send_request_SSP(ctx, pReq);
+	if (rc != HOST_DD_Good)
+		goto done;
+
+	pReq->timeout = timeout_mb * HZ;
+	if (wait_event_timeout(pReq->waitQ, atomic_read(&pReq->waitEvent), pReq->timeout) == 0) {
+		PRINTKE("Device %d: TIMEOUT on MB REQUEST!  Marking SSP offline\n", ctx->dev_index);
+		mark_ssp_offline(ctx, HOSTTimeout);
+		rc = HOST_DD_Timeout;
+		goto done;
+	}
+	else {
+		PDEBUG(2, "Device %d: MB REQUEST completed!\n", ctx->dev_index);
+		rc = HOST_DD_Good;
+	}
+	atomic_set(&pReq->waitEvent, 0);
+
+	// as above, an out-of-band event may have caused us to awaken...
+	// 
+	if (pReq->retcode) {
+		pRB->ReplyControlBlkLength = 0;
+		pRB->Status = pReq->retcode;
+		PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+		rc = HOST_DD_Good;
+		goto done;
+	}
+
+	// unlike other special requests, we have to do additional work for MB requests
+	//
+	if (ctx->status_ssp < SSP_OFFLINE) {
+		switch (pRB->UserDefined) {
+			case CMD_MB0_CONTINUE:
+			case CMD_MB0_RQ_STATUS:
+			case CMD_MB0_RQ_STATUS_NORESET:
+			case CMD_MB0_RQ_S0_HASH:
+			case CMD_MB1_S1Q_HEALTH:
+			case CMD_MB1_S1Q_CERTS:
+			case CMD_MB1_S1Q_FIRMWARE:
+			case CMD_MB1_S1Q_FIPS_TEST:
+				PDEBUG(2, "Device %d: Continuing...no HALT expected\n", ctx->dev_index);
+				break;
+
+			case CMD_MB1_IBM_INITIALIZE:
+				ctx->mb_round++;
+
+				if ((ctx->mb_round % 2) != 0) {
+					PDEBUG(2, "Device %d: Round #1...continuing...\n", ctx->dev_index);
+				}
+				else {
+					PDEBUG(2, "Device %d: Round #2..>waiting for HALT\n", ctx->dev_index);
+					wait_event(pReq->waitQ, atomic_read(&pReq->waitEvent));
+					atomic_set(&pReq->waitEvent, 0);
+				}
+				break;
+
+			// there are no 3-pass commands anymore..
+			//
+
+			// anything else must result in an SSP HALT...
+			//
+			case CMD_MB1_REMBURN3:
+				if (ctx->status_ssp == SSP_OFFLINE) {
+					PDEBUG(2, "Device %d: SSP already HALTed (is the system very busy?)...\n", ctx->dev_index);
+				}
+				else {
+					PDEBUG(2, "Device %d: Waiting for HALT...\n", ctx->dev_index);
+					wait_event(pReq->waitQ, atomic_read(&pReq->waitEvent));
+					atomic_set(&pReq->waitEvent, 0);
+				}
+
+				// presently MB does not use a dedicated HALT notification (eventually it will).  so
+				// it uses NOTIFICATION_FAIL which sets SSP_OFFLINE.  normally that's okay but here we
+				// really ought to check for !SSP_OFFLINE before proceeding with CDU.  
+				//
+				if (!pReq->retcode) {
+					switch (ctx->status_cdu) {
+						case CDU_REMBURN3_CDU:
+							if (ctx->status_mcpu == MCPU_ACTIVE) {
+								PDEBUG(2, "Device %d: Sending PROCEED_TO_CDU...\n", ctx->dev_index);
+								ctx->status_cdu = CDU_WAIT_FOR_READY_TO_PROCEED;
+								mod_timer(&ctx->cdu_timeout_timer, jiffies + HZ*timeout_cdu);
+								write32(cpu_to_be32(PROCEED_TO_CDU), H2M_MBX_H(ctx));
+
+								// wait until the CDU operation finishes
+								//
+								wait_event(pReq->waitQ, atomic_read(&pReq->waitEvent));
+								atomic_set(&pReq->waitEvent, 0);
+							}
+							else {
+								// MCPU is offline so CDU handshake would timeout...
+								PRINTKW("Device %d: REMBURN3 completed but MCPU is no longer active."
+									"CDU handshake cannot commence.\n", ctx->dev_index);
+								PRINTKW("Device %d: New SEG3 image will activate following the next device reset.\n", ctx->dev_index);
+								pReq->retcode = CDU_ERR_MCPU_FAIL;
+							}
+
+							del_timer_sync(&ctx->cdu_timeout_timer);
+							del_timer_sync(&ctx->agentid_list_timer);
+							ctx->status_cdu = CDU_INACTIVE;
+							break;
+
+						case CDU_REMBURN3_NOCDU_AUTORESET:
+							ctx->status_cdu = CDU_INACTIVE;
+							break;
+
+						case CDU_REMBURN3_NOCDU_NEED_USER_RESET:
+							pReq->retcode = CDU_ERR_NOT_CDUABLE;
+							ctx->status_cdu = CDU_INACTIVE;
+
+						default: break;
+					}
+				}
+				break;
+
+			default:
+				PDEBUG(2, "Device %d: Waiting for HALT...\n", ctx->dev_index);
+				wait_event(pReq->waitQ, atomic_read(&pReq->waitEvent));
+				atomic_set(&pReq->waitEvent, 0);
+		}
+	}
+	
+	// as above, an out-of-band event may have caused us to awaken...
+	// 
+	if (pReq->retcode) {
+		pRB->ReplyControlBlkLength = 0;
+		pRB->Status = pReq->retcode;  
+		PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+		rc = HOST_DD_Good;
+		goto done;
+	}
+	
+	if (pRB->ReplyControlBlkAddr != NULL) {
+		special_repl_header_t *hdr;
+		scatter_frag_t        *sf;
+		struct list_head    *pLH;
+		char                *src, *dest;
+		uint32_t             i, sz;
+		
+		// assume the command succeeded.  note that a HALT notification may be pending
+		// but that doesn't affect us here
+		//
+		i = 0;
+		dest = pRB->ReplyControlBlkAddr;
+
+		list_for_each(pLH, &pReq->hra1_slist.frags) {
+			sf   = list_entry(pLH, scatter_frag_t, list_head);
+			dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+
+			if (i++ == 0) {
+				hdr = (special_repl_header_t *)sf->vp.va;
+		
+				repl_bytes = cpu_to_be32(hdr->len);
+				if (repl_bytes > pRB->ReplyControlBlkLength) { 
+					PRINTKE("Device %d: specified reply buffer too small (%d bytes vs %d bytes)", 
+							ctx->dev_index, pRB->ReplyControlBlkLength, repl_bytes);
+			
+					pRB->ReplyControlBlkLength = repl_bytes; 
+					rc = HOST_DD_BufferTooSmall; 
+					goto done; 
+				} 
+				else { 
+#if defined(DEBUG)
+					hex_dump(ctx, 1, "special_repl_header_t contents", hdr, sizeof (special_repl_header_t));
+#endif
+					rc = HOST_DD_Good; 
+					pRB->ReplyControlBlkLength = repl_bytes; 
+					pRB->Status                = cpu_to_be32(hdr->status);
+					PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+				}
+
+				src = sf->vp.va + sizeof(special_repl_header_t);
+				sz  = sf->len - sizeof(special_repl_header_t);
+			}
+			else {
+				src = sf->vp.va;
+				sz  = sf->len;
+			}
+			
+			sz = MIN(sz, repl_bytes);
+
+			if (copy_to_user(dest, src, sz)) {
+				PRINTKW("Device %d: copy_to_user failed\n", ctx->dev_index);
+				rc = HOST_DD_BadAddress;
+				goto done;
+			}
+
+			dest += sz;
+			repl_bytes -= sz;
+			if (repl_bytes == 0)
+				break;
+		}
+	}
+
+done:
+	if (pReq) {
+		free_scatterlist(&pReq->hdr_slist);
+		free_scatterlist(&pReq->req_slist);
+		free_scatterlist(&pReq->hra1_slist);
+
+		dt_free_chain(ctx, &pReq->DT_req);
+		dt_free_chain(ctx, &pReq->DT_hra1);
+
+		ctx->mb_req = NULL;
+		request_release(ctx, pReq);
+	}
+
+	if (rc != HOST_DD_Good) {
+		// if we've already set pRB->Status, don't overwrite it!
+		if (!pRB->Status)
+			pRB->Status = rc;
+		PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+	}
+
+	msleep(5000);
+
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->active_mb = 0;
+
+	// in the 4767 universe, miniboot cannot safely drop the MCPU into reset.  instead
+	// it drops the MCPU into debug mode.  unfortunately, this means that the driver
+	// cannot rely on HRCSR::MCPU_RstS to know what the MCPU is doing so we try to
+	// keep track of state ourselves...
+	//
+	if (unsafe == TRUE)
+		ctx->status_mcpu = MCPU_IN_RESET;
+
+	ctx->status_flags &= ~FLAG_LOCKED;
+
+	spin_unlock_bh(&ctx->counter_lock);
+
+	//PDEBUG(1, "All DTs should be free now...\n");
+	//dump_chain(ctx, &ctx->dt_pool, FALSE);
+
+	return rc;
+}
+
+
+#if defined(ENABLE_MFG)
+int
+special_request_ssp(ibm4767_ctx_t *ctx, xcRB_t *pRB)
+{
+	request_t     *pReq = NULL;
+	char          *err_txt = NULL;
+	uint32_t       repl_bytes = 0;
+	request_wait_t rwt;
+	int            rc = HOST_DD_Good;
+
+	PDEBUG(2, "Enter %s...\n", __FUNCTION__);
+
+	spin_lock_bh(&ctx->counter_lock);
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST0:
+			PRINTKW("Device %d: Error.  %s invoked with XC_AGENTID_POST0!?\n", ctx->dev_index, __FUNCTION__);
+			return HOST_DD_InvalidParm;
+		case XC_AGENTID_POST1:
+			if (ctx->active_p1) 
+				err_txt = "POST1";
+			else 
+				ctx->active_p1++;
+			break;
+		case XC_AGENTID_IBM_TAS_SSP:
+			if (ctx->active_ssp_tas) 
+				err_txt = "TAS";
+			else 
+				ctx->active_ssp_tas++;
+			break;
+		case XC_AGENTID_IBM_TAPP_SSP:
+			if (ctx->active_ssp_tapp) 
+				err_txt = "TAPP";
+			else 
+				ctx->active_ssp_tapp++;
+			break;
+		default:
+			PRINTKW("Device %d: Error.  %s invoked with AgentID 0x%08x!\n", ctx->dev_index, __FUNCTION__, pRB->AgentID);
+			return HOST_DD_InvalidParm;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+	
+	if (err_txt) {
+		PRINTKW("Device %d: SPECIAL SSP %s REQUEST already active\n", ctx->dev_index, err_txt);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	switch (ctx->status_ssp) {
+		case SSP_OFFLINE:
+			PRINTKW("Device %d: SPECIAL SSP REQUEST when SSP OFFLINE\n", ctx->dev_index);
+			rc = HOST_DD_NoDevice;
+			goto done;
+
+		case SSP_TAMPER:
+			PRINTKW("Device %d: SPECIAL SSP REQUEST when SSP TAMPER\n", ctx->dev_index);
+			rc = HOST_DD_TamperDetected;
+			goto done;
+
+		case SSP_SOFT_TAMPER:
+			PRINTKW("Device %d: SPECIAL SSP REQUEST when SSP SOFT TAMPER\n", ctx->dev_index);
+			rc = HOST_DD_TamperDetected;
+			goto done;
+
+		case SSP_TEMP_SHUTDOWN:
+			PRINTKW("Device %d: SPECIAL SSP REQUEST when SSP TEMP SHUTDOWN\n", ctx->dev_index);
+			rc = HOST_DD_Temperature;
+			goto done;
+
+		default:
+			break;  // satisfy the compiler
+	}
+
+	ibm4767_request_throttle(ctx);
+
+	rc = request_allocate(ctx, REQUEST_SSP, &rwt); 
+	if (rc != HOST_DD_Good)
+		goto done;
+	pReq = rwt.pReq;
+
+	atomic_inc(&ctx->ssp_counter);
+
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST1:
+			ctx->p1_req = pReq;
+			rc = special_request_post1_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAS_SSP:
+			ctx->ssp_tas_req = pReq;
+			rc = special_request_ssp_tas_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAPP_SSP:
+			ctx->ssp_tapp_req = pReq;
+			rc = special_request_ssp_tapp_pre(ctx, pReq);
+			break;
+
+		default:
+			rc = HOST_DD_InvalidParm;
+			break;
+	}
+
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: SPECIAL SSP state transition failure\n", ctx->dev_index);
+		if (pReq->retcode) {
+			pRB->ReplyControlBlkLength = 0;
+			pRB->Status = pReq->retcode;
+			PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+			rc = HOST_DD_Good;
+		}
+		goto done;
+	}
+
+	if (signal_pending(current)) {
+		PRINTKW("Device %d: %s:%d woke up due to signal...aborting...\n", ctx->dev_index, __FUNCTION__, __LINE__);
+		rc = HOST_DD_Interrupted;
+		goto done;
+	}
+
+	// at this point, the card is in the correct state.  now we need to assemble
+	// the request and dispatch
+	//
+	rc = special_request_assemble(ctx, pReq, pRB, TRUE);
+	if (rc != HOST_DD_Good)
+		goto done;
+
+	rc = dma_send_request_SSP(ctx, pReq);
+	if (rc != HOST_DD_Good)
+		goto done;
+
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+
+	if (signal_pending(current)) {
+		PRINTKW("Device %d: %s:%d woke up due to signal...aborting...\n", ctx->dev_index, __FUNCTION__, __LINE__);
+		rc = HOST_DD_Interrupted;
+		goto done;
+	}
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		pRB->ReplyControlBlkLength = 0;
+		pRB->Status = pReq->retcode;
+		PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+		rc = HOST_DD_Good;
+		goto done;
+	}
+
+	if (pRB->ReplyControlBlkAddr != NULL) {
+		special_repl_header_t *hdr;
+		scatter_frag_t        *sf;
+		struct list_head    *pLH;
+		char                *src, *dest;
+		uint32_t             i, sz;
+
+		// assume the command succeeded.  note that a HALT notification may be pending
+		// but that doesn't affect us here
+		//
+		i = 0;
+		dest = pRB->ReplyControlBlkAddr;
+
+		list_for_each(pLH, &pReq->hra1_slist.frags) {
+			sf   = list_entry(pLH, scatter_frag_t, list_head);
+			dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+
+			if (i++ == 0) {
+				hdr = (special_repl_header_t *)sf->vp.va;
+		
+				repl_bytes = cpu_to_be32(hdr->len);
+				if (repl_bytes > pRB->ReplyControlBlkLength) { 
+					PRINTKE("Device %d: specified reply buffer too small (%d bytes vs %d bytes)", 
+							ctx->dev_index, pRB->ReplyControlBlkLength, repl_bytes);
+			
+					pRB->ReplyControlBlkLength = repl_bytes; 
+					rc = HOST_DD_BufferTooSmall; 
+					goto done; 
+				} 
+				else { 
+					rc = HOST_DD_Good; 
+					pRB->ReplyControlBlkLength = repl_bytes; 
+					pRB->Status                = cpu_to_be32(hdr->status);
+				}
+
+				src = sf->vp.va + sizeof(special_repl_header_t);
+				sz  = sf->len - sizeof(special_repl_header_t);
+			}
+			else {
+				src = sf->vp.va;
+				sz  = sf->len;
+			}
+			
+			sz = MIN(sz, repl_bytes);
+
+			if (copy_to_user(dest, src, sz)) {
+				PRINTKW("Device %d: copy_to_user failed\n", ctx->dev_index);
+				rc = HOST_DD_BadAddress;
+				goto done;
+			}
+
+			dest += sz;
+			repl_bytes -= sz;
+			if (repl_bytes == 0)
+				break;
+		}
+	}
+
+done:
+	if (pReq) {
+		free_scatterlist(&pReq->hdr_slist);
+		free_scatterlist(&pReq->req_slist);
+		free_scatterlist(&pReq->hra1_slist);
+
+		dt_free_chain(ctx, &pReq->DT_req);
+		dt_free_chain(ctx, &pReq->DT_hra1);
+
+		// we probably ought to verify that the last DT is not freed
+		// until the next request...
+		//
+		request_release(ctx, pReq);
+	}
+
+	if (rc != HOST_DD_Good) {
+		// if we've already set pRB->Status, don't overwrite it!
+		if (!pRB->Status)
+			pRB->Status = rc;
+		PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+	}
+
+	spin_lock_bh(&ctx->counter_lock);
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST1:
+			ctx->active_p1--; 
+			ctx->p1_req = NULL; 
+			break;
+		case XC_AGENTID_IBM_TAS_SSP:  
+			ctx->active_ssp_tas--; 
+			ctx->ssp_tas_req = NULL; 
+			break; 
+		case XC_AGENTID_IBM_TAPP_SSP: 
+			ctx->active_ssp_tapp--; 
+			ctx->ssp_tapp_req = NULL; 
+			break;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+
+	//PDEBUG(1, "All DTs should be free now...\n");
+	//dump_chain(ctx, &ctx->dt_pool, FALSE);
+
+	return rc;
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+int
+special_request_mcpu(ibm4767_ctx_t *ctx, xcRB_t *pRB)
+{
+	request_t      *pReq = NULL;
+	char           *err_txt= NULL;
+	uint32_t        repl_bytes = 0;
+	request_wait_t  rwt;
+	int             rc = HOST_DD_Good;
+
+	PDEBUG(2, "Enter %s...\n", __FUNCTION__);
+
+	spin_lock_bh(&ctx->counter_lock);
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST2:
+			if (ctx->active_p2) 
+				err_txt = "POST2";
+			else 
+				ctx->active_p2++;
+			break;
+		case XC_AGENTID_IBM_TAS_MCPU:
+			if (ctx->active_mcpu_tas) 
+				err_txt = "TAS";
+			else 
+				ctx->active_mcpu_tas++;
+			break;
+		case XC_AGENTID_IBM_TAPP_MCPU:
+			if (ctx->active_mcpu_tapp) 
+				err_txt = "TAPP";
+			else 
+				ctx->active_mcpu_tapp++;
+			break;
+		default:
+			PRINTKW("Device %d: %s invoked with AgentID 0x%08x!\n", ctx->dev_index, __FUNCTION__, pRB->AgentID);
+			return HOST_DD_InvalidParm;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+	
+	if (err_txt) {
+		PRINTKW("Device %d: SPECIAL MCPU %s request_t already active\n", ctx->dev_index, err_txt);
+		atomic_inc(&ctx->ebusy_counter);
+		rc = HOST_DD_DeviceBusy;
+		goto done;
+	}
+
+	switch (ctx->status_mcpu) {
+		case MCPU_UNINITIALIZED:
+			PRINTKW("Device %d: SPECIAL MCPU %s REQUEST failed (MCPU_UNINITIALIZED)\n", ctx->dev_index, err_txt);
+			rc = HOST_DD_NoDevice;
+			goto done;
+
+		case MCPU_INITIALIZED:
+			PRINTKW("Device %d: SPECIAL MCPU %s REQUEST failed (MCPU_INITIALIZED)\n", ctx->dev_index, err_txt);
+			atomic_inc(&ctx->ebusy_counter);
+			rc = HOST_DD_DeviceBusy;
+			goto done;
+
+		case MCPU_ACTIVE:
+			PRINTKW("Device %d: SPECIAL MCPU %s REQUEST failed (MCPU_ACTIVE)\n", ctx->dev_index, err_txt);
+			atomic_inc(&ctx->ebusy_counter);
+			rc = HOST_DD_DeviceBusy;
+			goto done;
+
+		case MCPU_TAMPER:
+			PRINTKW("Device %d: SPECIAL MCPU %s REQUEST failed (MCPU_TAMPER)\n", ctx->dev_index, err_txt);
+			rc = HOST_DD_TamperDetected;
+			goto done;
+
+		case MCPU_SOFT_TAMPER:
+			PRINTKW("Device %d: SPECIAL MCPU %s REQUEST failed (MCPU_SOFT_TAMPER)\n", ctx->dev_index, err_txt);
+			rc = HOST_DD_TamperDetected;
+			goto done;
+
+		case MCPU_TEMP_SHUTDOWN:
+			PRINTKW("Device %d: SPECIAL MCPU %s REQUEST failed (MCPU_TEMP_SHUTDOWN)\n", ctx->dev_index, err_txt);
+			rc = HOST_DD_Temperature;
+			goto done;
+
+		case MCPU_OFFLINE:
+			PRINTKW("Device %d: SPECIAL MCPU %s REQUEST failed (MCPU_OFFLINE)\n", ctx->dev_index, err_txt);
+			rc = HOST_DD_NoDevice;
+			goto done;
+
+		default:
+			break;  // satisfy the compiler
+	}
+
+	ibm4767_request_throttle(ctx);
+
+	rc = request_allocate(ctx, REQUEST_MCPU, &rwt); 
+	if (rc != HOST_DD_Good)
+		goto done;
+	pReq = rwt.pReq;
+
+	atomic_inc(&ctx->mcpu_counter);
+
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST2:
+			ctx->p2_req = pReq;
+			rc = special_request_post2_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAS_MCPU:
+			ctx->mcpu_tas_req = pReq;
+			rc = special_request_mcpu_tas_pre(ctx, pReq);
+			break;
+
+		case XC_AGENTID_IBM_TAPP_MCPU:
+			ctx->mcpu_tapp_req = pReq;
+			rc = special_request_mcpu_tapp_pre(ctx, pReq);
+			break;
+
+		default:
+			rc = HOST_DD_InvalidParm;
+			break;
+	}
+	
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: SPECIAL MCPU state transition failure\n", ctx->dev_index);
+		if (pReq->retcode) {
+			pRB->ReplyControlBlkLength = 0;
+			pRB->Status = pReq->retcode;
+			PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+			rc = HOST_DD_Good;
+		}
+		goto done;
+	}
+
+	if (signal_pending(current)) {
+		PRINTKW("Device %d: %s:%d woke up due to signal...aborting...\n", ctx->dev_index, __FUNCTION__, __LINE__);
+		rc = HOST_DD_Interrupted;
+		goto done;
+	}
+
+	// at this point, the card is in the correct state.  now we need to assemble
+	// the request and dispatch
+	//
+	rc = special_request_assemble(ctx, pReq, pRB, FALSE);
+	if (rc != HOST_DD_Good)
+		goto done;
+
+	rc = dma_send_request_MCPU(ctx, pReq);
+	if (rc != HOST_DD_Good)
+		goto done;
+
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+
+	if (signal_pending(current)) {
+		PRINTKW("Device %d: %s:%d woke up due to signal...aborting...\n", ctx->dev_index, __FUNCTION__, __LINE__);
+		rc = HOST_DD_Interrupted;
+		goto done;
+	}
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		pRB->ReplyControlBlkLength = 0;
+		pRB->Status = pReq->retcode;
+		rc = HOST_DD_Good;
+		goto done;
+	}
+
+	if (pRB->ReplyControlBlkAddr != NULL) {
+		special_repl_header_t *hdr;
+		scatter_frag_t        *sf;
+		struct list_head    *pLH;
+		char                *src, *dest;
+		uint32_t             i, sz;
+
+		// assume the command succeeded.  note that a HALT notification may be pending
+		// but that doesn't affect us here
+		//
+		i = 0;
+		dest = pRB->ReplyControlBlkAddr;
+
+		list_for_each(pLH, &pReq->hra1_slist.frags) {
+			sf   = list_entry(pLH, scatter_frag_t, list_head);
+			dma_sync_single_for_cpu(ctx->dev, sf->vp.dma, sf->vp.len, DMA_FROM_DEVICE);
+
+			if (i++ == 0) {
+				hdr = (special_repl_header_t *)sf->vp.va;
+		
+				repl_bytes = cpu_to_be32(hdr->len);
+				if (repl_bytes > pRB->ReplyControlBlkLength) { 
+					PRINTKE("Device %d: specified reply buffer too small (%d bytes vs %d bytes)", 
+							ctx->dev_index, pRB->ReplyControlBlkLength, repl_bytes);
+			
+					pRB->ReplyControlBlkLength = repl_bytes; 
+					rc = HOST_DD_BufferTooSmall; 
+					goto done; 
+				} 
+				else { 
+					rc = HOST_DD_Good; 
+					pRB->ReplyControlBlkLength = repl_bytes; 
+					pRB->Status                = cpu_to_be32(hdr->status);
+				}
+
+				src = sf->vp.va + sizeof(special_repl_header_t);
+				sz  = sf->len - sizeof(special_repl_header_t);
+			}
+			else {
+				src = sf->vp.va;
+				sz  = sf->len;
+			}
+			
+			sz = MIN(sz, repl_bytes);
+
+			if (copy_to_user(dest, src, sz)) {
+				PRINTKW("Device %d: copy_to_user failed\n", ctx->dev_index);
+				rc = HOST_DD_BadAddress;
+				goto done;
+			}
+
+			dest += sz;
+			repl_bytes -= sz;
+			if (repl_bytes == 0)
+				break;
+		}
+	}
+
+done:
+	if (pReq) {
+		free_scatterlist(&pReq->hdr_slist);
+		free_scatterlist(&pReq->req_slist);
+		free_scatterlist(&pReq->hra1_slist);
+
+		dt_free_chain(ctx, &pReq->DT_req);
+		dt_free_chain(ctx, &pReq->DT_hra1);
+
+		// we probably ought to verify that the last DT is not freed
+		// until the next request...
+		//
+		request_release(ctx, pReq);
+	}
+
+	if (rc != HOST_DD_Good) {
+		// if we've already set pRB->Status, don't overwrite it!
+		if (!pRB->Status)
+			pRB->Status = rc;
+		PDEBUG(1, "Device %d: %s:%d Setting pRB->Status = %08x\n", ctx->dev_index, __FUNCTION__, __LINE__, pRB->Status);
+	}
+	
+	spin_lock_bh(&ctx->counter_lock);
+	switch (pRB->AgentID) {
+		case XC_AGENTID_POST2: 
+			ctx->active_p2--; 
+			ctx->p2_req = NULL; 
+			break; 
+		case XC_AGENTID_IBM_TAS_MCPU:
+			ctx->active_mcpu_tas--; 
+			ctx->mcpu_tas_req = NULL; 
+			break; 
+		case XC_AGENTID_IBM_TAPP_MCPU: 
+			ctx->active_mcpu_tapp--; 
+			ctx->mcpu_tapp_req = NULL; 
+			break;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+
+	//PDEBUG(1, "All DTs should be free now...\n");
+	//dump_chain(ctx, &ctx->dt_pool, FALSE);
+
+	return rc;
+}
+#endif
+
+
+int
+special_request_assemble(ibm4767_ctx_t *ctx, 
+		                 request_t   *pReq, 
+				 xcRB_t      *pRB,
+				 bool         is_SSP)
+{
+
+	struct dt_t    *pDT;
+	char           *p;
+	pilot_msg_t      *pilot;
+	uint32_t       hra1_len, hra2_len = 0, hrb_len, padded_len, remain;
+	uint16_t       frag_total;
+	int            rc = HOST_DD_Good;
+	
+
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+        PDEBUG(3, "UserDefined:              %08x\n", pRB->UserDefined);
+        PDEBUG(3, "RequestControlBlkLength:  %08x\n", pRB->RequestControlBlkLength);
+        PDEBUG(3, "RequestDataLength:        %08x\n", pRB->RequestDataLength);
+        PDEBUG(3, "ReplyControlBlkLength:    %08x\n", pRB->ReplyControlBlkLength);
+        PDEBUG(3, "ReplyDataLength:          %08x\n", pRB->ReplyDataLength);
+
+	// HRA #1
+	//
+	hra1_len = sizeof(special_repl_header_t);  
+	if (pRB->ReplyControlBlkLength) {
+		hra1_len = pRB->ReplyControlBlkLength + sizeof(special_repl_header_t);
+		hra1_len = (hra1_len + 7) & ~7;
+	}
+
+	rc = alloc_scatterlist(ctx,
+			       NULL, 0,
+			       hra1_len, 
+			       FALSE,
+			       &pReq->hra1_slist,
+			       DMA_FROM_DEVICE);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: alloc_scatterlist hra1 failed\n", ctx->dev_index);
+		return rc;
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_hra1, &pReq->hra1_slist);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist hra1 failed\n", ctx->dev_index);
+		return rc;
+	}
+
+#if defined(DEBUG)
+	//PDEBUG(3, "Device %d: HRA #1 contents:\n", ctx->dev_index);
+	//dump_chain(ctx, &pReq->DT_hra1, TRUE);
+#endif
+	
+	atomic_inc(&pReq->num_hras);
+
+	// HRA #2
+	// at present, no special request will ever have more than one HRA but the
+	// pilot message structure supports two HRAs so we'll play along.  keep in mind, though,
+	// that this is just part-one of a two-part puzzle.  the calling function must 
+	// have code to actually copy HRA#2 to userspace...
+	//
+	if (pRB->ReplyDataAddr) {
+		hra2_len = pRB->ReplyDataLength + sizeof(special_repl_header_t);
+		hra2_len = (hra2_len + 7) & ~7;
+	
+		rc = alloc_scatterlist(ctx, 
+				       NULL, 0, 
+				       hra2_len, 
+				       FALSE, 
+				       &pReq->hra2_slist, 
+				       DMA_FROM_DEVICE); 
+		if (rc != HOST_DD_Good) { 
+			PRINTKW("Device %d: alloc_scatterlist hra2 failed\n", ctx->dev_index); 
+			return rc; 
+		}
+	
+		rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_hra2, &pReq->hra2_slist); 
+		if (rc != HOST_DD_Good) { 
+			PRINTKW("Device %d: dt_append_scatterlist hra2 failed\n", ctx->dev_index); 
+			return rc; 
+		}
+	
+		atomic_inc(&pReq->num_hras);
+
+#if defined(DEBUG)
+		//PDEBUG(3, "Device %d: HRA #2 contents:\n", ctx->dev_index);
+		//dump_chain(ctx, &pReq->DT_hra2, TRUE);
+#endif
+
+	}
+
+	// a special request's chain will look something like:
+	//    pilot_msg_t (with no-TX and no-INT-no-IV bits set in DMA header)
+	//    FRAGMENT #1 
+	//    FRAGMENT #2
+	//    ...
+	//    FRAGMENT #n
+	//
+	// where FRAGMENT chain looks like
+	//    dma_header16_t
+	//    'frag size' bytes of data
+	//
+	//    all but the last fragment has no-INT-no-IV bit set in DMA header.  last
+	//    fragment has int-but-no-IV bit set in DMA header...
+	//
+	
+	padded_len = (sizeof(pilot_msg_t) + 7) & ~7;
+	hrb_len    = (padded_len - sizeof(dma_header8_t));
+	frag_total = (pRB->RequestControlBlkLength + (XC_MIN_FRAG_LEN - 1)) / XC_MIN_FRAG_LEN;
+
+	pilot = (pilot_msg_t *)pReq->header;
+
+	pilot->dma_hdr.hdr8.bytes[0] = (is_SSP ? DMA_HTYPE_H2S : DMA_HTYPE_H2M);
+	pilot->dma_hdr.hdr8.bytes[1] = DMA_RQSTR_HOST | DMA_CW_NORM;
+	pilot->dma_hdr.hdr8.bytes[2] = DMA_CW_NO_TX | DMA_CW_MRB0;
+	pilot->dma_hdr.hdr8.bytes[3] = 0;
+	pilot->dma_hdr.hdr8.bytes[4] = ctx->vfid;
+	// for host->card DMA, the DMA header length is ignored by hardware.
+	//pilot->dma_hdr.hdr8.bytes[5] = (hrb_len & 0x0F0000) >> 16;
+	//pilot->dma_hdr.hdr8.bytes[6] = (hrb_len & 0x00FF00) >>  8;
+	//pilot->dma_hdr.hdr8.bytes[7] = (hrb_len & 0x0000FF);
+	pilot->dma_hdr.sig = cpu_to_be16((is_SSP ? DMA_SIG_H2S : DMA_SIG_H2M));
+
+	pilot->len2     = cpu_to_be16(0x46);  // hard-coded.  do not change
+	pilot->len3     = cpu_to_be16(0x34);  // hard-coded.  do not change 
+	pilot->num_hras = cpu_to_be16(1);
+	pilot->tag_len_hra[0].tag    = 0x1;
+	pilot->tag_len_hra[0].len[0] = (hra1_len & 0x0F0000) >> 16;
+	pilot->tag_len_hra[0].len[1] = (hra1_len & 0x00FF00) >>  8;
+	pilot->tag_len_hra[0].len[2] = (hra1_len & 0x0000FF);
+#if defined(WINDOWS)
+	pDT = CONTAINING_RECORD(pReq->DT_hra1.Flink, struct dt_t, list_entry);
+	pilot->tag_len_hra[0].hra = cpu_to_be64(pDT->this_LA);
+#endif
+#if defined(LINUX)
+	pDT = list_entry(pReq->DT_hra1.next, struct dt_t, list_head);
+	pilot->tag_len_hra[0].hra = cpu_to_be64(pDT->this_dma);
+#endif
+	if (pRB->ReplyDataAddr) {
+		pilot->num_hras = cpu_to_be16(2);
+		pilot->tag_len_hra[1].tag    = 0x2;
+		pilot->tag_len_hra[1].len[0] = (hra2_len & 0x0F0000) >> 16;
+		pilot->tag_len_hra[1].len[1] = (hra2_len & 0x00FF00) >>  8;
+		pilot->tag_len_hra[1].len[2] = (hra2_len & 0x0000FF);
+#if defined(WINDOWS)
+		pDT = CONTAINING_RECORD(pReq->DT_hra2.Flink, struct dt_t, list_entry);
+		pilot->tag_len_hra[1].hra = cpu_to_be64(pDT->this_LA);
+#endif
+#if defined(LINUX)
+		pDT = list_entry(pReq->DT_hra2.next, struct dt_t, list_head);
+		pilot->tag_len_hra[1].hra = cpu_to_be64(pDT->this_dma);
+#endif
+	}
+	pilot->agent_id   = cpu_to_be16(pRB->AgentID);
+	//pilot->frag_total = cpu_to_be16(frag_total);
+	pilot->frag_total = 0;
+	pilot->req_id     = cpu_to_be16(pRB->RequestID);
+	pilot->user_def   = cpu_to_be32(pRB->UserDefined);
+	pilot->status     = cpu_to_be32(pRB->Status);
+	pilot->msg_len    = cpu_to_be32(pRB->RequestControlBlkLength);
+	pilot->frag_len   = cpu_to_be32(XC_MIN_FRAG_LEN);
+
+#if defined(DEBUG)
+	PDEBUG(4, "Pilot msg size: 0x%lx  Padded to: 0x%x\n", sizeof(pilot_msg_t), padded_len);
+	hex_dump(ctx, 5, "Pilot Msg contents", pilot, sizeof(pilot_msg_t));
+#endif
+
+	rc = alloc_scatterlist(ctx,
+			       pReq->header, padded_len, padded_len,
+			       TRUE,
+			       &pReq->hdr_slist,
+			       DMA_TO_DEVICE);
+	if (rc != HOST_DD_Good) {
+		PDEBUG(1, "Device %d: alloc_scatterlist hdr failed\n", ctx->dev_index);
+		goto error;
+	}
+
+	rc = dt_append_scatterlist(ctx, pReq, &pReq->DT_req, &pReq->hdr_slist);
+	if (rc != HOST_DD_Good) {
+		PRINTKW("Device %d: dt_append_scatterlist req 1 failed\n", ctx->dev_index);
+		goto error;
+	}
+	
+	dt_set_hrb_len(&pReq->DT_req, hrb_len);
+
+
+	// now, assemble each fragment into a standalone 'request' 
+
+	remain = pRB->RequestControlBlkLength;
+	p = pRB->RequestControlBlkAddr;
+	while (remain > 0) {
+		uint8_t intIV = DMA_CW_NOINT_NOIV;
+		uint32_t sz = MIN(remain, XC_MIN_FRAG_LEN);
+
+		remain -= sz;
+		if (remain == 0)
+			intIV = DMA_CW_INT_NOIV;
+
+		rc = build_special_fragment(ctx,
+					    pReq,
+					    &pReq->req_slist,
+					    &pReq->DT_req,
+					    p, sz,
+					    is_SSP,
+					    FALSE,
+					    intIV);
+		if (rc != HOST_DD_Good)
+			goto error;
+
+		p += sz;
+	}
+	
+error:
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	
+	// caller is responsible for freeing scatterlists and DT chains upon error...
+	//
+	return rc;
+}
+
+
+int
+build_special_fragment(ibm4767_ctx_t    *ctx,
+		       request_t        *pReq,
+		       y_scatterlist_t  *slist,
+		       struct list_head *dtlist,
+		       void             *buf,
+		       uint32_t          datalen,
+		       bool              is_SSP,
+		       bool              is_kmem,
+		       uint8_t           intIV)
+{
+	dma_header16_t      dma_hdr;
+	struct list_head  tmp_dtlist;
+	y_scatterlist_t     tmp_slist;
+	int               rc, hrb_len;
+
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// keep in mind that even though this is a fragment, we construct it as
+	// a standalone request...
+
+	hrb_len = XC_MIN_FRAG_LEN + sizeof(dma_header16_t) - sizeof(dma_header8_t);
+
+	dma_hdr.hdr8.bytes[0] = (is_SSP ? DMA_HTYPE_H2S : DMA_HTYPE_H2M);
+	dma_hdr.hdr8.bytes[1] = DMA_RQSTR_HOST | DMA_CW_NORM;
+	dma_hdr.hdr8.bytes[2] = DMA_CW_TX | DMA_CW_MRB0 | intIV;
+	dma_hdr.hdr8.bytes[3] = 0;  // UF (reserved)
+	dma_hdr.hdr8.bytes[4] = ctx->vfid;
+	dma_hdr.sig           = cpu_to_be16((is_SSP ? DMA_SIG_H2S : DMA_SIG_H2M));
+
+	INIT_LIST_HEAD(&tmp_dtlist);
+	init_scatterlist(&tmp_slist);
+
+	rc = alloc_scatterlist(ctx,
+			       (char *)&dma_hdr,  sizeof(dma_hdr), sizeof(dma_hdr),
+			       TRUE,
+			       &tmp_slist,
+			       DMA_TO_DEVICE);
+	
+	if (rc != HOST_DD_Good)
+		goto error;
+
+	rc = alloc_scatterlist(ctx,
+			       buf,  datalen, XC_MIN_FRAG_LEN,
+			       is_kmem,
+			       &tmp_slist,
+			       DMA_TO_DEVICE);
+
+	if (rc != HOST_DD_Good)
+		goto error;
+
+	// FIXME - this is a nasty hack and needs to be fixed.  recall that the
+	// 4767 requires that the 'hrb_len' field of the first DT for each request
+	// must contain the length of that request.  we're allocating all frags
+	// into a single scatterlist so we can't simply convert to a DT chain at the
+	// end and then set 'head::hrb_len' like we would do for normal requests.
+	//
+	rc = dt_append_scatterlist(ctx, pReq, &tmp_dtlist, &tmp_slist);
+	if (rc != HOST_DD_Good)
+		goto error;
+
+	dt_set_hrb_len(&tmp_dtlist, hrb_len);
+	
+	merge_scatterlist(slist, &tmp_slist);
+	
+	slist->ctx = tmp_slist.ctx;
+	slist->direction = tmp_slist.direction;
+
+	dt_merge(dtlist, &tmp_dtlist);
+
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return rc;
+
+error:
+	PDEBUG(3, "Error cleanup for %s...\n", __FUNCTION__);
+	free_scatterlist(&tmp_slist);
+	dt_free_chain(ctx, &tmp_dtlist);
+	return rc;
+}
+
+
+#if defined(ENABLE_MFG)
+int
+special_request_post2_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current mcpu state
+	// BOOTING POST2   TAS    TAPP    NORMAL   TAMPER/DISABLED
+	// wait    ok      busy   busy    busy     fail
+	//
+	if (ctx->status_mcpu == MCPU_POST2_HELLO) {
+		PDEBUG(3, "Already in POST2...\n");
+		return HOST_DD_Good;
+	}
+	
+	if (ctx->status_mcpu > MCPU_POST2_HELLO) {
+		PRINTKW("Device %d: State engine failure.  Can't get to POST2 from here.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	pReq->reqd_state = MCPU_POST2_HELLO;
+
+	PDEBUG(3, "Sleep until state = MCPU_POST2...\n");
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+int
+special_request_mcpu_tas_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current mcpu state
+	// BOOTING POST2   TAS    TAPP    NORMAL   TAMPER/DISABLED
+	// wait    wait    ok     wait    busy     fail
+	//
+	// (note, I don't expect we'll ever encounter a situation where the MCPU state is
+	// MCPU_ACTIVE when an FTE request arrives...)
+	//
+	if (ctx->status_mcpu == MCPU_IBM_TAS_ACTIVE) {
+		PDEBUG(3, "Already in TAS MCPU...\n");
+		return HOST_DD_Good;
+	}
+
+	if (ctx->status_mcpu > MCPU_IBM_TAPP_ACTIVE) {
+		PRINTKW("Device %d: State engine failure.  Can't get to TAS MCPU from here.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	pReq->reqd_state = MCPU_IBM_TAS_ACTIVE;
+
+	// we're going to go to sleep until an the mailbox handler wakes us
+	// when we're in the desired state...
+	//
+	PDEBUG(3, "Sleep until state = MCPU_IBM_TAS_ACTIVE...\n");
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+	
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+#endif
+
+#if defined(ENABLE_MFG)
+int
+special_request_mcpu_tapp_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current mcpu state
+	// BOOTING POST2   TAS    TAPP    NORMAL   TAMPER/DISABLED
+	// wait    wait    wait   ok      fail     fail
+	//
+	// (note, I don't expect we'll ever encounter a situation where the MCPU state is
+	// MCPU_ACTIVE when an FTE request arrives...)
+	//
+	if (ctx->status_mcpu == MCPU_IBM_TAPP_ACTIVE) {
+		PDEBUG(3, "Already in TAPP MCPU...\n");
+		return HOST_DD_Good;
+	}
+
+	if (ctx->status_mcpu > MCPU_IBM_TAPP_ACTIVE) {
+		PRINTKW("Device %d: State engine failure.  Can't get to TAPP MCPU from here.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	pReq->reqd_state = MCPU_IBM_TAPP_ACTIVE;
+	
+	// we're going to go to sleep until an the mailbox handler wakes us
+	// when we're in the desired state...
+	//
+	PDEBUG(3, "Sleep until state = MCPU_IBM_TAPP_ACTIVE...\n");
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+	
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+int
+special_request_post1_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current ssp state
+	// BOOTING MB0     POST1  TAS     TAPP     MB1    HALT/TAMPER   QUIESCED/TEMP/DISABLED
+	// wait    wait    ok     busy    busy     busy   reset         fail
+	//
+	switch (ctx->status_ssp) {
+		case SSP_POST1_HELLO:
+			PDEBUG(3, "Already in POST1...\n");
+			return HOST_DD_Good;
+
+		case SSP_MB0:
+			PDEBUG(1, "Device %d: Incoming POST1 request while SSP in MB0.  Initiating state transition\n", ctx->dev_index);
+			write32(cpu_to_be32(CONTINUE_NORMAL), H2S_MBX_H(ctx));  // will this work?
+			ctx->status_ssp = SSP_WAIT_FOR_MB0_END;
+			break;
+
+		case SSP_MB_DONE:
+		case SSP_OFFLINE:
+		case SSP_TAMPER:
+		case SSP_SOFT_TAMPER:
+			PDEBUG(1, "Device %d: resetting SSP to resync with incoming POST1 request\n", ctx->dev_index);
+			ibm4767_reset_ssp(ctx, FALSE);
+			break;
+		
+		default:
+			// satisfy the compiler
+			break;
+	}
+
+	if (ctx->status_ssp > SSP_POST1_HELLO) {
+		PRINTKW("Device %d: State engine failure.  Can't get to TAPP MCPU from here.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+
+	pReq->reqd_state = SSP_POST1_HELLO;
+
+	// we're going to go to sleep until an the mailbox handler wakes us
+	// when we're in the desired state...
+	//
+	PDEBUG(3, "Sleep until state = SSP_POST1_HELLO...\n");
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+	
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+	
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+int
+special_request_ssp_tas_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current ssp state
+	// BOOTING MB0     POST1  TAS     TAPP     MB1    HALT/TAMPER   QUIESCED/TEMP/DISABLED
+	// wait    wait    wait   ok      wait     busy   reset         fail
+	//
+	switch (ctx->status_ssp) {
+		case SSP_IBM_TAS_ACTIVE:
+			PDEBUG(3, "Already in TAS SSP...\n");
+			return HOST_DD_Good;
+
+		case SSP_MB_DONE:
+		case SSP_OFFLINE:
+		case SSP_TAMPER:
+		case SSP_SOFT_TAMPER:
+			PDEBUG(1, "Device %d: resetting SSP to resync with incoming SSP TAS request\n", ctx->dev_index);
+			ibm4767_reset_ssp(ctx, FALSE);
+			break;
+		
+		default:
+			// satisfy the compiler
+			break;
+	}
+
+	if (ctx->status_ssp >= SSP_WAIT_FOR_MB1_START)  {
+		PRINTKW("Device %d: State engine failure.  Can't get to TAPP MCPU from here.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+	
+	pReq->reqd_state = SSP_IBM_TAS_ACTIVE;
+
+	// we're going to go to sleep until an the mailbox handler wakes us
+	// when we're in the desired state...
+	//
+	PDEBUG(3, "Sleep until state = SSP_TAS_ACTIVE...\n");
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+	
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+int
+special_request_ssp_tapp_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current ssp state
+	// BOOTING MB0     POST1  TAS     TAPP     MB1    HALT/TAMPER  QUIESCED/TEMP/DISABLED
+	// wait    wait    wait   wait    ok       fail   reset        fail
+	//
+	switch (ctx->status_ssp) {
+		case SSP_IBM_TAPP_ACTIVE:
+			PDEBUG(3, "Already in TAPP SSP...\n");
+			return HOST_DD_Good;
+
+		case SSP_MB_DONE:
+		case SSP_OFFLINE:
+		case SSP_TAMPER:
+		case SSP_SOFT_TAMPER:
+			PDEBUG(1, "Device %d: resetting SSP to resync with incoming SSP TAPP request\n", ctx->dev_index);
+			ibm4767_reset_ssp(ctx, FALSE);
+			break;
+		
+		default:
+			// satisfy the compiler
+			break;
+	}
+
+	if (ctx->status_ssp >= SSP_WAIT_FOR_MB1_START)  {
+		PRINTKW("Device %d: State engine failure.  Can't get to TAPP MCPU from here.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter);
+		return HOST_DD_DeviceBusy;
+	}
+	
+	pReq->reqd_state = SSP_IBM_TAPP_ACTIVE;
+	
+	// we're going to go to sleep until an the mailbox handler wakes us
+	// when we're in the desired state...
+	//
+	PDEBUG(3, "Sleep until state = SSP_IBM_TAPP_ACTIVE...\n");
+	wait_event_interruptible(pReq->waitQ, atomic_read(&pReq->waitEvent));
+	atomic_set(&pReq->waitEvent, 0);
+	
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+	
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+#endif
+
+
+int
+special_request_mb0_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	const char *txt = NULL;
+
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current ssp state
+	// BOOTING MB0     POST1  TAS     TAPP     MB1    HALT/TAMPER   QUIESCED/TEMP/DISABLED
+	// wait    ok      reset  reset   reset    reset  reset         fail
+	//
+	// note: miniboot most likely will not exist on cards undergoing FTE so we should
+	//       never see an incoming MB request while in TAS or TAPP state...
+	//
+	switch (ctx->status_ssp) {
+		case SSP_MB0:
+			PDEBUG(3, "Already in MB0...\n");
+			return HOST_DD_Good;
+
+		default:
+			// in principle, we should have already filtered out the TAMPER/TEMP/DISABLED states...
+			//
+			PDEBUG(2, "Device %d: resetting SSP to resync with incoming SSP MB0 request\n", ctx->dev_index);
+
+			// in principle, we don't need to reset if state is < MB0 but those states tend
+			// to change quickly and resetting avoids a race condition
+
+			// here, we do NOT want to do a deferred main reset (the deferred routine will see
+			// a MB request is active and defer indefinitely).  it's safe to do this because this
+			// routine will never be called from interrupt context
+			//
+			ibm4767_reset_ssp(ctx, FALSE);
+
+			break;
+	}
+
+	pReq->reqd_state = SSP_MB0;
+
+	// we're going to go to sleep until an the mailbox handler wakes us
+	// when we're in the desired state...
+	//
+	PDEBUG(3, "Sleep until state = SSP_MB0...\n");
+
+	pReq->timeout = timeout_mb * HZ;
+	if (wait_event_interruptible_timeout(pReq->waitQ, atomic_read(&pReq->waitEvent), pReq->timeout) == 0) {
+		PRINTKE("Device %d: TIMEOUT waiting for SSP state change (MB0)\n", ctx->dev_index);
+
+		txt = ssp_state_to_str(ctx->status_ssp);
+		if (txt)
+			PRINTKE("Device %d:  Current SSP state:  %s\n", ctx->dev_index, txt);
+		else
+			PRINTKE("Device %d:  Current SSP state:  UNKNOWN (%d)\n", ctx->dev_index, ctx->status_ssp);
+
+		mark_ssp_offline(ctx, HOSTTimeout);
+		atomic_set(&pReq->waitEvent, 0);
+		return HOST_DD_Timeout;
+	}
+
+	atomic_set(&pReq->waitEvent, 0);
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+	
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+
+
+int
+special_request_mb1_pre(ibm4767_ctx_t *ctx, request_t *pReq)
+{
+	const char *txt = NULL;
+
+
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	// current ssp state
+	// BOOTING MB0     POST1  TAS     TAPP     MB1    HALT/TAMPER   QUIESCED/TEMP/DISABLED
+	// wait    cont    wait   reset   reset    ok     reset         fail
+	//
+	switch (ctx->status_ssp) {
+		case SSP_MB0:
+			PDEBUG(1, "Device %d: Incoming MB1 request while SSP in MB0.  Initiating state transition\n", ctx->dev_index);
+			write32(cpu_to_be32(CONTINUE_NORMAL), H2S_MBX_H(ctx));  // will this work?
+			ctx->status_ssp = SSP_WAIT_FOR_MB0_END;
+			break;
+
+		case SSP_MB1:
+			PDEBUG(3, "Already in MB1...\n");
+			return HOST_DD_Good;
+
+#if defined(ENABLE_MFG)
+		case SSP_POST1_SERIAL_DBG:
+		case SSP_IBM_TAS_START:
+		case SSP_IBM_TAS_ACTIVE:
+		case SSP_IBM_TAPP_START:
+		case SSP_IBM_TAPP_ACTIVE:
+#endif
+		case SSP_WAIT_FOR_MB1_END:
+		case SSP_MB_DONE:
+		case SSP_OFFLINE:
+			PDEBUG(1, "Device %d: resetting SSP to resync with incoming SSP MB1 request\n", ctx->dev_index);
+			// SSP high-priority reset is broken so the driver cannot reliably reset the SSP
+			//ibm4767_main_reset(ctx, FALSE);
+
+			// here, we do NOT want to do a deferred main reset (the deferred routine will see
+			// a MB request is active and defer indefinitely).  it's safe to do this because this
+			// routine will never be called from interrupt context
+			//
+			ibm4767_reset_ssp(ctx, FALSE);
+			break;
+
+		default:
+			// satisfy the compiler
+			break;
+	}
+
+	if (ctx->status_ssp > SSP_MB1) {
+		PRINTKW("Device %d: State engine failure.  Can't get to MB1 from here.\n", ctx->dev_index);
+		atomic_inc(&ctx->ebusy_counter); 
+		return HOST_DD_DeviceBusy; 
+	}
+	
+	pReq->reqd_state = SSP_MB1;
+
+	// we're going to go to sleep until an the mailbox handler wakes us
+	// when we're in the desired state...
+	//
+	PDEBUG(2, "Sleep until state = SSP_MB1...\n");
+	
+	pReq->timeout = timeout_mb * HZ;
+	if (wait_event_interruptible_timeout(pReq->waitQ, atomic_read(&pReq->waitEvent), pReq->timeout) == 0) {
+		PRINTKE("Device %d: TIMEOUT waiting for SSP state change (MB1)\n", ctx->dev_index);
+		
+		txt = ssp_state_to_str(ctx->status_ssp);
+		if (txt)
+			PRINTKE("Device %d:  Current SSP state:  %s\n", ctx->dev_index, txt);
+		else
+			PRINTKE("Device %d:  Current SSP state:  UNKNOWN (%d)\n", ctx->dev_index, ctx->status_ssp);
+	
+		mark_ssp_offline(ctx, HOSTTimeout);	
+		atomic_set(&pReq->waitEvent, 0);
+		return HOST_DD_Timeout;
+	}
+
+	atomic_set(&pReq->waitEvent, 0);
+
+	// did an out-of-band event occur that caused us to abort?
+	//
+	if (pReq->retcode) {
+		PDEBUG(3, "Woke up but retcode = 0x%x\n", pReq->retcode);
+		return HOST_DD_Aborted;
+	}
+
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+	return HOST_DD_Good;
+}
+
diff --git drivers/misc/ibm4767/target.c drivers/misc/ibm4767/target.c
new file mode 100755
index 000000000000..450c6390f314
--- /dev/null
+++ drivers/misc/ibm4767/target.c
@@ -0,0 +1,211 @@
+/*************************************************************************
+ *  Filename:target.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Visegrady, Tamas  IBM Poughkeepsie  <tamas@us.ibm.com>
+ *           Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0))
+#include <linux/signal.h>
+#else
+#include <linux/sched/signal.h>
+#endif
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_mailbox.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+
+int 
+target_read8_ssp_fifo(ibm4767_ctx_t *ctx, unsigned char *data, int safe)
+{
+	uint64_t hbmsr;
+	unsigned long j = jiffies;
+			
+	hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	while (hbmsr & HBMSR_S2H_FE) {
+		mdelay(1);
+		if (jiffies > (j + HZ)) 
+			return -EIO;
+
+		if (safe && signal_pending(current))
+			return HOST_DD_Interrupted;
+		
+		hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	}
+
+	// it would seem that at least some AMD+mainboard combinations are unable
+	// to perform an atomic 64bit MMIO read.  instead, it gets broken into two 32bit 
+	// reads (this seems to happen somewhere below the kernel).  you cannot synthesize a
+	// 64bit FIFO read by reading the HI and LO words separately as any read will cause
+	// the FIFO to advance.  instead you have to read HI from the AUX port.
+	//
+#if defined(BROKEN_64BIT_READS)
+	{
+		uint32_t *hi = (uint32_t *)&data[0];
+		uint32_t *lo = (uint32_t *)&data[4];
+		*hi = readl(S2H_FIFO_AUX_DOUT(ctx));
+		*lo = readl(S2H_FIFO_DOUT_L(ctx));
+		PDEBUG(4, "READ S2HF: %08X : %08X\n", *hi, *lo);
+	}
+#else
+	{
+		uint64_t *val = (uint64_t *)data;
+		*val = read64(S2H_FIFO_DOUT(ctx));
+		PDEBUG(4, "READ S2HF: %016llX\n", *val);
+	}
+#endif
+	return 0;
+}
+
+		
+int 
+target_send8_ssp_fifo(ibm4767_ctx_t *ctx, unsigned char *data)
+{
+	uint64_t hbmsr;
+	unsigned long j = jiffies;
+
+	hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	while (hbmsr & HBMSR_H2S_FF) {
+		mdelay(1);
+		if (jiffies > (j + HZ))
+			return -EIO;
+
+		hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	}
+
+#if defined(BROKEN_64BIT_READS)
+	{
+		uint32_t hi = *(uint32_t *)data;
+		uint32_t lo = *(uint32_t *)(data + 4);
+		writel(hi, H2S_FIFO_DIN_H(ctx));
+		writel(lo, H2S_FIFO_DIN_L(ctx));
+	}
+#else
+	{
+		uint64_t val = cpu_to_be64(*(uint64_t *)data);
+		write64(val, H2S_FIFO_DIN(ctx));
+	}
+#endif
+	return 0;
+}
+
+
+// we have to be careful when reading the FIFO since it's also used for emergency dumps
+//
+// if we ever see that HBMSR::H45_DE == 0, then assume an emergency dump is pending and 
+// immediately abort the current read operation
+//
+int 
+target_read8_mcpu_fifo(ibm4767_ctx_t *ctx, unsigned char *data, int safe)
+{
+	uint64_t hbmsr;
+	unsigned long j = jiffies;
+
+	hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	while (hbmsr & HBMSR_M2H_FE) {
+		mdelay(1);
+		if (jiffies > (j + HZ))
+			return -EIO;
+
+		if (safe && signal_pending(current))
+			return HOST_DD_Interrupted;
+		
+		hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	}
+
+#if defined(BROKEN_64BIT_READS)
+	{
+		uint32_t *hi = (uint32_t *)&data[0];
+		uint32_t *lo = (uint32_t *)&data[4];
+		*hi = readl(M2H_FIFO_AUX_DOUT(ctx));
+		*lo = readl(M2H_FIFO_DOUT_L(ctx));
+		PDEBUG(4, "READ M2HF: %08X : %08X\n", *hi, *lo);
+	}
+#else
+	{
+		uint64_t *val = (uint64_t *)data;
+		*val = read64(M2H_FIFO_DOUT(ctx));
+		PDEBUG(4, "READ M2HF: %016llX\n", *val);
+	}
+#endif
+	return 0;
+}
+
+
+int 
+target_send8_mcpu_fifo(ibm4767_ctx_t *ctx, unsigned char *data)
+{
+	uint64_t hbmsr;
+	unsigned long j = jiffies;
+
+	hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	while (hbmsr & HBMSR_H2M_FF) {
+		mdelay(1);
+		if (jiffies > (j + HZ))
+			return -EIO;
+
+		hbmsr = be64_to_cpu(read64(HBMSR(ctx)));
+	}
+
+#if defined(BROKEN_64BIT_READS)
+	{
+		uint32_t hi = *(uint32_t *)data;
+		uint32_t lo = *(uint32_t *)(data + 4);
+		writel(hi, H2M_FIFO_DIN_H(ctx));
+		writel(lo, H2M_FIFO_DIN_L(ctx));
+	}
+#else
+	{
+		uint64_t val = cpu_to_be64(*(uint64_t *)data);
+		write64(val, H2M_FIFO_DIN(ctx));
+	}
+#endif
+
+	return 0;
+}
+
diff --git drivers/misc/ibm4767/timers.c drivers/misc/ibm4767/timers.c
new file mode 100755
index 000000000000..6192de626336
--- /dev/null
+++ drivers/misc/ibm4767/timers.c
@@ -0,0 +1,663 @@
+/*************************************************************************
+ *  Filename:timers.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Visegrady, Tamas  IBM Poughkeepsie  <tamas@us.ibm.com>
+ *           Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_mailbox.h"
+#include "driver.h"
+#include "y_funcs.h"
+#include "host_err.h"
+
+
+// a word about deleting self-rescheduling timers...
+//
+// the conventional wisdom when deleting a timer that reschedules itself 
+// is that you *must* tell the timer to not reschedule otherwise you can wind up
+// with a timer that's still scheduled even after del_timer_sync() says it's gone.  
+//
+// hence we use the ctx::timer_quiesce bit array to quiesce our timers
+//
+// however, if you look at the code for del_timer_sync() and try_to_del_timer_sync() 
+// in kernel/timer.c you'll see something like this:
+//
+// while (1) {
+//     if (timer is not running) {
+//         if (timer is scheduled) {
+//             unschedule it
+//         }
+//         break
+//     }
+//     else {
+//         sleep
+//     }
+// }
+//
+// (this pseudocode ignores the various locks that are grabbed)
+//
+// so the loop polls until the timer is no longer running on any CPU
+// in the system at which point the kernel checks the list of scheduled timers
+// and removes it if necessary.
+//
+// so given this construct it seems unnecessary to require timers
+// to quiesce themselves.  requests for clarification on the kernel mailing lists 
+// have gone unanswered so I'll play along...
+// 
+
+
+// governed by ibm4767_ctx_t::seg3_started_timer
+//
+// this timer gets restarted each time the card gets reset
+//
+void
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_seg3_started_timer(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_seg3_started_timer(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, seg3_started_timer);
+#endif
+	uint32_t  hrcsr;
+
+	// this guy polls HRCSR::SEG3_START and HRCSR_SEG3_CMPL until they indicate that
+	// seg3 app has attached.  this will be used in place of the deprecated 
+	// AGENTID_ATTACHED mailbox notification...
+	//
+	hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+	if (hrcsr & (HRCSR_SEG3_START | HRCSR_SEG3_CMPL)) {
+		ctx->status_mcpu = MCPU_ACTIVE;
+		ctx->status_fp   = FP_READY;
+		return;
+	}
+
+	spin_lock_bh(&ctx->timer_quiesce_lock);
+	if (!(ctx->timer_quiesce & TIMER_QUIESCE_SEG3_STARTED)) 
+		mod_timer(&ctx->seg3_started_timer, jiffies + HZ/2);
+	spin_unlock_bh(&ctx->timer_quiesce_lock);
+}
+
+
+// governed by ibm4767_ctx_t::deferred_buf_update_timer
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_deferred_buf_update(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_deferred_buf_update(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, deferred_buf_update_timer);
+#endif
+
+	if (ctx->status_mcpu == MCPU_OFFLINE)
+		return;
+
+	// check to see if a pending Req Buf Update can be satisfied now...
+	// 
+	// there might be a race condition here if a REQBUFUPDATE mailbox message
+	// arrives while we're doing this.  this would imply that two or more
+	// REQBUFUPDATE messages were sent between buffer updates.  I don't
+	// think that can happen but who knows.  just be aware if you see weirdness.
+	//
+	if (atomic_read(&ctx->asym_attr.req_buf_update)) { 
+		atomic_set(&ctx->asym_attr.req_buf_update, 0); 
+		PDEBUG(1, "Device %d: Deferred req buffer update...\n", ctx->dev_index); 
+		asym_hra_pool_update(ctx); 
+		return;
+	}
+	
+	// otherwise, try again in 200ms
+	spin_lock_bh(&ctx->timer_quiesce_lock);
+	if (!(ctx->timer_quiesce & TIMER_QUIESCE_DEFERRED_BUF_UPDATE)) 
+		mod_timer(&ctx->deferred_buf_update_timer, jiffies + HZ/5);
+	spin_unlock_bh(&ctx->timer_quiesce_lock);
+}
+
+
+// governed by ibm4767_ctx_t::deferred_forced_edump_timer
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_deferred_forced_edump(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_deferred_forced_edump(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, deferred_forced_edump_timer);
+#endif
+
+	// per Richard K's request, if a log message has been received within the 
+	// last 2 seconds, defer this timer...
+	//
+	if (!(time_before_eq(jiffies, (ctx->lastlog_timestamp + HZ*2)))) {
+		PRINTKW("Device %d: <KERN_EMERG> not rcvd within 2 sec of last log msg.  Forcing e-dump...\n", ctx->dev_index);
+		write32(cpu_to_be32(MBX_FORCE_EDUMP), H2M_MBX_H(ctx));
+	}
+	else {
+		PDEBUG(2, "Log msg rcvd within last 2 seconds...deferring forced EDUMP\n");
+		spin_lock_bh(&ctx->timer_quiesce_lock);
+		if (!(ctx->timer_quiesce & TIMER_QUIESCE_DEFERRED_FORCED_EDUMP)) 
+			mod_timer(&ctx->deferred_forced_edump_timer, jiffies + HZ*2);
+		spin_unlock_bh(&ctx->timer_quiesce_lock);
+	}
+}
+
+
+// goverend by ibm4767_ctx_t::deferred_main_reset_timer
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_deferred_main_reset(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_deferred_main_reset(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, deferred_main_reset_timer);
+#endif
+	uint32_t  hrcsr;
+
+	// this timer is used when we'd like to reset the card but cannot guarantee that
+	// all interrupts associated with an event have yet been received/handled.
+	//
+	// for example: When a critical error interrupt occurs on the card, the SSP will
+	// potentially generate an emergency dump and notify the driver.  At the same time,
+	// the MCPU will potentially generate an emergency dump (on a separate FIFO) and 
+	// notify the driver.  After it finishes the dump, the MCPU will then generate
+	// a kernel panic notification at which point the host driver would like to reset
+	// the card.
+	//
+	// We can't guarantee the order in which these interrupts arrive and, indeed, there's
+	// no guarantee that both the SSP and MCPU will generate dumps depending on the
+	// severity of the critical error.  So upon receiving the kernel panic notification,
+	// we wait a few seconds before resetting to give time to handle any SSP data
+	// that may yet arrive.
+	//
+	// We also need to ensure that there are no requests in flight.  It's possible
+	// that the MCPU could generate a kernel panic while a miniboot command is
+	// active on the SSP.  In this case, we CANNOT reset the card until the miniboot
+	// command completes or times-out.
+	//
+
+
+	// active_mb is protected by counter_lock.  we don't really need to grab it here
+	// because ctx->status_flags already has FLAG_MAIN_RESET_PENDING so no new MB
+	// commands will be issued.  if an existing MB command finishes after we read
+	// active_mb == 1 (the read itself will be atomic on intel), all that will happen 
+	// is we'll reschedule the timer.
+	// 
+	if (ctx->active_mb) {
+		PDEBUG(1, "Device #%d:  MB request in progress...deferring main reset 1 sec...\n", ctx->dev_index);
+		spin_lock_bh(&ctx->timer_quiesce_lock);
+		if (!(ctx->timer_quiesce & TIMER_QUIESCE_DEFERRED_MAIN_RESET)) 
+			mod_timer(&ctx->deferred_main_reset_timer, jiffies + HZ);
+		spin_unlock_bh(&ctx->timer_quiesce_lock);
+		return;
+	}
+
+	// if a tamper occurs, the card will be held in reset:
+	// A) if it's a hard tamper, HRCSR::TRst will go to 0 and HRCSR::RRAT will
+	//    go to 1 after the card has entered diagnostic mode.  
+	//
+	// B) if it's a soft tamper, HRCSR::STRst will go to 0 and HRCSR::RRAST
+	//    will go to 1 after the condition that caused the tamper 
+	//    (temperature, voltage) disappears
+	//
+	hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+	if (hrcsr & HRCSR_TRST) {
+		PRINTKW("Device #%d:  Tamper reset active...deferring reset...\n", ctx->dev_index);
+		spin_lock_bh(&ctx->timer_quiesce_lock);
+		if (!(ctx->timer_quiesce & TIMER_QUIESCE_DEFERRED_MAIN_RESET)) 
+			mod_timer(&ctx->deferred_main_reset_timer, jiffies + HZ);
+		spin_unlock_bh(&ctx->timer_quiesce_lock);
+		return;
+	}
+
+	if (hrcsr & HRCSR_STRST) {
+		PRINTKW("Device #%d:  Soft tamper reset active...deferring reset...\n", ctx->dev_index);
+		spin_lock_bh(&ctx->timer_quiesce_lock);
+		if (!(ctx->timer_quiesce & TIMER_QUIESCE_DEFERRED_MAIN_RESET)) 
+			mod_timer(&ctx->deferred_main_reset_timer, jiffies + HZ);
+		spin_unlock_bh(&ctx->timer_quiesce_lock);
+		return;
+	}
+
+	PRINTKW("Resetting device %d\n", ctx->dev_index);
+
+	// ibm4767_main_reset() might sleep while waiting for timers and work threads to terminate
+	// so we can't call it from atomic context...
+	//
+	schedule_work(&ctx->main_reset_work);
+}
+
+
+// governed by ibm4767_ctx_t::deferred_ssp_wakeup_timer
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_deferred_ssp_wakeup(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_deferred_ssp_wakeup(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, deferred_ssp_wakeup_timer);
+#endif
+	uint32_t  hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+	
+	
+	if (hrcsr & HRCSR_SSP_RstS) {
+		ibm4767_ssp_wur(ctx);
+		return;
+	}
+	
+	PDEBUG(2, "Device %d: %s - HRCSR indicates SSP not in reset...Defer WUR\n", ctx->dev_index, __FUNCTION__);
+	
+	spin_lock_bh(&ctx->timer_quiesce_lock);
+	if (!(ctx->timer_quiesce & TIMER_QUIESCE_DEFERRED_SSP_WUR))
+		mod_timer(&ctx->deferred_ssp_wakeup_timer, jiffies + HZ);
+	spin_unlock_bh(&ctx->timer_quiesce_lock);
+}
+
+
+// governed by ibm4767_ctx_t::cdu_timeout_timer
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_cdu_timeout(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_cdu_timeout(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, cdu_timeout_timer);
+#endif
+
+	// we've had a CDU timeout.  this is indicative that something bad has happened
+	// on the card.  perhaps the old embedded app crashed just prior to the
+	// PROCEED_TO_CDU and so Comm Mgr never sent the CDU_READY_TO_PROCEED msg.
+	//
+	// in any event, resetting the card here is probably a safe operation.  if the
+	// app sends a request while the reset is active, it'll get a failure return code
+	// (which would happen anyway if the app had crashed as in the above scenario
+	// though the error code would be different)...
+	//
+	switch (ctx->status_cdu) {
+		case CDU_WAIT_FOR_READY_TO_PROCEED:
+			PRINTKW("Timeout waiting for M2H msg CDU_READY_TO_PROCEED.  Resetting card %d\n", ctx->dev_index);
+			ibm4767_cdu_finalize(ctx, CDU_ERR_TIMEOUT);
+			defer_main_reset(ctx, 5, FALSE);
+			break;
+
+		case CDU_WAIT_FOR_PROCEED_ACK:
+			PRINTKW("Timeout waiting for M2H msg CDU_PROCEED_ACK.  Resetting card %d\n", ctx->dev_index);
+			ctx->status_cdu = CDU_WAIT_FOR_AGENTID_LIST;
+			spin_lock_bh(&ctx->timer_quiesce_lock);
+			if (!(ctx->timer_quiesce & TIMER_QUIESCE_CDU))
+				mod_timer(&ctx->agentid_list_timer, jiffies + HZ*2);
+			spin_unlock_bh(&ctx->timer_quiesce_lock);
+			write32(cpu_to_be32(MBX_LIST_ALL_AGENTIDS), H2M_MBX(ctx));
+			ibm4767_cdu_finalize(ctx, CDU_ERR_TIMEOUT);
+			defer_main_reset(ctx, 8, FALSE);
+			break;
+
+		case CDU_INACTIVE:
+			break;
+
+		case CDU_WAIT_FOR_AGENTID_LIST:
+			// this should never happen 
+			break;
+
+		default:
+			PRINTKW("Timeout:  unknown CDU state (%d) for card %d!?\n", ctx->status_cdu, ctx->dev_index);
+			ibm4767_cdu_finalize(ctx, CDU_ERR_TIMEOUT);
+			defer_main_reset(ctx, 5, FALSE);
+			break;
+	}
+}
+
+
+// governed by ibm4767_ctx_t::agentid_list_timer
+//
+// the LIST_ALL_AGENTIDS handshake is initiated when a CDU timeout occurs when waiting for the Comm Mgr
+// to acknowledge the PROCEED_TO_CDU command.  this typically means that the new embedded application has
+// terminated abnormally.
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_agentid_list(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_agentid_list(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, agentid_list_timer);
+#endif
+
+	if (ctx->status_cdu == CDU_WAIT_FOR_AGENTID_LIST) {
+		PRINTKW("Timeout waiting for AGENT_ID_LIST.  Resetting card %d\n", ctx->dev_index);
+		defer_main_reset(ctx, 5, FALSE);
+	}
+}
+
+
+// governed by ibm4767_ctx_t::ssp_timer
+//
+// this timer tracks the progress of the SSP state machine.  in this sense, it's 
+// a little different from ibm4767_ctx_t::miniboot_timer which doesn't care about the state
+// machine.
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_ssp_timeout(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx= (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_ssp_timeout(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx= (ibm4767_ctx_t *)from_timer(ctx, arg, ssp_timer);
+#endif
+	int timeout = 0;
+
+	switch (ctx->status_ssp) {
+		case SSP_OFFLINE:
+			break;
+
+		case SSP_WAIT_FOR_POST0:
+			PRINTKW("Device %d:  timeout waiting for POST0 _START.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+		
+		case SSP_WAIT_FOR_POST0_END:
+			PRINTKW("Device %d:  timeout waiting for POST0 _END.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+
+		case SSP_WAIT_FOR_MB0_START:
+			PRINTKW("Device %d: timeout waiting for MB0 _START.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+
+		case SSP_WAIT_FOR_MB0_HELLO:
+			PRINTKW("Device %d: timeout waiting for MB0 _HELLO.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+
+		case SSP_WAIT_FOR_MB0_END:
+			PRINTKW("Device %d:  timeout waiting for MB0 _END.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+
+		case SSP_WAIT_FOR_POST1_START:
+			PRINTKW("Device %d:  timeout waiting for POST1 _START.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+		
+		case SSP_WAIT_FOR_POST1_HELLO:
+			PRINTKW("Device %d:  timeout waiting for POST1 _HELLO.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+		
+		case SSP_WAIT_FOR_POST1_END:
+			PRINTKW("Device %d:  timeout waiting for POST1 _END.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+		
+		case SSP_WAIT_FOR_MB1_START:
+			PRINTKW("Device %d:  timeout waiting for MB1 _START.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+
+		case SSP_WAIT_FOR_MB1_HELLO:
+			PRINTKW("Device %d:  timeout waiting for MB1 _HELLO.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+
+		case SSP_WAIT_FOR_MB1_END:
+			PRINTKW("Device %d:  timeout waiting for MB1 _END.\n", ctx->dev_index);
+			timeout = 1;
+			break;
+		
+		default:
+			PRINTKW("MB timeout unknown state 0x%X\n", ctx->status_ssp);
+			// satisfy the compiler
+			break;
+	}
+
+	if (timeout) {
+		dump_regs(ctx);
+		mark_ssp_offline(ctx, HOSTTimeout);
+	}
+}
+
+
+// governed by ibm4767_ctx_t::temp_throttle_timer
+//
+// this timer is enabled by the high temperature warning interrupt.  it monitors the
+// temperature as reported in the RMSR2 register and adjusts the request throttle
+// accordingly.  note: the temperature sensor that generated the high temp interrupt
+// is inaccessible from the host driver.  the RMSR2 accesses a temperature sensor
+// elsewhere on the card so some discrepency is expected...
+//
+void 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_temp_throttle_timer(unsigned long arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)arg;
+#else
+ibm4767_timer_temp_throttle_timer(struct timer_list *arg)
+{
+	ibm4767_ctx_t *ctx = (ibm4767_ctx_t *)from_timer(ctx, arg, temp_throttle_timer);
+#endif
+	uint64_t  hcsr;
+	uint32_t  temp;
+
+	if ((ctx->status_ssp >= SSP_OFFLINE) && (ctx->status_mcpu >= MCPU_OFFLINE))
+		return;
+
+	hcsr = cpu_to_be64(read64(HCSR(ctx)));
+	temp = ((hcsr & HCSR_CURR_TEMP) >> 40) & 0xFF;
+
+	if (temp > DEFAULT_HIGH_TEMP_THRESH) {
+		PRINTKE("Device %d (S/N: %s): High Temp Threshhold reached (%d deg C).  Disabling device to avoid tamper event.\n", ctx->dev_index, ctx->hwinfo.serial_num, DEFAULT_HIGH_TEMP_THRESH);
+		mark_device_offline(ctx, HOSTTemperature);
+		notify_observers(ctx, EVENT_HIGH_TEMP_OFFLINE);
+		dump_regs(ctx);
+
+		// don't reschedule the timer if we've marked outselves offline
+	}
+	else {
+		if (temp <= DEFAULT_LOW_TEMP_THRESH) {
+			if (ctx->throttle_lvl > 0) {
+				PRINTKE("Device %d (S/N: %s): Low Temp Threshhold reached (%d deg C).  Disengaging request throttling\n", ctx->dev_index, ctx->hwinfo.serial_num, DEFAULT_LOW_TEMP_THRESH);
+				ctx->throttle_lvl = 0;
+			}
+		}
+		else {
+			uint32_t old = ctx->throttle_lvl;
+			ctx->throttle_lvl = temp - DEFAULT_LOW_TEMP_THRESH;
+			if (ctx->throttle_lvl != old)
+				PRINTKE("Device %d (S/N: %s): New temp: %d deg C. Adjusting throttle level to %d.\n", ctx->dev_index, ctx->hwinfo.serial_num, temp, ctx->throttle_lvl);
+		}
+
+		spin_lock_bh(&ctx->timer_quiesce_lock);
+		if (!(ctx->timer_quiesce & TIMER_QUIESCE_TEMP_THROTTLE))
+			mod_timer(&ctx->temp_throttle_timer, jiffies + HZ);
+		spin_unlock_bh(&ctx->timer_quiesce_lock);
+	}
+}
+
+
+#if 0
+// governed by ibm4767_ctx_t::soft_tamper_timer
+//
+void
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+ibm4767_timer_soft_tamper_timer(unsigned long arg)
+#else
+ibm4767_timer_soft_tamper_timer(struct timer_list *arg)
+#endif
+	ibm4767_ctx_t *ctx = NULL;
+	int flag;
+
+	read_lock(&ibm4767_lock);
+
+	// because of hotplug, we can't just stop at the first NULL slot...
+	for (i=0, flag=0; i < MAX_DEV_COUNT; i++) {
+		if (!ibm4767_list[i])
+			continue;
+
+		ctx = ibm4767_list[i];
+		hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+		if (hrcsr & HRCSR_SOFT_TEMP)
+			flag = 1;
+	}
+
+	if (!flag) {
+		if (atomic_read(&temp_soft_tamper_detected)) {
+			atomic_set(&temp_soft_tamper_detected, 0);
+
+			for (i=0; i < MAX_DEV_COUNT; i++) {
+				if (!ibm4767_list[i])
+					break;
+
+				ctx = ibm4767_list[i];
+				ctx->tamper_status = TAMPER_NONE;
+				defer_main_reset(ctx, 1, FALSE);
+			}
+		}
+	}
+
+	// FIXME
+
+done:
+	read_unlock(&ibm4767_lock);
+	mod_timer(&ctx->sot_tamper_timer, jiffies + HZ);
+}
+#endif
+
+
+void
+ibm4767_timers_init(ibm4767_ctx_t *ctx)
+{
+	ctx->timer_quiesce = 0;
+
+	spin_lock_init(&ctx->timer_quiesce_lock);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+	init_timer(&ctx->agentid_list_timer);
+	ctx->agentid_list_timer.data = (unsigned long)ctx;
+	ctx->agentid_list_timer.function = ibm4767_timer_agentid_list;
+	
+	init_timer(&ctx->deferred_buf_update_timer);
+	ctx->deferred_buf_update_timer.data = (unsigned long)ctx;
+	ctx->deferred_buf_update_timer.function = ibm4767_timer_deferred_buf_update;
+
+	init_timer(&ctx->cdu_timeout_timer);
+	ctx->cdu_timeout_timer.data = (unsigned long)ctx;
+	ctx->cdu_timeout_timer.function = ibm4767_timer_cdu_timeout;
+
+	init_timer(&ctx->deferred_forced_edump_timer);
+	ctx->deferred_forced_edump_timer.data = (unsigned long)ctx;
+	ctx->deferred_forced_edump_timer.function = ibm4767_timer_deferred_forced_edump;
+
+	init_timer(&ctx->deferred_main_reset_timer);
+	ctx->deferred_main_reset_timer.data = (unsigned long)ctx;
+	ctx->deferred_main_reset_timer.function = ibm4767_timer_deferred_main_reset;
+	
+	init_timer(&ctx->deferred_ssp_wakeup_timer);
+	ctx->deferred_ssp_wakeup_timer.data = (unsigned long)ctx;
+	ctx->deferred_ssp_wakeup_timer.function = ibm4767_timer_deferred_ssp_wakeup;
+
+	init_timer(&ctx->ssp_timer);
+	ctx->ssp_timer.data = (unsigned long)ctx;
+	ctx->ssp_timer.function = ibm4767_timer_ssp_timeout;
+
+	init_timer(&ctx->temp_throttle_timer);
+	ctx->temp_throttle_timer.data = (unsigned long)ctx;
+	ctx->temp_throttle_timer.function = ibm4767_timer_temp_throttle_timer;
+
+	init_timer(&ctx->seg3_started_timer);
+	ctx->seg3_started_timer.data = (unsigned long)ctx;
+	ctx->seg3_started_timer.function = ibm4767_timer_seg3_started_timer;
+#else
+	timer_setup(&ctx->agentid_list_timer, ibm4767_timer_agentid_list, 0);
+	timer_setup(&ctx->deferred_buf_update_timer, ibm4767_timer_deferred_buf_update, 0);
+	timer_setup(&ctx->cdu_timeout_timer, ibm4767_timer_cdu_timeout, 0);
+	timer_setup(&ctx->deferred_forced_edump_timer, ibm4767_timer_deferred_forced_edump, 0);
+	timer_setup(&ctx->deferred_main_reset_timer, ibm4767_timer_deferred_main_reset, 0);
+	timer_setup(&ctx->deferred_ssp_wakeup_timer, ibm4767_timer_deferred_ssp_wakeup, 0);
+	timer_setup(&ctx->ssp_timer, ibm4767_timer_ssp_timeout, 0);
+	timer_setup(&ctx->temp_throttle_timer, ibm4767_timer_temp_throttle_timer, 0);
+	timer_setup(&ctx->seg3_started_timer, ibm4767_timer_seg3_started_timer, 0);
+#endif
+}
+
+
+void
+ibm4767_timers_unload(ibm4767_ctx_t *ctx)
+{
+	ctx->timer_quiesce = TIMER_QUIESCE_ALL;
+
+	del_timer_sync(&ctx->agentid_list_timer);
+	del_timer_sync(&ctx->deferred_buf_update_timer);
+	del_timer_sync(&ctx->deferred_forced_edump_timer);
+	del_timer_sync(&ctx->deferred_main_reset_timer);
+	del_timer_sync(&ctx->deferred_ssp_wakeup_timer);
+	del_timer_sync(&ctx->cdu_timeout_timer);
+	del_timer_sync(&ctx->ssp_timer);
+	del_timer_sync(&ctx->temp_throttle_timer);
+	del_timer_sync(&ctx->seg3_started_timer);
+	
+	ctx->timer_quiesce = 0;
+}
+
diff --git drivers/misc/ibm4767/util.c drivers/misc/ibm4767/util.c
new file mode 100755
index 000000000000..918095c72564
--- /dev/null
+++ drivers/misc/ibm4767/util.c
@@ -0,0 +1,1756 @@
+/*************************************************************************
+ *  Filename:util.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:          Common functions to transfer data.            
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_slowops.h"
+#include "y_ioctl.h"
+#include "driver.h"
+#include "y_funcs.h"
+#include "host_err.h"
+
+
+struct list_head *
+list_remove_head(struct list_head *list)
+{
+	struct list_head *lh;
+
+	if (!list)
+		return NULL;
+
+	if (list_empty(list))
+		return NULL;
+
+	lh = list->next;
+	list_del_init(lh);
+
+	return lh;
+}
+
+
+struct list_head *
+list_remove_tail(struct list_head *list)
+{
+	struct list_head *lh;
+
+	if (!list)
+		return NULL;
+
+	if (list_empty(list))
+		return NULL;
+
+	lh = list->prev;
+	list_del_init(lh);
+
+	return lh;
+}
+
+
+static scatter_frag_t *
+alloc_scatterfrag(uint32_t len)
+{
+	scatter_frag_t *frag = NULL;
+	char         *va = NULL;
+	int           order;
+
+	frag = kmalloc(sizeof(scatter_frag_t), GFP_KERNEL);
+	if (!frag) 
+		return NULL;
+
+	INIT_LIST_HEAD(&frag->list_head);
+
+	// while __get_free_pages supports up to order 11, we're going to convert the
+	// frags to DTs which have an upper bound of 1MB.  so we limit ourselves to order 10
+	//
+	order = MIN(10, get_order(len));
+
+	while (order >= 0) {
+		va = (char *)__get_free_pages(GFP_KERNEL, order);
+		if (va) {
+			frag->order  = order;
+			frag->vp.len = PAGE_SIZE * (1 << order);
+			frag->vp.va  = va;
+			frag->vp.dma = 0;
+		
+			memset(va, 0, frag->vp.len);
+			return frag;
+		}
+
+		order--;
+	} 
+
+	kfree(frag);
+
+	PRINTKW("no free pages!\n");
+	return NULL;
+}
+
+
+void
+init_scatterlist(y_scatterlist_t *list)
+{
+#if defined(LINUX)
+	INIT_LIST_HEAD(&list->frags);
+	list->direction = 0;
+#endif
+#if defined(WINDOWS)
+	InitializeListHead(&list->frags);
+#endif
+	list->num       = 0;
+	list->ctx       = NULL;
+}
+
+
+// allocate a scatterlist for 'buffer' and append it to 'list'
+//
+int
+alloc_scatterlist(ibm4767_ctx_t   *ctx, 
+		  char          *buffer, 
+		  uint32_t       buflen, 
+		  uint32_t       total_len, 
+		  int            is_kernel,
+		  y_scatterlist_t *list, 
+		  int            direction)
+{
+	scatter_frag_t *frag = NULL;
+	uint32_t  remain, sz;
+	int       rc = HOST_DD_Good;
+
+	if (!ctx || !list)
+		return HDD_FAIL;
+
+	PDEBUG(3, "Enter %s\n", __FUNCTION__);
+		
+	list->ctx       = ctx;
+#if defined(LINUX)
+	list->direction = direction;
+#endif
+
+	remain = total_len;
+
+	while (remain > 0) {
+		frag = alloc_scatterfrag(remain);
+		if (!frag) {
+			rc = HOST_DD_NoMemory;
+			goto error;
+		}
+
+		//PDEBUG(1, "   needed %d, got %d\n", remain, frag->vp.len);
+
+		// there are three lengths involved here:
+		// frag->vp.len: internal size of the allocated fragment.  this will be
+		//               a multiple of PAGE_SIZE.  linux requires that we keep
+		//               track of this for mapping/unmapping
+		//
+		// frag->len: the fragment size.  this is what will be used when
+		//            preparing DT chains.  
+		//
+		// buflen: represents the size of the src buffer to be copied.
+		//         the quantity (total_len - buflen) represents
+		//         any tail padding that needs to be added.
+		//
+		// note the following relationship:
+		//    frag->vp.len >= frag->len >= buflen
+		//             
+		sz = MIN(frag->vp.len, remain);
+
+		frag->len = sz;
+
+		// buffer might be userspace or kernelspace or it might not exist
+		if (buffer && (buflen > 0)) {
+			uint32_t bytes = MIN(buflen, sz);
+			if (!is_kernel) {
+				rc = copy_from_user(frag->vp.va, buffer, bytes);
+				if (rc) {
+					PRINTKW("copy_from_user for %d bytes failed (rc=%d)\n", bytes, rc);
+					rc = HOST_DD_BadAddress;
+					goto error;
+				}
+			}
+			else {
+				memcpy(frag->vp.va, buffer, bytes);
+			}
+			buffer += bytes;
+			buflen -= bytes;
+		}
+
+		frag->vp.dma = dma_map_single(ctx->dev,
+					      frag->vp.va, 
+					      frag->vp.len, 
+					      direction);
+		if (dma_mapping_error(ctx->dev, frag->vp.dma)) {
+			PRINTKW("error mapping scatter memory\n"); 
+			rc = HOST_DD_NoMemory; 
+			goto error;
+		}
+
+		list_add_tail(&frag->list_head, &list->frags);
+		list->num++;
+
+		remain -= sz;
+	}
+
+	PDEBUG(3, "Leave %s\n", __FUNCTION__);
+	return HOST_DD_Good;
+
+error:
+	free_scatterlist(list);
+	
+	if (frag) {
+		if (frag->vp.va) {
+			free_pages((unsigned long)frag->vp.va, frag->order);
+		}
+		kfree(frag);
+	}
+	
+	PDEBUG(3, "ErrLeave %s\n", __FUNCTION__);
+	return rc;
+}
+
+
+void
+free_scatterlist(y_scatterlist_t *list)
+{
+	struct list_head *pLH, *tmp;
+
+	PDEBUG(3, "Enter %s...\n", __FUNCTION__);
+
+	if (!list || (list->num == 0))  {
+		PDEBUG(3, "Free an empty list!?\n");
+		return;
+	}
+
+	list_for_each_safe(pLH, tmp, &list->frags) {
+		scatter_frag_t *frag = list_entry(pLH, scatter_frag_t, list_head);
+
+		list_del(pLH);
+		if (frag) {
+			if (frag->vp.dma) {
+				dma_unmap_single(list->ctx->dev,
+						 frag->vp.dma,
+						 frag->vp.len,
+						 list->direction);
+				frag->vp.dma = 0;
+			}
+
+			if (frag->vp.va) {
+				free_pages((unsigned long)frag->vp.va, frag->order);
+				frag->vp.va = 0;
+			}
+
+			kfree(frag);
+		}
+	}
+
+	INIT_LIST_HEAD(&list->frags);
+	list->num       = 0;
+	list->direction = 0;
+	list->ctx       = NULL;
+	PDEBUG(3, "Leave %s...\n", __FUNCTION__);
+}
+
+
+void
+merge_scatterlist(y_scatterlist_t *listA, y_scatterlist_t *listB)
+{
+	list_splice_tail(&listB->frags, &listA->frags);
+	listA->num += listB->num;
+}
+
+	
+void
+dump_scatterlist(y_scatterlist_t *list)
+{
+#if defined(DEBUG)
+	struct list_head *pLH, *tmp;
+	int i=0;
+
+	if (!list->num || list_empty(&list->frags)) {
+		PDEBUG(1, "SLIST EMPTY\n");
+		return;
+	}
+	else
+		PDEBUG(2, "SLIST has %d frags\n", list->num);
+
+	list_for_each_safe(pLH, tmp, &list->frags) {
+		scatter_frag_t *frag = list_entry(pLH, scatter_frag_t, list_head);
+		
+		if (frag) 
+			PDEBUG(3, "%d: order %d  %d bytes @ VA %p, LA %16llx\n", i++, frag->order, frag->vp.len, frag->vp.va, frag->vp.dma);
+	}
+#endif
+}
+
+
+void 
+dump_hw_info(ibm4767_ctx_t  *ctx)
+{
+	uint64_t hcsr;
+
+        hcsr  = cpu_to_be64(read64(HCSR(ctx)));
+
+        PRINTKW("ASIC Revision ID:     0x%08x\n", (uint8_t)((hcsr & HCSR_ASIC_REV) >> 8));
+        PRINTKW("Card Revision ID:     0x%08x\n", (uint8_t)(hcsr & HCSR_CARD_REV));
+}
+
+
+void 
+dump_pci_cfg(ibm4767_ctx_t *ctx)
+{
+        int       i;
+        uint32_t  dword[64];
+
+        for (i=0; i < 64; i++)
+                pci_read_config_dword(ctx->pci_dev, i*4, &dword[i]);
+
+        hex_dump(ctx, 1, "PCI config space", &dword[0], 256);
+}
+
+
+// if 'level' == 0, then we perform the hexdump with PRINTKW with no regard for debug level
+// otherwise we use the PDEBUG facility which will generate logs IFF the driver was built 
+// with DEBUG enabled and if debug_level is set sufficiently high
+//
+void
+hex_dump(ibm4767_ctx_t *ctx, uint32_t level, unsigned char *descr, void *data, uint32_t len)
+{
+	uint8_t *p = NULL, *buf = (uint8_t *)data;
+	uint8_t out[128];
+	uint32_t i, j;
+
+	if (level > dbg_level)
+		return;
+
+	if (descr) {
+		if (!level)
+			PRINTKW("%s\n", descr);
+		else
+			PDEBUG(level, "%s\n", descr);
+	}
+
+	if (len == 0) {
+		if (!level)
+			PRINTKW("EMPTY BLOCK\n");
+		else
+			PDEBUG(level, "EMPTY BLOCK\n");
+	}
+
+
+	// this might be tricky to re-implement in Windows...
+
+	i = 0;
+	while (i < len) {
+		memset(out, 0, sizeof(out));
+		sprintf(out, "%04x: ", i);
+		p = out+6;
+
+		for (j=0; (j < 16) && (i < len); j++, i++) {
+			sprintf(p, "%02X", buf[i]);
+			p += 2;
+			if ((j & 3) == 3)
+				sprintf(p++, " ");
+		}
+
+		if (!level) 
+			PRINTKW("%s\n", out);
+		else
+			PDEBUG(level, "%s\n", out);
+	}
+}
+
+
+// be careful if you add additional registers to this routine.  some registers will
+// clear bits when read
+//
+void 
+dump_regs(ibm4767_ctx_t  *ctx)
+{ 
+	PRINTKW("---Register Dump for device %d---\n", ctx->dev_index); 
+
+	PRINTKW("HISR       0x%08x\n",    cpu_to_be32(read32(HISR(ctx))));
+	PRINTKW("HTB_TC     0x%016llx\n", cpu_to_be64(read64(HTB_TC(ctx))));
+	PRINTKW("HTB_WA     0x%016llx\n", cpu_to_be64(read64(HTB_WA(ctx))));
+	PRINTKW("S2H_MBX    0x%016llx\n", cpu_to_be64(read64(S2H_MBX(ctx))));
+	PRINTKW("M2H_MBX    0x%016llx\n", cpu_to_be64(read64(M2H_MBX(ctx))));
+	PRINTKW("HCR        0x%08x\n",    cpu_to_be32(read32(HCR(ctx))));
+	PRINTKW("HCSR       0x%016llx\n", cpu_to_be64(read64(HCSR(ctx))));
+	PRINTKW("HRCSR      0x%08x\n",    cpu_to_be32(read32(HRCSR(ctx))));
+	PRINTKW("HBMCR      0x%08x\n",    cpu_to_be32(read32(HBMCR(ctx))));
+	PRINTKW("HBMSR      0x%016llx\n", cpu_to_be64(read64(HBMSR(ctx))));
+	PRINTKW("HIER       0x%08x\n",    cpu_to_be32(read32(HIER(ctx))));
+	PRINTKW("HMVMC      0x%08x\n",    cpu_to_be32(read32(HMVMC(ctx))));
+	PRINTKW("HDMAERR    0x%08x\n",    cpu_to_be32(read32(H_CPU_DMAERR(ctx))));
+	PRINTKW("H2M_TCP    0x%016llx\n", cpu_to_be64(read64(H2M_TCP(ctx))));
+	PRINTKW("H2S_TCP    0x%016llx\n", cpu_to_be64(read64(H2S_TCP(ctx))));
+	PRINTKW("H2SKA_CTRL 0x%016llx\n", cpu_to_be64(read64(H2SKA_DT_CTRL_REG(ctx))));
+	PRINTKW("H2SKB_CTRL 0x%016llx\n", cpu_to_be64(read64(H2SKB_DT_CTRL_REG(ctx))));
+	PRINTKW("H2SKA_LAST 0x%016llx\n", cpu_to_be64(read64(H2SKA_LAST_DT_PTR(ctx))));
+	PRINTKW("H2SKB_LAST 0x%016llx\n", cpu_to_be64(read64(H2SKB_LAST_DT_PTR(ctx))));
+
+	dump_hw_info(ctx);
+}
+
+
+const char *
+cdu_state_to_str(CDUSTAT state)
+{
+	switch (state) {
+		case CDU_INACTIVE:                       return "Inactive";
+		case CDU_WAIT_FOR_AGENTID_LIST:          return "Wait for AGENTID LIST";
+		case CDU_REMBURN3_NOCDU_NEED_USER_RESET: return "REMBURN3 (NOCDU need user reset)";
+		case CDU_REMBURN3_NOCDU_AUTORESET:       return "REMBURN3 (NOCDU autoreset)";
+		case CDU_REMBURN3_CDU:                   return "REMBURN3 (CDU)";
+		case CDU_WAIT_FOR_READY_TO_PROCEED:      return "Wait for READY_TO_PROCEED";
+		case CDU_WAIT_FOR_PROCEED_ACK:           return "Wait for PROCEED_ACK";
+	}
+
+	return NULL;
+}
+
+
+const char *
+mcpu_state_to_str(MCPUSTAT state)
+{
+	switch (state) { 
+		case MCPU_UNINITIALIZED:    return "Uninitialized";
+		case MCPU_IN_RESET:         return "In Reset";
+		case MCPU_BOOTING:          return "Booting";
+		case MCPU_POST2_START:      return "POST2 _START";
+		case MCPU_POST2_HELLO:      return "POST2 _HELLO";
+		case MCPU_POST2_END:        return "POST2 _END";
+#if defined(ENABLE_MFG)
+		case MCPU_POST2_SERIAL_DBG: return "POST2 Serial DBG";
+		case MCPU_IBM_TAS_START:    return "IBM TAS _START";
+		case MCPU_IBM_TAS_ACTIVE:   return "IBM TAS ACTIVE";
+		case MCPU_IBM_TAPP_START:   return "IBM TAPP _START";
+		case MCPU_IBM_TAPP_ACTIVE:  return "IBM TAPP ACTIVE";
+#endif
+		case MCPU_INITIALIZED:      return "Initialized";
+		case MCPU_ACTIVE:           return "Active";
+		case MCPU_OFFLINE:          return "OFFLINE";
+		case MCPU_TAMPER:           return "TAMPER";
+		case MCPU_SOFT_TAMPER:      return "SOFT TAMPER";
+		case MCPU_TEMP_SHUTDOWN:    return "TEMPERATURE SHUTDOWN";
+	}
+
+	return NULL;
+}
+
+
+
+const char *
+ssp_state_to_str(SSPSTAT state)
+{
+	switch (state) {
+		case SSP_UNINITIALIZED:        return "Uninitialized";
+		case SSP_WAIT_FOR_SSPWUR:      return "Wait for SSP WUR";
+		case SSP_WAIT_FOR_MBHALT:      return "Wait for MBHALT";
+		case SSP_WAIT_FOR_POST0:       return "Wait for POST0";
+		case SSP_WAIT_FOR_POST0_END:   return "Wait for POST0 END";
+		case SSP_WAIT_FOR_MB0_START:   return "Wait for MB0 _START";
+		case SSP_WAIT_FOR_MB0_HELLO:   return "Wait for MB0 _HELLO";
+		case SSP_MB0:                  return "MB0";
+		case SSP_WAIT_FOR_MB0_END:     return "Wait for MB0 _END";
+		case SSP_WAIT_FOR_POST1_START: return "Wait for POST1 _START";
+		case SSP_WAIT_FOR_POST1_HELLO: return "Wait for POST1 _HELLO";
+		case SSP_POST1_HELLO:          return "POST1 _HELLO";
+		case SSP_WAIT_FOR_POST1_END:   return "Wait for POST1 _END";
+#if defined(ENABLE_MFG)
+		case SSP_POST1_SERIAL_DBG:     return "POST1 Serial DBG";
+		case SSP_IBM_TAS_START:        return "IBM TAS _START";
+		case SSP_IBM_TAS_ACTIVE:       return "IBM TAS ACTIVE";
+		case SSP_IBM_TAPP_START:       return "IBM TAPP _START";
+		case SSP_IBM_TAPP_ACTIVE:      return "IBM TAPP ACTIVE";
+#endif
+		case SSP_WAIT_FOR_MB1_START:   return "Wait for MB1 _START";
+		case SSP_WAIT_FOR_MB1_HELLO:   return "Wait for MB1 _HELLO";
+		case SSP_MB1:                  return "MB1";
+		case SSP_WAIT_FOR_MB1_END:     return "Wait for MB1 _END";
+		case SSP_MB_DONE:              return "MB Done";
+		case SSP_OFFLINE:              return "OFFLINE";
+		case SSP_TAMPER:               return "TAMPER";
+		case SSP_SOFT_TAMPER:          return "SOFT TAMPER";
+		case SSP_TEMP_SHUTDOWN:        return "TEMPERATURE SHUTDOWN";
+	}
+
+	return NULL;
+}
+
+
+const char *
+tamper_state_to_str(TAMPERSTAT state)
+{ 
+	switch (state) {
+		case TAMPER_NONE:       return "No Tamper";
+		case TAMPER_PERM:       return "Permanent";
+		case TAMPER_UNKNOWN:    return "Other";
+		case TAMPER_SOFT_TEMP:  return "Soft Tamper - Temperature";
+		case TAMPER_SOFT_VOLT:  return "Soft Tamper - Voltage";
+		case TAMPER_SOFT_INJ:   return "Soft Tamper - INJ";
+		case TAMPER_SOFT_OTHER: return "Soft Tamper - Other";
+	}
+
+	return NULL;
+}
+
+
+int 
+ibm4767_read8_indirect(ibm4767_ctx_t *ctx, uint8_t offset, uint8_t *value)
+{
+	uint32_t tmp;
+	uint8_t  *p = (uint8_t *)&tmp;
+	int count;
+	int rc = HOST_DD_Good;
+	
+	// 1) write the address offset to HTS_AP
+	// 2) poll HTS_AP for HTS_DONE (or HTS_ERROR)
+	// 3) read the data from HTS_DP
+
+	spin_lock_bh(&ctx->indirect_lock);
+
+	// for 4767, offset needs to be 32b boundary so we round down to the nearest 32b
+	// boundary and read 32b.  later we'll extract the requested byte
+	//
+	tmp = offset & ~0x3;
+	write32(cpu_to_be32(tmp), HTS_AP(ctx));
+
+	mdelay(1);
+
+	// 12ms is rather arbitrary
+	count = 12;
+	do {
+		tmp = cpu_to_be32(read32(HTS_AP(ctx)));
+		if (tmp & (HTS_DONE | HTS_ERROR)) 
+			break;
+		mdelay(1);
+		count--;
+	} while (count);
+	
+	if (tmp & HTS_ERROR) {
+		// FIXME - should we emit an error message here?
+		PRINTKW("Device %d: HTS_AP indicates error\n", ctx->dev_index);
+		rc = HDD_FAIL;
+		goto done;
+	}
+
+	if (!count) {
+		// FIXME - should we emit an error message here?
+		PRINTKW("Device %d: timeout waiting for HTS_READY\n", ctx->dev_index);
+		rc = HOST_DD_Timeout;
+		goto done;
+	}
+
+
+	// now extract the requested byte from the 4B value that was retrieved...
+	// note: we do NOT byte reverse for little endian
+	tmp = read32(HTS_DP(ctx));
+	*value = p[offset & 0x3];
+
+done:
+	spin_unlock_bh(&ctx->indirect_lock);
+	return rc;
+}
+
+
+int 
+ibm4767_read32_indirect(ibm4767_ctx_t *ctx, uint8_t offset, uint32_t *value)
+{
+	uint32_t tmp;
+	int count;
+	int rc = HOST_DD_Good;
+	
+	// 1) write the address offset to HTS_AP
+	// 2) poll HTS_AP for HTS_DONE (or HTS_ERROR)
+	// 3) read the data from HTS_DP
+
+	spin_lock_bh(&ctx->indirect_lock);
+
+	// for 4767, offset needs to be 32b boundary so we round down to the nearest 32b
+	// boundary and read 32b.  
+	//
+	tmp = offset & ~0x3;
+	write32(cpu_to_be32(tmp), HTS_AP(ctx));
+
+	mdelay(1);
+
+	// 12ms is rather arbitrary
+	count = 12;
+	do {
+		tmp = cpu_to_be32(read32(HTS_AP(ctx)));
+		if (tmp & (HTS_DONE | HTS_ERROR)) 
+			break;
+		mdelay(1);
+		count--;
+	} while (count);
+	
+	if (tmp & HTS_ERROR) {
+		// FIXME - should we emit an error message here?
+		PRINTKW("Device %d: HTS_AP indicates error\n", ctx->dev_index);
+		rc = HDD_FAIL;
+		goto done;
+	}
+
+	if (!count) {
+		// FIXME - should we emit an error message here?
+		PRINTKW("Device %d: timeout waiting for HTS_READY\n", ctx->dev_index);
+		rc = HOST_DD_Timeout;
+		goto done;
+	}
+
+
+	// now extract the requested byte from the 4B value that was retrieved...
+	// note: we do NOT byte reverse for little endian
+	*value = read32(HTS_DP(ctx));
+
+done:
+	spin_unlock_bh(&ctx->indirect_lock);
+	return rc;
+}
+
+
+int
+ibm4767_query_vpd(ibm4767_ctx_t *ctx, xcVpd_t *vpd)
+{
+	unsigned char *p = (unsigned char *)vpd;
+	uint32_t vpd_cap, tmp32, off, delay;
+	unsigned long jif = jiffies;
+	uint16_t tmp16;
+
+	if (!ctx)
+		return HOST_DD_NoDevice;
+
+	memset(vpd, 0x0, sizeof(xcVpd_t));
+
+	// retrieve base addr of PCI config space
+	vpd_cap = pci_find_capability(ctx->pci_dev, PCI_CAP_ID_VPD);
+	if (!vpd_cap) {
+		PRINTKW("Device %d: Could not find PCIe VPD capability!\n", ctx->dev_index);
+		return -EIO;
+	}
+	
+	PDEBUG(1, "Device: %d, VPD capability found at 0x%0x\n", ctx->dev_index, vpd_cap);
+
+	// to retrieve each 4-byte chunk:
+	//    - write the target address to the VPDAR (make sure high bit is 0)
+	//    - read VPDAR until the high bit is 1
+	//    - read 4 bytes from the VPDDR
+	//
+	// hardware timing bug means that sometimes PCI_VPD_DATA will contain 0x0 even if
+	// PCI_VPD_ADDR_F says data is waiting.  75% chance of 0x0 at PCIe gen1 speeds,
+	// 50% chance at gen2 speeds.  so we'll need to retry until we either get good
+	// data or time-out...
+	//
+	// since only a portion of the 256-byte VPD area is used, the reserved bytes will
+	// contain 0x0 (Nihad said he'll change this).  in the meantime, we stop reading
+	// at 'rv_length'
+	//
+	for (off = 0; off < offsetof(xcVpd_t, rv_length); off += 4) {
+		pci_write_config_word(ctx->pci_dev, PCI_VPD_ADDR + vpd_cap, off);
+		delay = 12;
+		do {
+			pci_read_config_word(ctx->pci_dev, PCI_VPD_ADDR + vpd_cap, &tmp16);
+
+			if (tmp16 & PCI_VPD_ADDR_F)
+				break;
+
+			mdelay(1);
+			delay--;
+		} while (delay);
+
+		if (!delay) {
+			PDEBUG(1, "Device %d: timeout querying VPD\n", ctx->dev_index);
+			return HOST_DD_Timeout;
+		}
+
+		// 4 bytes ready to read.  no guarantee on alignment of the local buffer
+		// so we use a DWORD-aligned intermediate buffer.  note: the VPD is in little
+		// endian format already so there's no need to byte reverse...
+		//
+		pci_read_config_dword(ctx->pci_dev, PCI_VPD_DATA + vpd_cap, &tmp32);
+		if (tmp32 == 0xFFFFFFFF) {
+			PDEBUG(1, "Device %d: VPD read returned 0xFFFFFFFF.  Assuming empty VPD...\n", ctx->dev_index);
+			return HOST_DD_Timeout;
+		}
+		if (tmp32 == 0) {
+			// arbitrarily allow 2 seconds to read the VPD...
+			//
+			if (jiffies < (jif + 2*HZ)) {
+				off -= 4;
+				continue;
+			}
+			else {
+				PDEBUG(1, "Device %d: timeout querying VPD\n", ctx->dev_index);
+				return HOST_DD_Timeout;
+			}
+		}
+
+		if (ctx->hwinfo.asic_rev == ASIC_REV_DD1)
+			tmp32 = be32_to_cpu(tmp32);
+
+		memcpy(p + off, &tmp32, 4); 
+	}
+
+	return HOST_DD_Good;
+}
+
+
+void 
+ibm4767_dump_tamper_regs(ibm4767_ctx_t *ctx)
+{
+	uint64_t val64;
+	uint8_t  val8;
+
+	if (allow_indirect) {
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_TMPR_LATCH_A, &val8))
+			PRINTKE("Tamper Latch 1: 0x%02x\n", val8);
+		else
+			PRINTKE("Tamper Latch 1: N/A\n");
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_TMPR_LATCH_B, &val8))
+			PRINTKE("Tamper Latch 2: 0x%02x\n", val8);
+		else
+			PRINTKE("Tamper Latch 2: N/A\n");
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_TMPR_LATCH_C, &val8))
+			PRINTKE("Tamper Latch 3: 0x%02x\n", val8);
+		else
+			PRINTKE("Tamper Latch 3: N/A\n");
+
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_VCCI, &val8)) 
+			PRINTKE("ADC Vcci:  0x%02X\n", val8); 
+		else 
+			PRINTKE("ADC Vcci:  N/A\n");
+		
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_VBAT, &val8)) 
+			PRINTKE("ADC VBAT:  0x%02X\n", val8); 
+		else
+			PRINTKE("ADC VBAT:  N/A\n");
+		
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_VREF, &val8))
+			PRINTKE("ADC VREF:  0x%02X\n", val8);
+		else
+			PRINTKE("ADC VREF:  N/A\n");
+		
+		if (!ibm4767_read8_indirect(ctx, INDIRECT_DS3645_ADC_TEMP, &val8))
+			PRINTKE("ADC Temp:  0x%02X\n", val8);
+		else
+			PRINTKE("ADC Temp:  N/A\n");
+	}
+	else {
+		PRINTKE("Can't read tamper latches because indirect register operations are disabled\n");
+	}
+
+	val64 = cpu_to_be64(read64(HCSR(ctx)));
+
+	PRINTKE("Battery Status: 0x%02x\n",   (uint8_t)((val64 & HCSR_BATTERY)   >> 54));
+	PRINTKE("PM Temp: 0x%02x\n",          (uint8_t)((val64 & HCSR_CURR_TEMP) >> 40));
+	PRINTKE("Soft Tamper Temp: 0x%02x\n", (uint8_t)((val64 & HCSR_TAMPER_TEMP) >> 32));
+}
+
+
+void
+mark_device_offline(ibm4767_ctx_t *ctx, int code)
+{
+        PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+	
+	del_timer_sync(&ctx->seg3_started_timer);
+
+	// early in development POST had a habit of flooding the host with edump interrupts
+	// but the POST guys assure me that this has been fixed.  if it happens again, just
+	// disable ints...
+	//write32(0, HIER(ctx));
+
+	dma_disable(ctx, HBMCR_BMEN_ALL);
+        ctx->status_mcpu = MCPU_OFFLINE;
+        ctx->status_ssp  = SSP_OFFLINE;
+        ctx->status_cdu  = CDU_INACTIVE;
+        ctx->status_fp   = FP_OFFLINE;
+        flush_all_requests(ctx, code);
+}
+
+
+void
+mark_fp_offline(ibm4767_ctx_t *ctx, int code)
+{
+        PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+
+        dma_disable(ctx, HBMCR_BMEN_FP);
+        ctx->status_fp = FP_OFFLINE;
+        flush_fp_request_queue(ctx, code);
+}
+
+
+void
+mark_mcpu_offline(ibm4767_ctx_t *ctx, int code)
+{
+	//uint32_t hier = read32(HIER(ctx));
+
+        PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+
+	// if CDU handshake has started, we want to abort the CDU.  i don't like having
+	// MCPU-related routines mucking with SSP-related requests and state but we have to do this
+	//
+	ibm4767_cdu_finalize(ctx, CDU_ERR_MCPU_FAIL);
+	
+	del_timer_sync(&ctx->seg3_started_timer);
+
+        dma_disable(ctx, HBMCR_BMEN_MCPU);
+        ctx->status_mcpu = MCPU_OFFLINE;
+        flush_mcpu_request_queue(ctx, code);
+
+	// this guy might get called from a timer so be careful about deleting timers...
+	//
+}
+
+
+void
+mark_ssp_offline(ibm4767_ctx_t *ctx, int code)
+{
+        PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+
+	// can't disable SSP mailboxes because we may still receive an edump notice
+
+        dma_disable(ctx, HBMCR_BMEN_SSP);
+
+        ctx->status_ssp = SSP_OFFLINE;
+
+	// when MB1 halts following a successful REMBURN3, we don't want to abort
+	// the ensuing CDU...
+	//
+	if (code) {
+        	ctx->status_cdu = CDU_INACTIVE;
+	}
+
+        flush_ssp_request_queue(ctx, code);
+	
+	// this guy might get called from a timer so be careful about deleting timers...
+	//
+}
+
+
+//
+// reset mechanisms
+//
+
+
+// if you're looking for a way to reset the card, you most likely want to use defer_main_reset.
+// 
+void 
+defer_main_reset(ibm4767_ctx_t *ctx, uint32_t secs, int force)
+{
+	PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+
+	// if the HW team is tracing a problem, an automatic reset will cause
+	// problems.  the user can still force a card reset by using the RESET ioctl
+	// or by using the /proc/driver/ibm4767/reset mechanism...
+	//
+	if (!auto_reset && !force) {
+		PRINTKW("auto_reset=0 and !force.  card %d reset aborted\n", ctx->dev_index);
+		return;
+	}
+
+	if (ctx) {
+		spin_lock_bh(&ctx->counter_lock);
+		ctx->status_flags |= FLAG_MAIN_RESET_PENDING;
+		spin_unlock_bh(&ctx->counter_lock);
+		mod_timer(&ctx->deferred_main_reset_timer, jiffies + HZ*secs);
+	}
+}
+
+
+void
+cancel_deferred_main_reset(ibm4767_ctx_t *ctx)
+{
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->status_flags &= ~FLAG_MAIN_RESET_PENDING;
+	spin_unlock_bh(&ctx->counter_lock);
+	del_timer_sync(&ctx->deferred_main_reset_timer);
+}
+
+
+int
+ibm4767_main_reset(ibm4767_ctx_t *ctx, int flush)
+{
+	uint32_t hrcsr, is_tampered = 0;
+	
+	PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->status_flags |= FLAG_MAIN_RESET_ACTIVE;
+	ctx->status_flags &= ~FLAG_MAIN_RESET_PENDING;
+	spin_unlock_bh(&ctx->counter_lock);
+
+
+	// make sure we've cancelled all applicable timers
+	// don't try to cancel the deferred_main_reset_timer since there's
+	// a fair chance that it's the guy who invoked us and will still be on 
+	// the stack...
+	ctx->timer_quiesce = TIMER_QUIESCE_ALL;
+	
+	// don't try to cancel the deferred_main_reset_timer since there's
+	// a fair chance that it's the guy who invoked us and will still be on 
+	// the stack...
+	//del_timer_sync(&ctx->deferred_main_reset_timer);
+
+	del_timer_sync(&ctx->agentid_list_timer);
+	del_timer_sync(&ctx->deferred_buf_update_timer);
+	del_timer_sync(&ctx->cdu_timeout_timer);
+	del_timer_sync(&ctx->deferred_forced_edump_timer);
+	del_timer_sync(&ctx->deferred_ssp_wakeup_timer);
+	del_timer_sync(&ctx->ssp_timer);
+	del_timer_sync(&ctx->temp_throttle_timer);
+	del_timer_sync(&ctx->seg3_started_timer);
+
+	ctx->work_quiesce = WORK_QUIESCE_ALL;
+	// don't try to cancel the main_reset work thread since there's a fair chance
+	// that it's the guy who invoked us and will still be on the stack...
+	//
+	cancel_work_sync(&ctx->hra_pool_work);
+	cancel_work_sync(&ctx->post0_edump_work);
+	cancel_work_sync(&ctx->post2_edump_work);
+	cancel_work_sync(&ctx->seg2_edump_work);
+#if defined(ENABLE_MFG)
+	cancel_work_sync(&ctx->mcpu_tas_edump_work);
+	cancel_work_sync(&ctx->ssp_tas_edump_work);
+#endif
+
+	hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+	if (!ignore_tamper && (hrcsr & HRCSR_PCT)) {
+		ctx->status_tamper = TAMPER_PERM;
+		is_tampered = 1;
+	}
+	else {
+		// if it's a soft tamper then if we're here then HRCSR::STRst must have gone 
+		// back to 0 so the tamper condition has gone away.  if not, we'll get another
+		// tamper interrupt following the reset...
+		ctx->status_tamper = TAMPER_NONE;
+
+		ctx->status_mcpu = MCPU_UNINITIALIZED;
+		ctx->status_ssp  = SSP_UNINITIALIZED;
+		ctx->status_fp   = FP_OFFLINE;
+	}
+
+	dma_disable(ctx, HBMCR_BMEN_ALL);
+
+	if (flush)
+		flush_all_requests(ctx, HOSTAborted);
+
+	spin_lock_bh(&ctx->fetch_lock_fpgaA);
+	if (ctx->last_DT_FPGA_A) {
+		dt_append(&ctx->dt_pool, ctx->last_DT_FPGA_A);
+		ctx->last_DT_FPGA_A = NULL;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_fpgaA);
+
+	spin_lock_bh(&ctx->fetch_lock_fpgaB);
+	if (ctx->last_DT_FPGA_B) {
+		dt_append(&ctx->dt_pool, ctx->last_DT_FPGA_B);
+		ctx->last_DT_FPGA_B = NULL;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_fpgaB);
+
+	spin_lock_bh(&ctx->fetch_lock_skch);
+	if (ctx->last_DT_SKCH) {
+		dt_append(&ctx->dt_pool, ctx->last_DT_SKCH);
+		ctx->last_DT_SKCH = NULL;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_skch);
+
+	spin_lock_bh(&ctx->fetch_lock_pka);
+	if (ctx->last_DT_PKA) {
+		dt_append(&ctx->dt_pool, ctx->last_DT_PKA);
+		ctx->last_DT_PKA = NULL;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_pka);
+
+	spin_lock_bh(&ctx->fetch_lock_ssp);
+	if (ctx->last_DT_SSP) {
+		dt_append(&ctx->dt_pool, ctx->last_DT_SSP);
+		ctx->last_DT_SSP = NULL;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_ssp);
+
+	spin_lock_bh(&ctx->fetch_lock_mcpu);
+	if (ctx->last_DT_MCPU) {
+		dt_append(&ctx->dt_pool, ctx->last_DT_MCPU);
+		ctx->last_DT_MCPU = NULL;
+	}
+	spin_unlock_bh(&ctx->fetch_lock_mcpu);
+
+	ctx->hwinfo.post0_ver = 0xFFFFFFFF;
+	ctx->hwinfo.post1_ver = 0xFFFFFFFF;
+	ctx->hwinfo.post2_ver = 0xFFFFFFFF;
+	ctx->hwinfo.mb0_ver   = 0xFFFFFFFF;
+	ctx->hwinfo.mb1_ver   = 0xFFFFFFFF;
+	ibm4767_get_hwinfo(ctx);
+
+	ctx->timer_quiesce = 0;
+	ctx->work_quiesce = 0;
+
+	ibm4767_reset_via_hrcsr(ctx);
+	
+	// i worry that setting the timestamp to 0 will have unintended consequences
+	// if the kernel has just booted...probably safe to set it to the current
+	// timestamp instead since it'll take much longer than 2 seconds to reach seg2...
+	//
+	ctx->lastlog_timestamp = jiffies;
+
+	atomic_set(&ctx->int_state_wr_next, 0);
+	atomic_set(&ctx->int_state_rd_next, 0);
+
+#if defined(ENABLE_MFG)
+	ctx->ssp_mbxhist_r_idx = 0;
+	ctx->ssp_mbxhist_w_idx = 0;
+	ctx->mcpu_mbxhist_r_idx = 0;
+	ctx->mcpu_mbxhist_w_idx = 0;
+#endif
+	// at this point, the card has reset and the SSP is most likely awake and possibly
+	// running POST0.  it's possible that the ISR has already handled a POST0 error 
+	// interrupt.  if so, don't proceed any further...
+	//
+	if (ctx->status_ssp == SSP_OFFLINE) 
+		goto done;
+
+	// if there has been a real tamper, MB0 will not run so the SSP timeout
+	// will fire.  to avoid this, don't set the timer if HRCSR::PCT is set
+	//
+	// furthermore...when there's a temperature soft tamper, all cards are reset and 
+	// marked offline (to prevent code running on the MCPU from continuing to 
+	// heat the card(s)).  the cards remain offline until the driver is restarted so
+	// we don't start the SSP timer...
+	//
+	if (!is_tampered && (atomic_read(&temp_soft_tamper_detected) == 0)) {
+		mod_timer(&ctx->ssp_timer,           jiffies + HZ*timeout_ssp);
+		mod_timer(&ctx->temp_throttle_timer, jiffies + HZ);
+		mod_timer(&ctx->seg3_started_timer,  jiffies + HZ);
+	
+		ctx->status_mcpu = MCPU_BOOTING;
+		ctx->status_ssp  = SSP_WAIT_FOR_POST0;
+		ctx->status_cdu  = CDU_INACTIVE;
+		ctx->status_fp   = FP_OFFLINE;
+	}
+
+	// in the 4765 universe, we could initialize DMA here.  for 4767, the device DMA stuff
+	// will be held in reset until explicitly released by firmware.  until this happens, 
+	// any writes by the host driver to DMA registers like HTB_WA will be quietly ignored...
+	// so we'll have to enable DMA in the mailbox handlers...
+	//
+	//htb_swap(ctx);
+	ctx->htb_active = 0xFF;
+
+	// strange...if we don't do this then we'll generate an AER or NMI almost immediately...
+	dma_enable(ctx, HBMCR_BMEN_SSP | HBMCR_HTB_BMEN | HBMCR_BMEN_MCPU);
+	
+done:
+	atomic_set(&ctx->ebusy_counter, 0);
+	atomic_set(&ctx->mcpu_counter, 0);
+	atomic_set(&ctx->mrb0_counter, 0);
+	atomic_set(&ctx->mrb1_counter, 0);
+	atomic_set(&ctx->ssp_counter, 0);
+	atomic_set(&ctx->fp_counter, 0);
+	atomic_set(&ctx->ioctl_counter, 0);
+
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->status_flags &= ~FLAG_MAIN_RESET_ACTIVE;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	ctx->ebusy_resetmsg_count = 0;
+	ctx->reqfail_msg_count = 0;
+
+	PDEBUG(1, "Leaving %s...\n", __FUNCTION__);
+
+	return HOST_DD_Good;
+}
+
+
+// perform the low-level main reset sequence via HRCSR::HOST_RESET.  it's invoked by ibm4767_main_reset
+//
+// it's difficult to imagine a scenario where you'd want to call this routine directly...
+//
+void 
+ibm4767_reset_via_hrcsr(ibm4767_ctx_t *ctx)
+{
+	uint32_t hisr, count;
+
+#ifdef DEBUG
+	PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+	if (dbg_level > 0)
+		dump_regs(ctx);
+	dump_pci_cfg(ctx);
+#endif
+
+	// set the reset bit, wait 1sec, then clear the bit...
+	//
+	write32(cpu_to_be32(HRCSR_HOST_RESET), HRCSR(ctx)); 
+	wmb(); 
+	mdelay(1000); 
+	write32(cpu_to_be32(0), HRCSR(ctx)); 
+	wmb(); 
+
+	// in general, we should be able to READ registers as soon as we release reset
+	// but since we want to WRITE the HIER, we need to wait until various components
+	// wake up.  the hardware guys are working on a more elegant solution but for
+	// now we can poll H2M_DT_CTRL_REG::H2M_DT_SIGN for 0xD64D...
+	//
+	// update for DD2: it seems that post-reset register behavior is different for DD2 and
+	// this polling loop no longer works.  it seems that H2M_DT_CTRL_REG always contains
+	// a valid value even when the hardware is still in reset.  so we'll have to resort to
+	// blindly delaying.  yuck.
+	//
+	count = 0;
+	while (1) {
+		if ((cpu_to_be64(read64(H2M_DT_CTRL_REG(ctx))) & 0xFFFF) == 0xD64D)
+			break;
+
+		if (++count > 50) {
+			PRINTKW("Device %d: Bad.  Waited 250ms for DT_CTRL_REG to come out of reset...\n", ctx->dev_index);
+			break;
+		}
+
+		mdelay(5);
+	}
+
+	mdelay(25);
+
+	// for DD1, we need to read the HISR to clear out any self-resetting bits.
+	// otherwise we might see an anomalous DMA interrupt following the reset...
+	//
+	{
+		uint64_t hcsr = cpu_to_be64(read64(HCSR(ctx)));
+		if (((hcsr & HCSR_ASIC_REV) >> 8) == ASIC_DD_10) {
+			PRINTKW("Device %d:  ASIC 1.0 detected.  Clearing HISR...\n", ctx->dev_index);
+			hisr = read32(HISR(ctx));
+		}
+	}
+
+	if (atomic_read(&temp_soft_tamper_detected) != 0) {
+		PDEBUG(1, "%s: soft tamper detected.  setting HIER to 0x0\n", __FUNCTION__);
+		write32(cpu_to_be32(0L), HIER(ctx));
+	}
+	else {
+		PDEBUG(1, "%s: setting HIER to 0x%08lx\n", __FUNCTION__, HIER_INITIALIZE);
+		write32(cpu_to_be32(HIER_INITIALIZE), HIER(ctx));
+		mdelay(5);
+	}
+
+	PDEBUG(1, "Leaving %s...\n", __FUNCTION__);
+}
+
+
+// in principle, we can force a PCI "hot reset" by sending a secondary bus reset to the
+// parent bridge.  this might not be desirable, though, if there are multiple devices
+// sharing the bridge (is it possible to have BRIDGE --> PCIe switch --> multiple devices?)
+//
+void 
+ibm4767_reset_via_hot_reset(ibm4767_ctx_t *ctx)
+{
+	struct pci_dev *dev = ctx->pci_dev;
+	struct pci_bus *bus = dev->bus;
+	unsigned char ctl;
+	int    i;
+
+	PDEBUG(1, "Device %d: %s\n", ctx->dev_index, __FUNCTION__);
+
+	// save the config space...
+	//
+	for (i=0; i < 16; i++) {
+		pci_read_config_dword(dev, i*4, &dev->saved_config_space[i]);
+	}
+
+	bus = dev->bus;
+	pci_read_config_byte(bus->self, PCI_BRIDGE_CONTROL, &ctl);
+	ctl |= PCI_BRIDGE_CTL_BUS_RESET;
+	pci_write_config_byte(bus->self, PCI_BRIDGE_CONTROL, ctl);
+
+	// kernel guys do 100ms.  who am I to disagree?
+	msleep(100);
+
+	ctl &= ~PCI_BRIDGE_CTL_BUS_RESET;
+	pci_write_config_byte(bus->self, PCI_BRIDGE_CONTROL, ctl);
+
+	msleep(100);
+
+	for (i=0; i < 16; i++) {
+		pci_write_config_dword(dev, i*4, dev->saved_config_space[i]);
+	}
+}
+
+
+// Reset the SSP
+//
+// if HRCSR indicates that MCPU is under reset or this is a cold boot then we
+// reset the whole card.  Otherwise it's tricky:  
+//
+// As with 4765, SSP reset is actually a two-step process:
+//    1. halt the SSP 
+//    2. issue a wake-up request
+//
+// 4767 has a couple hardware bugs that make it risky for a host driver to drop
+// the SSP into reset using HRCSR's high-priority reset.  Instead, the SSP needs
+// to go into reset under its own control.  So we either need to wait for the SSP
+// to halt on its own or do something that causes the SSP software to halt early.
+//
+// If the SSP is running MB0 or MB1 then if we send a bogus mailbox command.
+// MB will emit a _FAIL notification and halt the SSP.  The driver just needs to
+// make sure the _FAIL doesn't accidentally cause pending requests to fail.
+//
+// Another 4767 bug exists that causes the hardware to ignore any SSP wake-up
+// request unless the SSP is halted.  So the host driver creates a timer
+// to poll the SSP reset status.  When the timer detects the SSP has finally halted,
+// it will issue a wake-up request.
+//
+// So...
+//
+// 1. set up the timer
+// 2. if we're running MB0 or MB1, send a bogus command
+// 3. if/when the SSP halts, the timer will trigger the wake-up.
+//
+void 
+ibm4767_reset_ssp(ibm4767_ctx_t *ctx, int deferred)
+{
+	uint32_t hrcsr;
+
+	PDEBUG(1, "Device %d:  %s\n", ctx->dev_index, __FUNCTION__);
+
+	if (promote_ssp_reset) {
+		PDEBUG(1, "%s:  promote_ssp_reset != 0.  promoting SSP to main reset\n", __FUNCTION__);
+		ibm4767_main_reset(ctx, FALSE);
+		return;
+	}
+
+	hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+	if ((!(hrcsr & HRCSR_SWRMBOOT)) || (hrcsr & HRCSR_MCPU_RstS) || (ctx->status_mcpu == MCPU_OFFLINE)) {
+		PDEBUG(1, "Device %d: HRCSR shows cold boot or MCPU in reset or offline.  Invoking main reset instead\n", ctx->dev_index);
+
+		// in general, we want main resets to flush all requests but in this case, flushing
+		// might cancel an SSP request that just forced an SSP reset to get to the proper state
+		//
+		// this routine might be running in interrupt context so we don't want to
+		// call main_reset() directly (because it needs to free the IRQ if MSI-X mode)
+		if (deferred) {
+			defer_main_reset(ctx, 2, FALSE);
+		}
+		else {
+			ibm4767_main_reset(ctx, FALSE);
+		}
+		return;
+	}
+
+	// Set the SSP reset pending flag.  It's cleared when POST0 _START is received
+	//
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->status_flags |= FLAG_SSP_RESET_PENDING;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	dma_disable(ctx, HBMCR_BMEN_SSP);
+	del_timer_sync(&ctx->ssp_timer);
+
+	if ((ctx->status_ssp == SSP_MB0) || (ctx->status_ssp == SSP_MB1)) {
+		ibm4767_ssp_mbr(ctx);
+	}
+
+	ibm4767_ssp_wur(ctx);
+}
+
+
+// this routine is called from the MB0 and MB1 _HELLO mbx notification handlers.
+// it checks to see if we'd like to reset the SSP and, if so, kick-starts the process
+// by triggering an MBR while Miniboot is running its command-loop
+//
+// returns:
+//    0 : neither an SSP reset nor WUR was pending
+//    1 : an SSP WUR or reset is pending or in progress
+//
+int
+ibm4767_check_ssp_reset_pending(ibm4767_ctx_t *ctx)
+{
+	spin_lock_bh(&ctx->counter_lock);
+	if (ctx->status_flags & FLAG_SSP_WUR_PENDING) {
+		// we should never reach this.  if WUR_PENDING is set then the SSP was
+		// in reset and we issued a WUR to the hardware.  this flag should have
+		// been cleared at POST0 _START
+		//
+		PRINTKW("Device %d: WUR_PENDING inside %s!\n", ctx->dev_index, __FUNCTION__);
+		spin_unlock_bh(&ctx->counter_lock);
+		return 1;
+	}
+	else if (ctx->status_flags & FLAG_SSP_RESET_PENDING) {
+		spin_unlock_bh(&ctx->counter_lock);
+		if ((ctx->status_ssp == SSP_MB0) || (ctx->status_ssp == SSP_MB1)) {
+			ibm4767_ssp_mbr(ctx);
+		}
+		return 1;
+	}
+	spin_unlock_bh(&ctx->counter_lock);
+
+	return 0;
+}
+
+
+int
+ibm4767_check_mbfail_expected(ibm4767_ctx_t *ctx)
+{
+	int rc;
+
+        spin_lock_bh(&ctx->counter_lock);
+        rc = (ctx->status_flags & FLAG_MBFAIL_EXPECTED) ? 1 : 0;
+        spin_unlock_bh(&ctx->counter_lock);
+
+	return rc;
+}
+
+
+void
+ibm4767_ssp_mbr(ibm4767_ctx_t *ctx)
+{
+	PDEBUG(2, "Device %d: issuing SSP MBR\n", ctx->dev_index);
+
+	// An invalid command sent to the H2S mailbox will cause MB0 and MB1
+	// to halt with an error.  At this point, the driver can issue a WUR
+	// (either directly or via the deferred SSP wakeup timer).
+	//
+	// Make sure we only do this if MB0 or MB1 cmd handlers are running.
+	// Undefined results may occur if something else is running on the SSP
+	// when we send the garbage mailbox...
+	//
+	// it's up to the caller to make sure we're in MB0 or MB1...
+	//
+	// this flag will be cleared when we receive the POST0 _START notification
+	//
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->status_flags |=  FLAG_MBFAIL_EXPECTED;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	ctx->status_ssp = SSP_WAIT_FOR_MBHALT;
+	write32(cpu_to_be32(0xABCDEF01L), H2S_MBX(ctx));
+}
+
+
+void
+ibm4767_ssp_wur(ibm4767_ctx_t *ctx)
+{
+	uint32_t  hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+
+	// 4767 hardware bug won't queue a WUR submitted while SSP is awake
+	//
+	if (!(hrcsr & HRCSR_SSP_RstS)) {
+		PDEBUG(1, "Device %d: %s called but SSP is awake.  Seeding WUR timer...\n", ctx->dev_index, __FUNCTION__);
+		ctx->status_ssp = SSP_WAIT_FOR_SSPWUR;
+		mod_timer(&ctx->deferred_ssp_wakeup_timer, jiffies + HZ);
+		return;
+	}
+
+	PDEBUG(1, "Device %d: ...issuing SSP WUR...\n", ctx->dev_index);
+
+	del_timer_sync(&ctx->ssp_timer);
+
+	// set HRCSR::H2S_WUE to 0x1.  probably unnecessary.
+	write32(cpu_to_be32(0x00000010L), HRCSR(ctx));
+
+	write32(cpu_to_be32(HRCSR_H2S_WURqst), HRCSR(ctx));
+
+#if defined(ENABLE_MFG)
+	ctx->ssp_mbxhist_r_idx = 0;
+	ctx->ssp_mbxhist_w_idx = 0;
+#endif
+
+	spin_lock_bh(&ctx->counter_lock);
+	ctx->status_flags |= FLAG_SSP_WUR_PENDING;
+	spin_unlock_bh(&ctx->counter_lock);
+
+	// we should be awake now so we can start the boot sequence timers...
+	ctx->status_ssp = SSP_WAIT_FOR_POST0;
+	mod_timer(&ctx->ssp_timer, jiffies + HZ*timeout_ssp);
+}
+
+
+void
+log_event(ibm4767_ctx_t *ctx, asym_msg_t *msg_info, char *msg, int len)
+{
+#define CHUNKSIZE 512
+	char *buf = msg, chunk[CHUNKSIZE+1];
+
+	while (len) {
+		int sz = MIN(len, CHUNKSIZE);
+		memcpy(chunk, buf, sz);
+		chunk[sz] = 0;
+
+		PRINTKW("MESSAGE FROM %8s => v%d, f%d, Sev %d, \"%s\"\n", 
+				ctx->hwinfo.serial_num,
+				msg_info->version, 
+				msg_info->facility, 
+				msg_info->severity, 
+				chunk); 
+
+		len -= sz;
+		buf += sz;
+	}
+#undef CHUNKSIZE
+}
+
+
+void
+ibm4767_request_throttle(ibm4767_ctx_t *ctx)
+{
+        uint32_t  delay = 1;
+
+        if (ctx->throttle_lvl == 0)
+                return;
+
+	// this routine is called from user context so it's safe to sleep...
+	//
+        // for each degree above the low temperature threshhold, we double the delay...
+        //
+        msleep(delay << (ctx->throttle_lvl - 1));
+}
+
+
+int
+ibm4767_opcode_type(uint32_t agentid, uint32_t cmd)
+{
+	if (agentid != CCAAGENT)
+		return NORMAL_OP;
+
+	// for 4767, the driver no longer reads opcodes.txt so we
+	// use an internally-generated list of known-to-be-slow opcodes
+	//
+	switch (cmd) {
+		case SSDSV_ID:
+		case SSPKG_ID:
+		case SSPKE_ID:
+		case SSPKD_ID:
+		case SSVLD_ID:
+		case SSGLD_ID:
+		case SSEDH_ID:
+		case SSDSG_ID:
+		case SSSYX_ID:
+		case SSALCT_ID:
+		case SSAACM_ID:
+		case SSAACI_ID:
+		case SSAMKD_ID:
+		case SSMKP_ID:
+		case SSCFC_ID:
+		case SSDHMK_ID:
+		case SSDHKP_ID:
+		case SSCDE_ID:
+		case SSCTI_ID:
+		case SSSLGQ_ID:
+		case SSDPIC_ID:
+		case SSPIM_ID:
+		case SST34B_ID:
+		case SST34C_ID:
+		case SST34D_ID:
+		case SST34R_ID:
+			return SLOW_OP;
+
+		default:
+			return NORMAL_OP;
+	}
+}
+
+
+// caller should ensure that CDU timer is deleted (we can't delete it here
+// since we might be called from said timer)
+//
+void
+ibm4767_cdu_finalize(ibm4767_ctx_t *ctx, int rc)
+{
+	del_timer_sync(&ctx->agentid_list_timer);
+
+	if (!ctx->mb_req)
+		return;
+
+	// if we haven't begun the CDU handshake then there's nothing to wake up
+	// (only way that can happen is if we were called from mark_mcpu_offline()...)
+	//
+	if ((ctx->status_cdu == CDU_WAIT_FOR_READY_TO_PROCEED) ||
+	    (ctx->status_cdu == CDU_WAIT_FOR_PROCEED_ACK)) {
+		ctx->mb_req->retcode = rc;
+
+		atomic_set(&ctx->mb_req->waitEvent, 1); 
+		wake_up(&ctx->mb_req->waitQ);
+	}
+}
+
+
+void
+ibm4767_do_temperature_shutdown(ibm4767_ctx_t *ctx)
+{
+	ibm4767_ctx_t *pDev = NULL;
+	int i, count = 0;
+
+	// dealing with temperature soft tampers is a little tricky.  simply marking
+	// the affected device offline is insufficient since adjacent cards can continue
+	// to provide heating and can cause a soft tamper to turn into a bonafide
+	// unrecoverable hard tamper.  so we play it safe and mark all 4767 devices 
+	// offline if any of them experience a temperature soft tamper.
+	//
+	// we go a step further and reset the devices.  simply disabling DMA and
+	// marking the devices 'offline' doesn't help if their MCPU or crypto units are
+	// busy crunching on something.  
+	//
+	read_lock(&ibm4767_lock); 
+	for (i=0; i < MAX_DEV_COUNT; i++) { 
+		pDev = ibm4767_list[i]; 
+		if (pDev) { 
+			count++;
+						
+			mark_device_offline(pDev, HOSTTemperature); 
+
+			// caveat: if we reset the device that generated a soft tamper interrupt, 
+			// then it will simply generate another interrupt following the reset.
+			// rinse and repeat.  don't go there.
+			if (ctx && (pDev != ctx)) 
+				defer_main_reset(pDev, 1, TRUE);
+
+			pDev->status_mcpu = MCPU_TEMP_SHUTDOWN;
+			pDev->status_ssp  = SSP_TEMP_SHUTDOWN;
+		}
+	}
+
+	read_unlock(&ibm4767_lock);
+}
+
+
+int
+ibm4767_check_tamper(ibm4767_ctx_t *ctx, char *func_str)
+{
+	uint32_t hrcsr;
+	int rc = HOST_DD_Good;
+
+	hrcsr = cpu_to_be32(read32(HRCSR(ctx)));
+	if (!ignore_tamper && (hrcsr & HRCSR_PCT))
+		ctx->status_tamper = TAMPER_PERM;
+
+	if (ctx->status_tamper != TAMPER_NONE) {
+		char *p = NULL;
+
+		switch (ctx->status_tamper) {
+			case TAMPER_PERM:       p = "PERMANENT TAMPER (UNRECOVERABLE)"; 
+						break; 
+			case TAMPER_SOFT_TEMP:  p = "SOFT TAMPER (TEMPERATURE)"; 
+						break;
+			case TAMPER_SOFT_VOLT:  p = "SOFT TAMPER (VOLTAGE)"; 
+						break;
+			case TAMPER_SOFT_INJ:   p = "SOFT TAMPER (INJ)"; 
+						break;
+			case TAMPER_SOFT_OTHER: p = "SOFT TAMPER (OTHER)"; 
+						break;
+			default:                p = "UNKNOWN"; 
+						break;
+		}
+
+		PRINTKE("Device %d: %s failed (%s)\n", ctx->dev_index, func_str, p);
+		rc = HOST_DD_TamperDetected;
+	}
+
+	return rc;
+}
+
+
+void
+ibm4767_get_hwinfo(ibm4767_ctx_t *ctx)
+{
+	xcVpd_t  vpd;
+	uint64_t hcsr;
+	int      rc;
+
+	ctx->hwinfo.version = XC_HWINFO_VERSION_1;
+
+	hcsr = be64_to_cpu(read64(HCSR(ctx)));
+	ctx->hwinfo.fpga_rev = (hcsr >> 16) & 0xFFFF;
+	ctx->hwinfo.asic_rev = (hcsr >> 8) & 0xFF;
+	ctx->hwinfo.card_rev = hcsr & 0xFF;
+
+#if defined(ENABLE_VPD)
+	memset(&vpd, 0, sizeof(vpd));
+	rc = ibm4767_query_vpd(ctx, &vpd);
+	if (rc == HOST_DD_Good) {
+		// xcVpd_t specifies SN as a 4-byte header + 8-byte SN...
+		// BEAM vehemently objects to treating it as 12-contiguous bytes...
+		//
+		memcpy(ctx->hwinfo.serial_num,   &vpd.sn_hdr, sizeof(vpd.sn_hdr));
+		memcpy(ctx->hwinfo.serial_num+4, &vpd.sn,     sizeof(vpd.sn));
+		memcpy(ctx->hwinfo.part_num,     &vpd.pn,     sizeof(ctx->hwinfo.part_num) - 1);
+		memcpy(ctx->hwinfo.ec_level,     &vpd.ec,     sizeof(ctx->hwinfo.ec_level) - 1);
+		memcpy(ctx->hwinfo.fru_num,      &vpd.fn,     sizeof(ctx->hwinfo.fru_num)  - 1);
+		memcpy(ctx->hwinfo.mf_loc,       &vpd.mf,     sizeof(ctx->hwinfo.mf_loc)   - 1);
+	}
+	else {
+		PRINTKW("Device %d: unable to retrieve VPD\n", ctx->dev_index);
+		memcpy(ctx->hwinfo.serial_num, "UNKNOWN", MIN(8, sizeof(ctx->hwinfo.serial_num) - 1));
+		memcpy(ctx->hwinfo.part_num,   "N/A",     MIN(4, sizeof(ctx->hwinfo.part_num)   - 1));
+		memcpy(ctx->hwinfo.ec_level,   "N/A",     MIN(4, sizeof(ctx->hwinfo.ec_level)   - 1));
+		memcpy(ctx->hwinfo.fru_num,    "N/A",     MIN(4, sizeof(ctx->hwinfo.fru_num)    - 1));
+		memcpy(ctx->hwinfo.mf_loc,     " ",       MIN(2, sizeof(ctx->hwinfo.mf_loc)     - 1));
+	 }
+#endif
+
+}
+
+
+void
+ibm4767_pfvf_walkingones(ibm4767_ctx_t *ctx)
+{ }
+
+
+// returns FALSE if more than 'sev0_limit' logs have been received within 
+// the last 'sev0_timeframe' minutes of 'timestamp'.  otherwise returns TRUE.
+//
+// we assume the caller owns 'ctx->counter_lock'
+//
+int
+sev0_append(ibm4767_ctx_t *ctx,  unsigned long timestamp)
+{
+	struct list_head *pLH, *tmp;
+	sev0_entry_t *se;
+	int c = 0;
+
+	// remove any expired entries from the list
+	//
+	list_for_each_safe(pLH, tmp, &ctx->sev0_list) {
+		se = list_entry(pLH, sev0_entry_t, list_head);
+		if (se) {
+			if (time_before((se->timestamp + sev0_timeframe*60*HZ), timestamp)) { 
+				list_del(pLH);
+				kfree(se);
+			} 
+			else { 
+				c++; 
+			}
+		}
+	}
+
+	if (c >= sev0_limit)
+		return FALSE;
+
+	// caution: this routine gets called from a tasklet so must use GFP_ATOMIC
+	se = (sev0_entry_t *)kmalloc(sizeof(sev0_entry_t), GFP_ATOMIC);
+	if (!se) {
+		PRINTKE("kmalloc error in %s\n", __FUNCTION__);
+		return FALSE;
+	}
+
+	INIT_LIST_HEAD(&se->list_head);
+	se->timestamp = timestamp;
+	list_add_tail(&se->list_head, &ctx->sev0_list);
+
+	return TRUE;
+}
+
+
+// we assume that caller owns 'ctx->counter_lock'
+//
+int
+sev0_count(ibm4767_ctx_t *ctx)
+{
+	struct list_head *pLH, *tmp;
+	int c = 0;
+
+	unsigned long timestamp = jiffies;
+
+	// remove any expired entries from the list
+	//
+	list_for_each_safe(pLH, tmp, &ctx->sev0_list) {
+		sev0_entry_t *se = list_entry(pLH, sev0_entry_t, list_head);
+		if (se) {
+			if (time_before((se->timestamp + sev0_timeframe*60*HZ), timestamp)) { 
+				list_del(pLH);
+				kfree(se);
+			}
+			else {
+				c++;
+			}
+		}
+	}
+
+	return c;
+}
+
+
+// we assume that caller owns 'ctx->counter_lock' or that we're shutting down...
+//
+void
+sev0_purge(ibm4767_ctx_t *ctx)
+{
+	struct list_head *pLH, *tmp;
+	sev0_entry_t *se;
+
+	list_for_each_safe(pLH, tmp, &ctx->sev0_list) {
+		se = list_entry(pLH, sev0_entry_t, list_head);
+		list_del(pLH);
+		if (se) {
+			kfree(se);
+		}
+	}
+}
+
+
+// used in conjunction with procfs...
+void
+sev0_dump(ibm4767_ctx_t *ctx, struct seq_file *sf)
+{
+	struct list_head *pLH, *tmp;
+	
+	long timestamp = jiffies;
+
+	seq_printf(sf, "Sev0 Queue:\n");
+	seq_printf(sf, "%8s\t%8s\n", "Age", "Expires");
+
+	list_for_each_safe(pLH, tmp, &ctx->sev0_list) {
+		sev0_entry_t *se = list_entry(pLH, sev0_entry_t, list_head);
+		if (se) {
+			long age = (timestamp - se->timestamp) / HZ;
+			long expires = sev0_timeframe*60 - age;
+			seq_printf(sf, "%8ld\t%8ld\n", age, expires);
+		}
+	}
+}
+
diff --git drivers/misc/ibm4767/workqueues.c drivers/misc/ibm4767/workqueues.c
new file mode 100755
index 000000000000..84bf2fe83c91
--- /dev/null
+++ drivers/misc/ibm4767/workqueues.c
@@ -0,0 +1,558 @@
+/*************************************************************************
+ *  Filename:workqueues.c
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  Time-consuming routines that operate in workqueues or
+ *            secondary threads
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#include <linux/version.h>
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32))
+#error "Kernels prior to 2.6.32 are not supported"
+#endif
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33))
+#include <linux/autoconf.h>
+#endif
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/workqueue.h>
+
+
+
+#include "common-ras.h"
+#include "y_config.h"
+#include "y_hostRB.h"
+#include "y_regs.h"
+#include "y_mailbox.h"
+#include "driver.h"
+#include "y_funcs.h"
+
+
+
+void
+ibm4767_workqueues_init(ibm4767_ctx_t *ctx)
+{
+	spin_lock_init(&ctx->work_quiesce_lock);
+	ctx->work_quiesce = 0;
+
+	INIT_WORK(&ctx->hra_pool_work,       ibm4767_work_hra_load_packet);
+	INIT_WORK(&ctx->main_reset_work,     ibm4767_work_main_reset);
+	INIT_WORK(&ctx->post0_edump_work,    ibm4767_work_post0_edump);
+	INIT_WORK(&ctx->post2_edump_work,    ibm4767_work_post2_edump);
+	INIT_WORK(&ctx->seg2_edump_work,     ibm4767_work_seg2_edump);
+#if defined(ENABLE_MFG)
+	INIT_WORK(&ctx->mcpu_tas_edump_work, ibm4767_work_mcpu_tas_edump);
+	INIT_WORK(&ctx->ssp_tas_edump_work,  ibm4767_work_ssp_tas_edump);
+#endif
+}
+
+
+void
+ibm4767_workqueues_unload(ibm4767_ctx_t *ctx)
+{
+	ctx->work_quiesce = WORK_QUIESCE_ALL;
+
+	cancel_work_sync(&ctx->hra_pool_work);
+	cancel_work_sync(&ctx->main_reset_work);
+	cancel_work_sync(&ctx->post0_edump_work);
+	cancel_work_sync(&ctx->post2_edump_work);
+	cancel_work_sync(&ctx->seg2_edump_work);
+#if defined(ENABLE_MFG)
+	cancel_work_sync(&ctx->mcpu_tas_edump_work);
+	cancel_work_sync(&ctx->ssp_tas_edump_work);
+#endif
+	
+	ctx->work_quiesce = 0;
+}
+
+
+void
+ibm4767_work_hra_load_packet(struct work_struct *work)
+{
+	ibm4767_ctx_t *ctx = container_of(work, ibm4767_ctx_t, hra_pool_work);
+
+	if (ctx->work_quiesce & WORK_QUIESCE_HRA_POOL)
+		return;
+
+	asym_hra_pool_load_packet_init(ctx);
+	asym_hra_pool_load_packet_send(ctx);
+}
+
+
+void
+ibm4767_work_main_reset(struct work_struct *work)
+{
+	ibm4767_ctx_t *ctx = container_of(work, ibm4767_ctx_t, main_reset_work);
+
+	if (ctx->work_quiesce & WORK_QUIESCE_MAIN_RESET)
+		return;
+
+	ibm4767_main_reset(ctx, TRUE);
+}
+
+
+// emergency dump routines
+//
+// there are two ways we can drain the FIFOs using a workqueue:
+// 1) drain into a temporary buffer then hex dump the contents
+// 2) drain the FIFOs and hex dump on the fly
+//
+// we choose method #1 although it requires additional memory because: 
+// A) Draining a FIFO might take a few seconds depending on how quickly the card is writing data
+// B) Emergency dumps often occur in pairs (one for SSP, one for MCPU).  Method #1 allows
+//    us to train in parallel.  We'll serialize output in syslog using 'edump_lock'
+// 
+
+
+void
+ibm4767_work_seg2_edump(struct work_struct *work)
+{ 
+	ibm4767_ctx_t  *ctx = container_of(work, ibm4767_ctx_t, seg2_edump_work);
+	char         *dest = ctx->edump_mcpu;
+	asym_msg_t   *msg_info = NULL;
+	unsigned long jif = jiffies;
+	uint64_t      bytes, tmp64;
+	int           rc, total = 0;
+
+	if (ctx->work_quiesce & WORK_QUIESCE_SEG2_EDUMP)
+		return;
+
+	PDEBUG(2, "attempting to grab SEG2 emergency dump info...\n");
+
+	memset(dest, 0, SEG2_EMERGENCY_DUMP_SIZE);
+
+	// 24-byte 4767 Header (we strip this from the dump)
+	// 8 byte LENGTH field
+	// LENGTH bytes of e-dump data
+	//
+	rc = target_read8_mcpu_fifo(ctx, (char *)&tmp64, FALSE);
+	if (rc) {
+		PRINTKW("SEG2 emergency dump timeout #1\n");
+		return;
+	}
+	tmp64 = be64_to_cpu(tmp64); 
+	PRINTKW("SEG2 HDR0 [%llx]\n", tmp64);
+
+	rc = target_read8_mcpu_fifo(ctx, (char *)&tmp64, FALSE);
+	if (rc) {
+		PRINTKW("SEG2 emergency dump timeout #2\n");
+		return;
+	}
+	tmp64 = be64_to_cpu(tmp64); 
+	PRINTKW("SEG2 HDR1 [%llx]\n",tmp64);
+	
+	rc = target_read8_mcpu_fifo(ctx, (char *)&tmp64, FALSE);
+	if (rc) {
+		PRINTKW("SEG2 emergency dump timeout #3\n");
+		return;
+	}
+	tmp64 = be64_to_cpu(tmp64); 
+	PRINTKW("SEG2 HDR2 [%llx]\n",tmp64);
+	
+	target_read8_mcpu_fifo(ctx, (char *)&bytes, FALSE);
+	bytes = be64_to_cpu(bytes);
+	PRINTKW("SEG2 EDL [%llx]\n", bytes);
+	
+#define CRAZY_EMDUMP_SIZE 200000 
+	if (bytes > CRAZY_EMDUMP_SIZE) { 
+		PRINTKW("Huge dump size reported (0x%x bytes).  Assuming edump is corrupted\n", (int)bytes);
+		return; 
+	} 
+	else if (bytes > SEG2_EMERGENCY_DUMP_SIZE) {
+		PRINTKW("Reported dump size larger than driver maximum.  Truncating to %d (0x%x)\n", SEG2_EMERGENCY_DUMP_SIZE, SEG2_EMERGENCY_DUMP_SIZE);
+		bytes = SEG2_EMERGENCY_DUMP_SIZE; 
+	} 
+	else {
+		PRINTKW("Reported SEG2 emergency dump size: %d (0x%llx) bytes\n", (int)bytes, bytes);
+	}
+
+	while (1) {
+		rc = target_read8_mcpu_fifo(ctx, dest+sizeof(uint64_t)+total, FALSE);
+		if (rc)
+			goto drain_done;
+	
+		total += 8;
+
+		// if we haven't grabbed the dump in 5 seconds, stop and dump what we have...
+		//
+		if (jiffies > (jif + 5*HZ)) {
+			PRINTKW("edump took too long.  %d (0x%x) bytes extracted.\n", total, total);
+			goto drain_done;
+		}
+
+		if (total >= bytes)
+			goto drain_done;
+
+		// if the driver needs to cancel work queues, we abort and log what we've retrieved so far
+		//
+		if (ctx->work_quiesce)
+			goto drain_done;
+	}
+
+drain_done:
+	*(uint64_t *)dest = total;
+
+	if (total) {
+		msg_info = (asym_msg_t *)(dest + sizeof(uint64_t));
+	
+		be16_to_cpus(&msg_info->length);
+		be16_to_cpus(&msg_info->type);
+		be16_to_cpus(&msg_info->version);
+		be16_to_cpus(&msg_info->facility);
+		be16_to_cpus(&msg_info->severity);
+
+		spin_lock_bh(&ctx->edump_lock);
+		log_event(ctx, msg_info, dest+sizeof(uint64_t)+sizeof(asym_msg_t), total);
+		spin_unlock_bh(&ctx->edump_lock);
+	}
+}
+
+
+
+
+void
+ibm4767_work_post0_edump(struct work_struct *work)
+{
+	ibm4767_ctx_t  *ctx = container_of(work, ibm4767_ctx_t, post0_edump_work);
+	char         *dest = ctx->edump_ssp;
+	unsigned long jif = jiffies;
+	int           rc, total = 0;
+
+	if (ctx->work_quiesce & WORK_QUIESCE_POST0_EDUMP)
+		return;
+
+	PDEBUG(2, "attempting to grab POST0 emergency dump...\n");
+
+	// see fence section 17.4.1 for POST emergency dump format:
+	//
+	// <8-byte fake DMA header to satisfy the hardware>
+	// <32bit dump length>
+	// 'dump length' bytes of dump information
+	//
+	// we generally ignore the length, though, and just drain the dump until either
+	// there's nothing left or we reach the maximum expected size...
+	//
+	while (1) {
+		rc = target_read8_ssp_fifo(ctx, dest+total, FALSE);
+		if (rc) 
+			goto drain_done;
+
+		total += 8;
+
+		// if we haven't grabbed the dump in 5 seconds, stop and dump what we have...
+		//
+		if (jiffies > (jif + 5*HZ)) {
+			PRINTKW("Emergency dump took too long.  %d (0x%x) bytes extracted.\n", total, total);
+			goto drain_done;
+		}
+		
+		// our SSP edump buffer should be large enough but if it's not, truncate...
+		if (POST_EMERGENCY_DUMP_SIZE < (total+8)) {
+			total = POST_EMERGENCY_DUMP_SIZE - 4;
+			PRINTKW("Emergency dump too big.  Truncating at 0x%x bytes\n", total);
+			goto drain_done;
+		}
+
+		// if the driver needs to cancel work queues, we abort and log what we've retrieved so far
+		//
+		if (ctx->work_quiesce)
+			goto drain_done;
+	}
+
+drain_done:
+	spin_lock_bh(&ctx->edump_lock);
+	if (total) {
+		PRINTKW("POST0 EMERGENCY DUMP - %d (0x%x) bytes\n", total, total);
+		hex_dump(ctx, 0, "POST0 EMERGENCY DUMP", dest, total);
+	}
+	else {
+		PRINTKW("POST0 EMERGENCY DUMP - zero bytes drained\n");
+	}
+
+	if (ctx->edump_flag & EDUMP_FLAG_BYPASS_DE_SSP) {
+		PRINTKW("Device %d: 0xDE bypass invoked...\n", ctx->dev_index);
+		write64(read64(S2H_MBX(ctx)), H2S_MBX(ctx));
+		ctx->edump_flag &= ~EDUMP_FLAG_BYPASS_DE_SSP;
+
+		// tell the card to resume DMA operations.  if the card was slow to write
+		// edump data and we timed-out, this might cause problems.  we can live
+		// with that.
+		write32(cpu_to_be32(HCR_RESUME_SRM), HCR(ctx));
+	}
+
+	if (ctx->edump_flag & EDUMP_FLAG_MARK_SSP_OFFLINE) {
+		mark_ssp_offline(ctx, POSTError);
+		ctx->edump_flag &= ~EDUMP_FLAG_MARK_SSP_OFFLINE;
+	}
+	spin_unlock_bh(&ctx->edump_lock);
+}
+
+
+void
+ibm4767_work_post2_edump(struct work_struct *work)
+{
+	ibm4767_ctx_t  *ctx = container_of(work, ibm4767_ctx_t, post2_edump_work);
+	char         *dest = ctx->edump_mcpu;
+	unsigned long jif = jiffies;
+	int           rc, total = 0;
+
+	if (ctx->work_quiesce & WORK_QUIESCE_POST2_EDUMP)
+		return;
+
+	// POST2 and Seg2 share the same emergency dump buffer.  This is okay since
+	// they're mutually-exclusive.  Seg2 overwrites POST2's interrupt vector so
+	// it's not possible for both POST2 and Seg2 to generate an edump for the same event
+	//
+	PDEBUG(2, "attempting to grab POST2 emergency dump...\n");
+	
+	// see fence section 17.4.1 for POST emergency dump format:
+	//
+	// <8-byte fake DMA header to satisfy the hardware>
+	// <32bit dump length>
+	// 'dump length' bytes of dump information
+	//
+	// we generally ignore the length, though, and just drain the dump until either
+	// there's nothing left or we reach the maximum expected size...
+	//
+	while (1) {
+		rc = target_read8_mcpu_fifo(ctx, dest+total, FALSE);
+		if (rc) 
+			goto drain_done;
+
+		total += 8;
+
+		// if we haven't grabbed the dump in 5 seconds, stop and dump what we have...
+		//
+		if (jiffies > (jif + 5*HZ)) {
+			PRINTKW("Emergency dump took too long.  %d (0x%x) bytes extracted.\n", total, total);
+			goto drain_done;
+		}
+		
+		// our SSP edump buffer should be large enough but if it's not, truncate...
+		if (POST_EMERGENCY_DUMP_SIZE < (total+8)) {
+			total = POST_EMERGENCY_DUMP_SIZE - 4;
+			PRINTKW("Emergency dump too big.  Truncating at 0x%x bytes\n", total);
+			goto drain_done;
+		}
+
+		// if the driver needs to cancel work queues, we abort and log what we've retrieved so far
+		//
+		if (ctx->work_quiesce)
+			goto drain_done;
+	}
+
+drain_done:
+	spin_lock_bh(&ctx->edump_lock);
+	if (total) {
+		PRINTKW("POST2 EMERGENCY DUMP - %d (0x%x) bytes\n", total, total);
+		hex_dump(ctx, 0, "POST2 EMERGENCY DUMP", dest, total);
+	}
+	else {
+		PRINTKW("POST2 EMERGENCY DUMP - zero bytes drained\n");
+	}
+	
+	if (ctx->edump_flag & EDUMP_FLAG_BYPASS_DE_MCPU) {
+		PRINTKW("Device %d: 0xDE bypass invoked...\n", ctx->dev_index);
+		write64(read64(M2H_MBX(ctx)), H2M_MBX(ctx));
+		ctx->edump_flag &= ~EDUMP_FLAG_BYPASS_DE_MCPU;
+		
+		// tell the card to resume DMA operations.  if the card was slow to write
+		// edump data and we timed-out, this might cause problems.  we can live
+		// with that.
+		write32(cpu_to_be32(HCR_RESUME_MRM), HCR(ctx));
+	}
+
+	if (ctx->edump_flag & EDUMP_FLAG_MARK_MCPU_OFFLINE) {
+		mark_mcpu_offline(ctx, POSTError);
+		ctx->edump_flag &= ~EDUMP_FLAG_MARK_MCPU_OFFLINE;
+	}
+	spin_unlock_bh(&ctx->edump_lock);
+}
+
+
+#if defined(ENABLE_MFG)
+void
+ibm4767_work_ssp_tas_edump(struct work_struct *work)
+{ 
+	ibm4767_ctx_t  *ctx = container_of(work, ibm4767_ctx_t, ssp_tas_edump_work);
+	char         *dest = ctx->edump_ssp_tas;
+	//char          junk[8];
+	unsigned long jif = jiffies;
+	int           rc, total = 0;
+
+	if (ctx->work_quiesce & WORK_QUIESCE_SSP_TAS_EDUMP)
+		return;
+
+
+	PDEBUG(2, "attempting to grab TAS SSP emergency dump...\n");
+	
+	// TAS emergency dumps consist of ASCII text dumped by the card with no length prefix so
+	// we read until there's either nothing in the FIFO or the dump buffer is full.
+	//
+	// the first 8 bytes will be a fake DMA header prepended to satisfy the DMA hardware...
+	//
+#if 0
+	rc = target_read8_ssp_fifo(ctx, junk, FALSE);
+	if (rc)
+		goto drain_done;
+#endif
+
+	while (1) {
+		rc = target_read8_ssp_fifo(ctx, dest+total, FALSE);
+		if (rc) 
+			goto drain_done;
+		
+		total += 8;
+
+		// if we haven't grabbed the dump in 5 seconds, stop and dump what we have...
+		//
+		if (jiffies > (jif + 5*HZ)) {
+			PRINTKW("Emergency dump took too long.  %d (0x%x) bytes extracted.\n", total, total);
+			goto drain_done;
+		}
+
+		if (TAS_EMERGENCY_DUMP_SIZE < (total - 8)) {
+			total = TAS_EMERGENCY_DUMP_SIZE;
+			PRINTKW("Emergency dump too big.  Truncating to 0x%x bytes\n", total);
+			goto drain_done;
+		}
+		
+		// if the driver needs to cancel work queues, we abort and log what we've retrieved so far
+		//
+		if (ctx->work_quiesce)
+			goto drain_done;
+	}
+
+drain_done:
+	spin_lock_bh(&ctx->edump_lock);
+	if (total > 0) {
+		PRINTKW("TAS SSP EMERGENCY DUMP - %d (0x%x) bytes\n", total, total);
+
+#define CHUNKSIZE 16
+		while (total) {
+			int sz = MIN(total, CHUNKSIZE);
+			char chunk[CHUNKSIZE+1];
+
+			memcpy(chunk, dest, sz);
+			chunk[sz] = 0;
+			PRINTKW("%s\n", chunk);
+
+			total -= sz;
+			dest += sz;
+		}
+#undef CHUNKSIZE
+	}
+	else {
+		PRINTKW("TAS SSP EMERGENCY DUMP - zero bytes drained\n");
+	}
+	spin_unlock_bh(&ctx->edump_lock);
+}
+#endif
+
+
+#if defined(ENABLE_MFG)
+void
+ibm4767_work_mcpu_tas_edump(struct work_struct *work)
+{
+	ibm4767_ctx_t  *ctx = container_of(work, ibm4767_ctx_t, mcpu_tas_edump_work);
+	char         *dest = ctx->edump_mcpu;
+	//char          junk[8];
+	unsigned long jif = jiffies;
+	int           rc, total = 0;
+
+	if (ctx->work_quiesce & WORK_QUIESCE_MCPU_TAS_EDUMP)
+		return;
+
+	PDEBUG(2, "attempting to grab TAS MCPU emergency dump...\n");
+	
+	// TAS emergency dumps consist of ASCII text dumped by the card with no length prefix so
+	// we read until there's either nothing in the FIFO or the dump buffer is full.
+	//
+	// the first 8 bytes will be a fake DMA header prepended to satisfy the DMA hardware...
+	//
+#if 0
+	rc = target_read8_mcpu_fifo(ctx, junk, FALSE);
+	if (rc)
+		goto drain_done;
+#endif
+
+	while (1) {
+		rc = target_read8_mcpu_fifo(ctx, dest+total, FALSE);
+		if (rc) 
+			goto drain_done;
+
+		total += 8;
+
+		// if we haven't grabbed the dump in 5 seconds, stop and dump what we have...
+		//
+		if (jiffies > (jif + 5*HZ)) {
+			PRINTKW("Emergency dump took too long.  %d (0x%x) bytes extracted.\n", total, total);
+			goto drain_done;
+		}
+
+		if (TAS_EMERGENCY_DUMP_SIZE < (total - 8)) {
+			total = TAS_EMERGENCY_DUMP_SIZE;
+			PRINTKW("Emergency dump too big.  Truncating to 0x%x bytes\n", total);
+			goto drain_done;
+		}
+		
+		// if the driver needs to cancel work queues, we abort and log what we've retrieved so far
+		//
+		if (ctx->work_quiesce)
+			goto drain_done;
+	}
+
+drain_done:
+	spin_lock_bh(&ctx->edump_lock);
+	if (total > 0) {
+		PRINTKW("TAS MCPU EMERGENCY DUMP - %d (0x%x) bytes\n", total, total);
+
+#define CHUNKSIZE 16
+		while (total) {
+			int sz = MIN(total, CHUNKSIZE);
+			char chunk[CHUNKSIZE+1];
+
+			memcpy(chunk, dest, sz);
+			chunk[sz] = 0;
+			PRINTKW("%s\n", chunk);
+
+			total -= sz;
+			dest += sz;
+		}
+#undef CHUNKSIZE
+	}
+	else {
+		PRINTKW("TAS MCPU EMERGENCY DUMP - zero bytes drained\n");
+	}
+	spin_unlock_bh(&ctx->edump_lock);
+}
+#endif
+
+
diff --git drivers/misc/ibm4767/xc_host.h drivers/misc/ibm4767/xc_host.h
new file mode 100755
index 000000000000..36a06cce92bb
--- /dev/null
+++ drivers/misc/ibm4767/xc_host.h
@@ -0,0 +1,136 @@
+/*
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  IBM4767 host library interface definitions.                          
+ *                                                                      
+*************************************************************************/
+// $Revision: 1.1 $
+
+#ifndef _XC_HOST_H_
+#define _XC_HOST_H_
+
+#define MAX_DEV_COUNT 8
+
+typedef unsigned int xcAdapterNumber_t;
+
+#if defined(WINDOWS)
+typedef HANDLE xcAdapterHandle_t
+#else
+typedef int xcAdapterHandle_t;
+#endif
+
+typedef struct { 
+	char length;       // number of bytes in this struct 
+	uint8_t tmprbits;  // just the tamper bits
+} xcHdwTmpr_t;
+
+typedef struct {
+	uint64_t mbx[512];
+	uint32_t num;
+} xcMbxHist_t;
+
+/*
+ * xcAdapterCount - Return the number of XC adapters on the system
+ */
+unsigned int xcAdapterCount(xcAdapterNumber_t *adapter_count);
+
+
+/*
+ * xcOpenAdapter - Establish a connection with an XC adapter
+ */
+unsigned int xcOpenAdapter(xcAdapterNumber_t  adapter_number,
+                           xcAdapterHandle_t *handle);
+
+
+/*
+ * xcCloseAdapter - De-establish a connection with an XC adapter
+ */
+unsigned int xcCloseAdapter(xcAdapterHandle_t handle);
+
+
+/*
+ * xcRequest - Send a synchronous request to an XC adapter
+ */
+unsigned int xcRequest(xcAdapterHandle_t  handle,
+                       xcRB_t            *request_block);
+
+
+/*
+ * xcGetAdapterData 
+ *
+ *    Retrieve identification data from a crypto adapter.
+ *    This call is used by a host application to get device id data
+ *    a card.
+ */
+unsigned int xcGetAdapterData(xcAdapterHandle_t hAdapterHandle,
+                              xcVpd_t *         vpd);
+
+/*
+ * xcGetHardwareInfo -
+ *    Retrieve hardware and firmware information.
+ * 
+ *    Note: the firmware versions are unknown to the driver until
+ *          the device boots to that particular entity.  until then,
+ *          the corresponding version field in pHWInfo will be 0xFFFFFFFF.
+ *
+ *    Example: following a device reset, pHWInfo->mb1_ver will contain
+ *             0xFFFFFFFF until the Miniboot 1 runs on the device.
+ *          
+ */
+unsigned int xcGetHardwareInfo(xcAdapterHandle_t hAdapterHandle,
+                               xcHWInfo_t *      pHWInfo);
+
+/*
+ * xcGetTamperBits -
+ *    Retrieve tamper status
+ */
+unsigned int xcGetTamperBits(xcAdapterHandle_t hAdapterHandle,
+                             xcHdwTmpr_t *     pHdwTamperBits);
+
+
+/*
+ * xcReset -
+ *    Reset an adapter
+ */
+unsigned int xcResetAdapter(xcAdapterHandle_t handle);
+
+/*
+ * xcCriticalEventNotifier
+ */
+unsigned int xcCriticalEventNotifier(xcAdapterHandle_t handle,
+                                     xcCritEvent_t *   event);
+
+#if defined(ENABLE_MFG)
+unsigned int xcMailboxHistorySSP (xcAdapterHandle_t handle,
+                                  xcMbxHist_t *     dest);
+unsigned int xcMailboxHistoryMCPU(xcAdapterHandle_t handle,
+                                  xcMbxHist_t *     dest);
+unsigned int xcMailboxWrite      (xcAdapterHandle_t handle,
+			          xcMbxRB_t *       request_block);
+unsigned int xcBarRead           (xcAdapterHandle_t handle,
+                                  xcBarRB_t *     request_block);
+unsigned int xcBarWrite          (xcAdapterHandle_t handle,
+                                  xcBarRB_t *     request_block);
+#endif
+
+
+#endif 
diff --git drivers/misc/ibm4767/y_config.h drivers/misc/ibm4767/y_config.h
new file mode 100755
index 000000000000..690263deb5e3
--- /dev/null
+++ drivers/misc/ibm4767/y_config.h
@@ -0,0 +1,60 @@
+/*************************************************************************
+ *  Filename:y_config.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  configuration header file.                                          
+ *                                         
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#ifndef _YCONFIG_H_
+#define _YCONFIG_H_
+
+#undef  HOST32BIT
+#undef  HOST64BIT
+#if defined(CONFIG_64BIT)
+  #define HOST64BIT
+#else
+  #define HOST32BIT
+#endif
+
+// the following used to be defined in the Makefile...
+//
+//#define DEBUG 
+#define ENABLE_DMA
+//#define ENABLE_MFG
+#define ENABLE_VPD
+//#define ENABLE_LOGCHECK
+//#define ENABLE_REALTIME_REGS
+//#define ENABLE_REQ_TRACKING
+//#define ENABLE_HTB_SCRUBBING
+//#define ENABLE_EEH
+#define LINUX
+#define ENABLE_MODULE_DEVICE_TABLE
+//#define WINDOWS
+//#define BROKEN_64BIT_READS
+
+#endif
+
diff --git drivers/misc/ibm4767/y_funcs.h drivers/misc/ibm4767/y_funcs.h
new file mode 100755
index 000000000000..71a1dfc1495f
--- /dev/null
+++ drivers/misc/ibm4767/y_funcs.h
@@ -0,0 +1,265 @@
+// $Date$
+//
+
+#ifndef _YFUNCS_H
+#define _YFUNCS_H
+
+// asym hra routines
+int  asym_hra_pool_initialize(ibm4767_ctx_t *);
+void asym_hra_pool_unload(ibm4767_ctx_t *);
+void asym_hra_pool_dump(ibm4767_ctx_t *);
+void asym_hra_pool_update(ibm4767_ctx_t *);
+int  asym_hra_reply_handler(ibm4767_ctx_t *, uint64_t phys);
+int  asym_hra_pool_load_packet_init(ibm4767_ctx_t *);
+int  asym_hra_pool_load_packet_send(ibm4767_ctx_t *);
+int  asym_hra_pool_load_packet_reply(ibm4767_ctx_t *, request_t *pReq);
+
+
+// dma routines
+int  dma_initialize(ibm4767_ctx_t *);
+void dma_unload(ibm4767_ctx_t *);
+void dma_enable(ibm4767_ctx_t *, uint32_t mask);
+void dma_disable(ibm4767_ctx_t *, uint32_t mask);
+int  dma_check_fp_enable(ibm4767_ctx_t *, uint64_t hbmsr_mask, uint32_t hbmcr_mask);
+int  dma_send_request_MCPU(ibm4767_ctx_t *, request_t *pReq);
+int  dma_send_request_PKA(ibm4767_ctx_t *, request_t *pReq);
+int  dma_send_request_SKCH(ibm4767_ctx_t *, request_t *pReq);
+int  dma_send_request_SSP(ibm4767_ctx_t  *, request_t *pReq);
+int  dma_send_request_FPGA_A(ibm4767_ctx_t  *, request_t *pReq);
+int  dma_send_request_FPGA_B(ibm4767_ctx_t  *, request_t *pReq);
+int  dma_send_HRA_load_packet(ibm4767_ctx_t *);
+
+// target mode routines
+int  target_read8_ssp_fifo(ibm4767_ctx_t *, unsigned char *data, int safe);
+int  target_send8_ssp_fifo(ibm4767_ctx_t *, unsigned char *data);
+int  target_read8_mcpu_fifo(ibm4767_ctx_t *, unsigned char *data, int safe);
+int  target_send8_mcpu_fifo(ibm4767_ctx_t *, unsigned char *data);
+
+// dt routines
+int  dt_initialize(ibm4767_ctx_t *);
+void dt_unload(ibm4767_ctx_t *);
+void dt_merge(struct list_head  *list1, struct list_head  *list2);
+int  dt_append(struct list_head *list, struct dt_t *pDT);
+int  dt_prepend(struct list_head  *list, struct dt_t *pDT);
+struct dt_t *dt_allocate(ibm4767_ctx_t *, request_t *);
+void dt_free(ibm4767_ctx_t *, struct dt_t *pDT);
+void dt_free_chain(ibm4767_ctx_t *, struct list_head *chain_head);
+int  dt_append_scatterlist(ibm4767_ctx_t      *, 
+			   request_t *,
+			   struct list_head *dt_list, 
+			   y_scatterlist_t    *slist);
+void dt_set_hrb_len(struct list_head *dt_list, uint32_t hrb_len);
+void dt_set_rmri_last(struct list_head *dt_list, request_t *pReq);
+void dump_chain(ibm4767_ctx_t *, struct list_head *list, int verbose);
+void dump_dt(ibm4767_ctx_t *, struct dt_t *pDT);
+
+
+// htb routines
+int  htb_allocate(ibm4767_ctx_t *);
+void htb_unload(ibm4767_ctx_t *);
+void htb_swap(ibm4767_ctx_t *);
+htb_vector_t *htb_lookup(ibm4767_ctx_t *, uint64_t phys);
+void dump_htb(ibm4767_ctx_t *);
+
+
+// I/O routines
+int    ibm4767_open(struct inode *, struct file *);
+int    ibm4767_release(struct inode *, struct file *);
+loff_t ibm4767_llseek(struct file *, loff_t, int);
+
+
+// ioctl routines
+#if defined(ENABLE_MFG)
+int ibm4767_ioctl_ssp_mbx_hist (struct file *, unsigned long * );
+int ibm4767_ioctl_mcpu_mbx_hist(struct file *, unsigned long * );
+#endif
+int ibm4767_ioctl_gethdwtmpr(struct file *, unsigned long * );
+int ibm4767_ioctl_getvpd    (struct file *, unsigned long * );
+int ibm4767_ioctl_open      (struct file *                  );
+int ibm4767_ioctl_request   (struct file *, xcRB_t *        );
+int ibm4767_ioctl_special   (struct file *, xcRB_t *        );
+int ibm4767_ioctl_reset     (struct file *                  );
+int ibm4767_ioctl_observer  (struct file *, xcCritEvent_t * );
+int ibm4767_ioctl_mbx_write (struct file *, xcMbxRB_t *     );
+int ibm4767_ioctl_bar_read  (struct file *, xcBarRB_t *     );
+int ibm4767_ioctl_bar_write (struct file *, xcBarRB_t *     );
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36))
+int  ibm4767_ioctl(struct inode *, struct file *, unsigned int, unsigned long);
+#else
+long ibm4767_ioctl(struct file *, unsigned int, unsigned long);
+#endif
+#if defined(YC_USE_COMPAT_IOCTL)
+long ibm4767_compat_ioctl(struct file *, unsigned int, unsigned long);
+#endif
+
+
+// interrupt and mailbox routines
+//
+int  ibm4767_setup_msix(ibm4767_ctx_t *);
+int  ibm4767_setup_msi(ibm4767_ctx_t *);
+void ibm4767_free_msix(ibm4767_ctx_t *);
+void ibm4767_free_msi(ibm4767_ctx_t *);
+
+
+irqreturn_t ibm4767_isr(int, void *);
+void ibm4767_interrupt_tasklet(unsigned long);
+void analyze_dma_error(ibm4767_ctx_t *);
+void handle_xfer_complete(ibm4767_ctx_t *, interrupt_state_t *);
+int  handle_asym_reply(ibm4767_ctx_t *, uint64_t phys);
+void m2h_mbx_handler(ibm4767_ctx_t *, uint32_t, uint32_t);
+void m2h_mbx_comm_mgr_handler(ibm4767_ctx_t *, uint32_t, uint32_t);
+void s2h_mbx_handler(ibm4767_ctx_t *, uint32_t, uint32_t);
+
+
+// procfs stuff
+//
+void ibm4767_procfs_add_device(long num);
+void ibm4767_procfs_remove_device(int num);
+void ibm4767_procfs_setup(void);
+void ibm4767_procfs_cleanup(void);
+
+
+// request routines
+int        request_pool_initialize(ibm4767_ctx_t *);
+void       request_pool_unload(ibm4767_ctx_t *);
+int        request_allocate(ibm4767_ctx_t *, char mode, request_wait_t *);
+void       request_release(ibm4767_ctx_t *, request_t *pReq);
+request_t *request_locate_by_DT(ibm4767_ctx_t *, uint64_t phys);
+void       request_abort_by_requestid(ibm4767_ctx_t *, uint16_t requestid, int code);
+int        request_fuzzer(ibm4767_ctx_t *, uint32_t *);
+
+void flush_request(request_t *req, int code);
+void flush_all_requests(ibm4767_ctx_t *, int code);
+void flush_fp_request_queue(ibm4767_ctx_t *, int code);
+void flush_mcpu_request_queue(ibm4767_ctx_t *, int code);
+void flush_ssp_request_queue(ibm4767_ctx_t *, int code);
+
+int mcpu_request    (ibm4767_ctx_t *, xcRB_t *);
+int fastpath_request(ibm4767_ctx_t *, xcRB_t *);
+
+
+// special request routines
+int special_request_mb  (ibm4767_ctx_t *, xcRB_t *);
+int special_request_ssp (ibm4767_ctx_t *, xcRB_t *);
+int special_request_mcpu(ibm4767_ctx_t *, xcRB_t *);
+int special_request_assemble(ibm4767_ctx_t *, request_t *, xcRB_t *, bool is_SSP);
+
+int build_special_fragment(ibm4767_ctx_t   *, 
+			   request_t *,
+			   y_scatterlist_t *slist, 
+			   struct list_head *dtlist,
+			   void          *buf, 
+			   uint32_t       datalen, 
+			   bool           is_SSP, 
+			   bool           is_kmem, 
+			   uint8_t        intIV);
+
+int special_request_post2_pre    (ibm4767_ctx_t *, request_t *);
+int special_request_mcpu_tas_pre (ibm4767_ctx_t *, request_t *);
+int special_request_mcpu_tapp_pre(ibm4767_ctx_t *, request_t *);
+int special_request_post1_pre    (ibm4767_ctx_t *, request_t *);
+int special_request_ssp_tas_pre  (ibm4767_ctx_t *, request_t *);
+int special_request_ssp_tapp_pre (ibm4767_ctx_t *, request_t *);
+int special_request_mb0_pre      (ibm4767_ctx_t *, request_t *);
+int special_request_mb1_pre      (ibm4767_ctx_t *, request_t *);
+
+
+// event observer routines
+observer_t *event_observer_allocate(ibm4767_ctx_t *);
+void        event_observer_release(observer_t *pObs);
+void        notify_observers(ibm4767_ctx_t *, uint32_t event);
+
+
+
+// timers
+void ibm4767_timers_init(ibm4767_ctx_t *);
+void ibm4767_timers_unload(ibm4767_ctx_t *);
+
+// work queues
+void ibm4767_workqueues_init(ibm4767_ctx_t *);
+void ibm4767_workqueues_unload(ibm4767_ctx_t *);
+void ibm4767_work_hra_load_packet(struct work_struct *);
+void ibm4767_work_main_reset(struct work_struct *);
+void ibm4767_work_deferred_reset(struct work_struct *);
+void ibm4767_work_seg2_edump(struct work_struct *);
+void ibm4767_work_post0_edump(struct work_struct *);
+void ibm4767_work_post2_edump(struct work_struct *);
+void ibm4767_work_ssp_tas_edump(struct work_struct *);
+void ibm4767_work_mcpu_tas_edump(struct work_struct *);
+void ibm4767_work_wake_ssp(struct work_struct *);
+
+// misc routines
+struct list_head *list_remove_head(struct list_head *list);
+struct list_head *list_remove_tail(struct list_head *list);
+
+void init_scatterlist(y_scatterlist_t *list);
+int alloc_scatterlist(ibm4767_ctx_t   *,
+		      char          *buffer,
+		      uint32_t       buflen,
+		      uint32_t       total_len,
+		      int            is_kernel,
+		      y_scatterlist_t *list,
+		      int            direction);
+void free_scatterlist(y_scatterlist_t *list);
+void dump_scatterlist(y_scatterlist_t *list);
+void merge_scatterlist(y_scatterlist_t *listA, y_scatterlist_t *listB);
+
+void dump_hw_info(ibm4767_ctx_t *);
+void dump_pci_cfg(ibm4767_ctx_t *);
+void dump_regs(ibm4767_ctx_t *);
+
+int  ibm4767_read8_indirect(ibm4767_ctx_t *, uint8_t offset, uint8_t *value);
+int  ibm4767_read32_indirect(ibm4767_ctx_t *, uint8_t offset, uint32_t *value);
+int  ibm4767_query_vpd(ibm4767_ctx_t *, xcVpd_t *vpd);
+void ibm4767_dump_tamper_regs(ibm4767_ctx_t *);
+
+void hex_dump(ibm4767_ctx_t *, uint32_t level, unsigned char *descr, 
+	      void *data, uint32_t len);
+
+void mark_device_offline(ibm4767_ctx_t *, int code);
+void mark_fp_offline(ibm4767_ctx_t *, int code);
+void mark_mcpu_offline(ibm4767_ctx_t *, int code);
+void mark_ssp_offline(ibm4767_ctx_t *, int code);
+
+int ibm4767_check_wur_pending(ibm4767_ctx_t *);
+
+void log_event(ibm4767_ctx_t *, asym_msg_t *msg_info, char *msg, int len);
+
+void ibm4767_request_throttle(ibm4767_ctx_t *);
+
+int ibm4767_opcode_type(uint32_t agent, uint32_t op);
+
+void ibm4767_do_temperature_shutdown(ibm4767_ctx_t *);
+
+int ibm4767_check_tamper(ibm4767_ctx_t *, char *func_str);
+
+void ibm4767_cdu_finalize(ibm4767_ctx_t *, int rc);
+
+void ibm4767_get_hwinfo(ibm4767_ctx_t *ctx);
+
+int  sev0_append(ibm4767_ctx_t *ctx, unsigned long timestamp);
+int  sev0_count(ibm4767_ctx_t *ctx);
+void sev0_purge(ibm4767_ctx_t *ctx);
+void sev0_dump(ibm4767_ctx_t *ctx, struct seq_file *sf);
+
+const char *cdu_state_to_str(CDUSTAT state);
+const char *mcpu_state_to_str(MCPUSTAT state);
+const char *ssp_state_to_str(SSPSTAT state);
+const char *tamper_state_to_str(TAMPERSTAT state);
+
+
+// reset routines
+void defer_main_reset(ibm4767_ctx_t *, uint32_t secs, int force);
+void cancel_deferred_main_reset(ibm4767_ctx_t *);
+int  ibm4767_main_reset(ibm4767_ctx_t *, int flush);
+void ibm4767_reset_via_hrcsr(ibm4767_ctx_t *);
+void ibm4767_reset_via_slot(ibm4767_ctx_t *);
+void ibm4767_reset_ssp(ibm4767_ctx_t *, int deferred_okay);
+int  ibm4767_check_ssp_reset_pending(ibm4767_ctx_t *);
+int  ibm4767_check_mbfail_expected(ibm4767_ctx_t *);
+void ibm4767_ssp_mbr(ibm4767_ctx_t *);
+void ibm4767_ssp_wur(ibm4767_ctx_t *);
+
+#endif
+
diff --git drivers/misc/ibm4767/y_hostRB.h drivers/misc/ibm4767/y_hostRB.h
new file mode 100755
index 000000000000..a41f57cf26d9
--- /dev/null
+++ drivers/misc/ibm4767/y_hostRB.h
@@ -0,0 +1,299 @@
+/*************************************************************************
+ *  Filename:y_hostRB.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  Host interfaces                                    
+ *                                         
+ *                                                                      
+ * Change History:           
+ *  JRM - if we're ultimately going to integrate this driver into the linux kernel
+ *       tree, we don't want to be pulling in headers from other directories.  the
+ *       driver needs to be self-contained...
+ *  RVK - I have added some intelligence so this can work with the 
+ *       rest of the tree.
+ *      
+ *  PDP - @d1 - d4601  Use #pragma options align=reset instead 
+ *                     of  #pragma() for AIX_ON_RS6000 for code to be      
+ *                     compatible with default xlc compiler version of   
+ *                     various AIX releases.                             
+ *                     AIX 53 xlc 600x, AIX 61 xlc 900x, AIX 71 xlc 1100x.
+ *  rvk - f6492        update xcVpd_t for Sentry
+**************************************************************************/
+// $Date$
+
+#ifndef _Y_HOSTRB_H_
+#define _Y_HOSTRB_H_
+
+#define CCAAGENT 0x4341
+
+
+// Macros HOSTADDR and HOSTADDR_RBFPTR declare a 128-bit field, containing
+// padding (if necessary), followed by an unsigned char * or RBFPTR address
+// in the appropriate length, either 32, 64, or 128 bits.  The first
+// parameter is the name used for the pointer variable.  The second
+// parameter is the name used for the padding variable that may (or may
+// not) precede it (128-bit addresses require no padding).
+//
+#if !defined(HOSTADDR) || !defined(HOSTADDR_RBFPTR)
+  #if defined(HOST32BIT)
+  #define HOSTADDR(VARNAME,PADNAME) unsigned char PADNAME[12]; unsigned char *VARNAME
+  #define HOSTADDR_RBFPTR(VARNAME,PADNAME) unsigned char PADNAME[12]; RBFPTR VARNAME
+  #elif defined(HOST64BIT)
+  #define HOSTADDR(VARNAME,PADNAME) unsigned char PADNAME[8]; unsigned char *VARNAME
+  #define HOSTADDR_RBFPTR(VARNAME,PADNAME) unsigned char PADNAME[8]; RBFPTR VARNAME
+  #elif defined(HOST128BIT)
+  #define HOSTADDR(VARNAME,PADNAME) unsigned char *VARNAME
+  #define HOSTADDR_RBFPTR(VARNAME,PADNAME) RBFPTR VARNAME
+  #else
+  #error +++ HOSTADDR/HOSTADDR_RBFPTR macro definitions cannot be defined.  +++
+  #error +++ Macro definition requires HOST32BIT, HOST64BIT, or HOST128BIT  +++
+  #error +++ to be defined (for example, -DHOST32BIT).                      +++
+  #endif
+#endif
+
+typedef uint16_t  xcAgentID_t;        
+typedef uint32_t  xcStatus_t;              
+typedef uint16_t  xcRequestID_t;
+
+// special request AgentIDs (note these bear no relation to the hot bytes
+// used for mailbox notifications)
+//
+#define XC_AGENTID_MB0             0x0000    // Miniboot 0
+#define XC_AGENTID_POST0           0x0001    // POST 0
+#define XC_AGENTID_POST1           0x0002    // POST 1
+#define XC_AGENTID_POST2           0x0003    // POST 2
+#define XC_AGENTID_IBM_TAS_SSP     0x0004    // IBM TestAppServer on SSP
+#define XC_AGENTID_IBM_TAS_MCPU    0x0005    // IBM TestAppServer on MCPU
+#define XC_AGENTID_IBM_TAPP_SSP    0x0006    // IBM TestApp on SSP
+#define XC_AGENTID_IBM_TAPP_MCPU   0x0007    // IBM TestApp on MCPU
+#define XC_AGENTID_MB1             0x0008    // Miniboot 1
+#define XC_AGENTID_SKCH_FASTPATH   0x0009
+#define XC_AGENTID_PKA_FASTPATH    0x000A
+#define XC_AGENTID_FPGAA_FASTPATH  0x000B
+#define XC_AGENTID_FPGAB_FASTPATH  0x000C
+// 0xA9-0xAF reserved for future use
+
+#pragma pack(1)                                                         
+typedef struct
+{   
+    xcAgentID_t   AgentID; 
+    uint32_t      UserDefined;
+    xcRequestID_t RequestID;
+    uint32_t      RequestControlBlkLength; 
+    HOSTADDR(     RequestControlBlkAddr,padding1); 
+    uint32_t      RequestDataLength; 
+    HOSTADDR(     RequestDataAddress,padding2); 
+    uint32_t      ReplyControlBlkLength;
+    HOSTADDR(     ReplyControlBlkAddr,padding3); 
+    uint32_t      ReplyDataLength;
+    HOSTADDR(     ReplyDataAddr,padding4); 
+    uint16_t      PriorityWindow; 
+    xcStatus_t    Status;
+} xcRB_t;
+
+// xcRB_t uses the HOSTADDR macro to automagically create addresses and padding
+// depending on the wordsize of the CPU we're targetting.  This yields a consistent
+// sizeof(xcRB_t) regardless of CPU which is important for embedded apps.
+//
+// To support 32-bit host apps on a 64-bit kernel (and, thus, 64-bit driver), we
+// need our IOCTL routines to be aware of how HOSTADDR behaves in a 32-bit world.
+// We could do this with pointer tricks but it's perhaps better to just define a 
+// new 32-bit version of the xcRB_t structure that explicitly mimics the 
+// padding scheme.
+// 
+// NOTE: xcRB32_t is a internal structure.  Do not use xcRB32_t outside
+// of the device driver
+//
+typedef struct { 
+    xcAgentID_t   AgentID; 
+    uint32_t      UserDefined; 
+    xcRequestID_t RequestID;
+        
+    uint32_t      RequestControlBlkLength; 
+    unsigned char padding1[12]; 
+    uint32_t      RequestControlBlkAddr;
+        
+    uint32_t      RequestDataLength; 
+    unsigned char padding2[12]; 
+    uint32_t      RequestDataAddress;
+        
+    uint32_t      ReplyControlBlkLength; 
+    unsigned char padding3[12]; 
+    uint32_t      ReplyControlBlkAddr;
+        
+    uint32_t      ReplyDataLength; 
+    unsigned char padding4[12]; 
+    uint32_t      ReplyDataAddr;
+        
+    uint16_t      PriorityWindow; 
+    xcStatus_t    Status;
+} xcRB32_t;
+
+/*****
+** Note: VPD data is in little Endian format as required by PCI/PCIX specification,
+**       references may be shown in big endian for readability.
+ **       New 12 byte Serial Number; we break down into 4 + 8 bytes; 
+ **       The field sn remains 8 bytes; sn_length changed from 8 to 12
+******/
+#ifndef XCVPD_TYPE
+#define XCVPD_TYPE 1 //this define protects us from mults
+typedef struct
+{ 
+  /*  0 */  char           ds_tag;      /* 0x82  */
+  /*  1 */  unsigned short ds_length;   /* 0x002C */
+  /*  3 */  char           ds[0x2C];    /* ASCII                            */
+                                        /* 'IBM 4767-001 PCI-e              */
+                                        /* Cryptographic Coprocessor'       */
+
+  /*0x2F*/  char           vpdr_tag;    /* 0x90                             */
+  /*0x30*/  unsigned short vpdr_length;  /* 0x00CD                          */
+
+  /*0x32*/  char           ec_tag[2];   /* "EC" tag                         */
+  /*0x34*/  unsigned char  ec_length;   /* 0x07                             */
+  /*0x35*/  char           ec[7];       /* ASCII                            */
+                                        /* EC Level - 'N44178B'             */
+
+  /*0x3C*/  char           pn_tag[2];   /* "PN" tag                         */
+  /*0x3E*/  unsigned char  pn_length;   /* 0x07                             */
+  /*0x3F*/  char           pn[7];       /* ASCII                            */
+                                        /* Part Number - 'XXXXXXX'          */
+
+  /*0x46*/  char           fn_tag[2];   /* "FN" tag                         */
+  /*0x48*/  unsigned char  fn_length;   /* 0x07                             */
+  /*0x49*/  char           fn[7];       /* ASCII                            */
+                                        /* FRU Number - 'XXXXXXXX'          */
+
+  /*0x50*/  char           ve_tag[2];   /* "VE" tag                         */
+  /*0x52*/  unsigned char  ve_length;   /* 0x07                             */
+  /*0x53*/  char           ve[7];       /* ASCII                            */
+                                        /* FRU Number - 'XXXXXXXX'          */
+
+  /*0x5A*/  char           mf_tag[2];   /* "MN" tag                         */
+  /*0x5C*/  unsigned char  mf_length;   /* 0x02                             */
+  /*0x5D*/  char           mf[2];       /* ASCII                            */
+                                        /* Manufacturing Loc - '91'         */
+
+  /*0x5F*/  char           sn_tag[2];   /* "SN" tag Card Unique             */
+  /*0x61*/  unsigned char  sn_length;   /* 0x0C                             */
+                                        /* ASCII                            */
+                                        /* Serial Number - YH1hhhssssss     */
+  /*0x62*/  char           sn_hdr[4];   /* YH1h                             */
+  /*0x62*/  char           sn[8];       /* hhssssss                         */
+                                        /* hh 16 = Plant code               */
+                                        /* s   D = Year - 2013              */
+                                        /* s   3 = Month - March            */
+                                        /* s   W = Day of month - 28th      */
+                                        /* sss 244 = Sequence number        */
+                                        /*  hh = "92" for 2002              */
+                                        /*  ssssss = Serial Number          */
+
+  /*0x6E*/  char           rv_tag[2];   /* "RV" tag                         */
+  /*0x70*/  unsigned char  rv_length;   /* 0x8C                             */
+  /*0x71*/  char           checkSum;    /* Checksum from 0x00 to 0x70       */
+
+
+
+  /*0x72*/  char           reserved[0x8D]; /* 0x00 Reserved read only area  */
+  /*0xFF*/  char           end_tag;        /* 0x78 VPD required end tag     */
+
+} xcVpd_t;
+#endif /* XCVPD_TYPE */
+
+typedef struct {
+#define EVENT_NONE                0x00
+#define EVENT_HW_ERR              0x01
+#define EVENT_ACCESS_ERR          0x02
+#define EVENT_UNRECOV_DMA_ERR     0x03
+#define EVENT_HIGH_TEMP_WARNING   0x04
+#define EVENT_HARD_TAMPER         0x05
+#define EVENT_SOFT_TAMPER_VOLTS   0x06
+#define EVENT_SOFT_TAMPER_INJ     0x07
+#define EVENT_SOFT_TAMPER_TEMP    0x08
+#define EVENT_SOFT_TAMPER_OTHER   0x09
+#define EVENT_LOW_BATTERY         0x0A
+#define EVENT_POST0_ERR           0x0B
+#define EVENT_POST1_ERR           0x0C
+#define EVENT_POST2_ERR           0x0D
+#define EVENT_MB0_ERR             0x0E
+#define EVENT_MB1_ERR             0x0F
+#define EVENT_MCPU_PANIC          0x10
+#define EVENT_TAS_SSP_ERR         0x11
+#define EVENT_TAS_MCPU_ERR        0x12
+#define EVENT_TAPP_SSP_ERR        0x13
+#define EVENT_TAPP_MCPU_ERR       0x14
+#define EVENT_HIGH_TEMP_OFFLINE   0x15
+
+        uint32_t  event;
+} xcCritEvent_t;
+
+
+typedef struct {
+	xcAgentID_t  AgentID;
+	uint64_t     mbx;
+} xcMbxRB_t;
+
+typedef struct {
+	uint64_t offset;
+	uint64_t value;
+} xcBarRB_t;
+
+
+// the following hardware info structure is, by definition,
+// rather hardware-specific.  it may be difficult to maintain
+// backward compatibility from hardware generation to hardware
+// generation (eg. 4765 to 4767, etc)
+//
+typedef struct {
+#define XC_HWINFO_VERSION_1  1
+	uint32_t version;
+
+	// the following strings are NULL-teriminated
+	char serial_num[13];
+	char part_num[8];
+	char ec_level[8];
+	char fru_num[8];
+	char mf_loc[3];
+
+	uint32_t post0_ver;
+	uint32_t post1_ver;
+	uint32_t post2_ver;
+	uint32_t mb0_ver;
+	uint32_t mb1_ver;
+
+	uint32_t fpga_rev;
+	uint32_t asic_rev;
+	uint32_t card_rev;
+
+	uint64_t reserved[8];  // 64 bytes
+} xcHWInfo_t;
+
+
+
+
+#ifdef AIX_ON_RS6000
+    #pragma options align=reset
+#else
+    #pragma pack()
+#endif
+#endif 
+
diff --git drivers/misc/ibm4767/y_ioctl.h drivers/misc/ibm4767/y_ioctl.h
new file mode 100755
index 000000000000..f7754f12d673
--- /dev/null
+++ drivers/misc/ibm4767/y_ioctl.h
@@ -0,0 +1,67 @@
+/*************************************************************************
+ *  Filename:y_ioctl.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  IO Control definitions                                     
+ *                                         
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#ifndef _YIOCTL_H_
+#define _YIOCTL_H_
+
+#if defined(CONFIG_COMPAT)
+        #define YC_USE_COMPAT_IOCTL
+#endif
+
+
+#define EIRQFAIL       145     // Error in IRQ request
+#define EINTERNAL      146     // There was an internal error
+#define ESENDFAIL      147     // Error sending request
+#define EQUIESCE       148     // We are quiescing the entire driver
+
+// The driver IOCTLs
+
+#ifndef LINUX_ON_390
+  #define XC_MAGIC_NR    'x' // Crypto "magic" number
+
+  #define IOCTL_IBM4767_OPEN           _IO  ( XC_MAGIC_NR,  0 )
+  #define IOCTL_IBM4767_REQUEST        _IOWR( XC_MAGIC_NR,  1, xcRB_t)
+  #define IOCTL_IBM4767_GETCOUNT       _IOR ( XC_MAGIC_NR,  3, uint32_t)
+  #define IOCTL_IBM4767_GETVPD         _IOR ( XC_MAGIC_NR,  4, xcVpd_t)
+  #define IOCTL_IBM4767_QUIESCE        _IO  ( XC_MAGIC_NR,  5 )
+  #define IOCTL_IBM4767_RESET          _IO  ( XC_MAGIC_NR,  6 )
+  #define IOCTL_IBM4767_GETHWINFO      _IOR ( XC_MAGIC_NR,  7, xcHWInfo_t)
+  #define IOCTL_IBM4767_GETHDWTMPR     _IOR ( XC_MAGIC_NR,  8, xcHdwTmpr_t)
+  #define IOCTL_IBM4767_SSP_MBX_HIST   _IOR ( XC_MAGIC_NR,  9, xcMbxHist_t)
+  #define IOCTL_IBM4767_MCPU_MBX_HIST  _IOR ( XC_MAGIC_NR, 10, xcMbxHist_t)
+  #define IOCTL_IBM4767_OBSERVER       _IOR ( XC_MAGIC_NR, 11, xcCritEvent_t) 
+  #define IOCTL_IBM4767_MBX_WRITE      _IOWR( XC_MAGIC_NR, 12, xcMbxRB_t)
+  #define IOCTL_IBM4767_BAR_READ       _IOWR( XC_MAGIC_NR, 13, xcBarRB_t)
+  #define IOCTL_IBM4767_BAR_WRITE      _IOWR( XC_MAGIC_NR, 14, xcBarRB_t)
+#endif
+
+#endif 
+
diff --git drivers/misc/ibm4767/y_mailbox.h drivers/misc/ibm4767/y_mailbox.h
new file mode 100755
index 000000000000..886aee5b0364
--- /dev/null
+++ drivers/misc/ibm4767/y_mailbox.h
@@ -0,0 +1,194 @@
+/*************************************************************************
+ *  Filename: y_mailbox.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  Host mailbox-handling related definitions and functions.
+**************************************************************************/
+// $Date$
+
+#ifndef _MAILBOX_H_
+#define _MAILBOX_H_
+
+
+// this routine contains enough definitions that we'll leave it in its
+// own file for now.  at some point it might make sense to consolidate.
+//
+
+// mailbox notifications are in a state of flux.  prior to sentry, they were a mismash
+// of disjoint values.  in the sentry universe, there are several new entities that
+// can generate mailbox notifications so we're going to try to reign things in so
+// that each entity will be assigned a designated value for byte #2 (the hot byte)
+//
+// note that this new scheme caused significant indigestion for the comm mgr 
+// designers since at least one notification requires all 8 mailbox bytes.  thus,
+// comm mgr doesn't obey this scheme.
+//
+// (it is hoped that any new comm mgr mailbox notifications will use an unused hotbyte)
+//
+#define MBX_HOTBYTE_POST0         0xA0
+#define MBX_HOTBYTE_MB0           0xA1
+#define MBX_HOTBYTE_POST1         0xA2
+#define MBX_HOTBYTE_POST2         0xA3
+#define MBX_HOTBYTE_MB1           0xA4
+
+#if defined(ENABLE_MFG)
+#define MBX_HOTBYTE_IBM_TAS_SSP   0xA5
+#define MBX_HOTBYTE_IBM_TAS_MCPU  0xA6
+#define MBX_HOTBYTE_IBM_TAPP_SSP  0xA7
+#define MBX_HOTBYTE_IBM_TAPP_MCPU 0xA8
+#endif
+
+#define MBX_HOTBYTE_POST0_MFG     0xA9
+#define MBX_HOTBYTE_POST1_MFG     0xAA
+#define MBX_HOTBYTE_POST2_MFG     0xAB
+
+#define MBX_HOTBYTE_SAME_AS_LAST  0xAF
+
+// mailbox bytes look something like:
+// 0:1  - notification id.  0x8000 and above are reserved
+//   2  - agent id
+// 3:7  - agent-defined 
+//
+// this allows 5 bytes (3:7) for agent-defined purposes.  in fact, if an agent
+// only uses byte 0 for notification id, byte 1 will also be available...
+
+// systemish notifications will use constants 0x8000 and above in bytes 0:1
+//
+// agent has started...should be sent very early in agent's lifetime.  agent version info 
+// (if any) appears in mbx bytes 4:7 
+#define NOTIFICATION_START    0x8000
+// agent is ready to communicate with the driver (ie. "good morning")
+#define NOTIFICATION_HELLO    0x8001
+// agent is about to end.  no further communication to this agent is possible;
+// this may be followed by a _FAIL notification from the same agent
+#define NOTIFICATION_END      0x8002
+// agent failed.  additional information regarding the precise failure
+// appears in bytes 4:7
+#define NOTIFICATION_FAIL     0x8003
+// generic acknowledgement (see CONTINUE_MB_ACK in the 4764 and 4765 universes)
+#define NOTIFICATION_ACK      0x8004
+#define NOTIFICATION_INFO     0x8005
+
+#define NOTIFICATION_CRIT_ERROR  0xCE00
+#define NOTIFICATION_DIAG_ERROR  0xDE00
+
+// POST-specific notifications (bytes 0:1)
+#define NOTIFICATION_SERIAL_DBG_START 0x8006
+#define NOTIFICATION_SERIAL_DBG_END   0x8007
+// POST-specific error IDs for _FAIL notifications (byte 3)
+#define POST_ERR_CRIT         0xCE
+#define POST_ERR_DIAG         0xDE
+// H2S MBX POST1 commands 
+#define POST1_JUMP_TO_MB1     0x0A000000
+
+// MB-specific notifications 
+// FIXME - need to work with MB team to determine halt return/reason codes
+#define NOTIFICATION_SSP_HALT 0x8005
+// H2S MBX MB0/MB1 commands 
+#define CONTINUE_MB_MODE    0x00AA0000
+#define CONTINUE_NORMAL     0x00000000
+
+
+// using the notification scheme, MB return codes will come in on bytes 6:7
+// and will look like:
+//    bit 0-6:  reason code  (7 bits...usually contains the command ID)
+//    bit 7-16: result code  (9 bits...contains the failure code)
+//    
+#define MB_ERROR_MASK  0x1FF
+
+
+
+// the following comm mgr notifications are maintained for backward compatibility.  
+//
+// M2H Comm Mgr mailbox notifications
+//
+#define MBX_GOODMORNINGHOST 		0x00000100
+#define MBX_REQBUFUPDATE 		0x00000200
+#define MBX_CDUFAILED 			0x00000400
+#define MBX_NON_CDUABLE_APP 		0x00000600
+#define MBX_CDU_PROCEED_ACK 		0x00000700
+#define MBX_CDU_READY_TO_PROCEED 	0x00000800
+#define MBX_SYMPROTOCOL_ACK 		0x00000900
+#define MBX_QUERYSTATUS_RETURN 		0x00001000
+#define MBX_POST_2_COMPLETE		0x00001100
+#define MBX_AGENTID_LIST 		0x00005500
+#define MBX_AGENTID_ATTACHED 		0x00005600
+#define MBX_CEASE_EVENT_LOG_ACK 	0x00005700
+#define MBX_REQRESET                    0x00008000
+#define MBX_KERNEL_PANIC 		0x0000F000
+#define MBX_SW_ERROR 			0x0000F100
+#define MBX_NONCRIT_SW_ERROR 	        0x0000F200
+#define MBX_EMERGENCY_DUMP     		0x0000F300
+#define MBX_INVALID_MB_CMD 		0x0000FE00
+
+// MBX_SOFTWARE_ERROR subcodes (contained in byte 1) 
+#define    TOO_MANY_IRQ3S 		0x03	// too many nested irq3s
+#define    TOO_MANY_IRQ5S 		0x05	// too many nested irq5s
+#define    NO_MRB_SPACE 		0x07	// IRQ5 when MRB full
+#define    LEN_NOT_MULT_OF_8 		0x09	// hra pool len invalid
+						// M2H_MBX[6..7]=hra pool len
+#define    HRA_LEN_TOO_SMALL		0x0B	// hra pool len too small
+						// M2H_MBX[6..7]=hra pool len
+#define    NO_FREE_QES			0x0D	// no free H2Cqes
+#define    ALIGNMENT_ERROR		0x0E	// HRB length field bad
+						// M2H_MBX[6..7]=requestID
+#define    NO_TAG_OCPRB			0x0F	// no tag=01 in HRB
+
+// MBX_NONCRIT_SW_ERROR subcodes (contained in byte 1)
+#define    FFDC_WITH_DUMP 		0x01    // First Failure Data Capture
+#define    FFDC_NO_DUMP 		0x02    // FFDC but with no dump expected
+#define    REPLY_DATA_TOO_SMALL 	0x20	// Reply Data buffer is too small
+#define    OUTPUT_CPRB_TOO_SMALL 	0x40	// Output CPRB buffer is too small
+#define    HRA_MASK_INVALID 		0x80    // HRA mask bits are invalid
+
+
+// H2M Comm Mgr mailbox commands
+//
+#define MBX_BUFUPDATEHIGH32             0x00004000
+#define MBX_BUFUPDATELOW32              0x00002000
+#define MBX_FORCE_CRIT_ERR 		0x00001500
+#define PROCEED_WITH_AGENTID            0x00001400
+#define MBX_FORCE_HW_ERR		0x00001300
+#define MBX_FORCE_EDUMP			0x00001200
+#define MBX_QUERYSTATUS                 0x00001000
+#define MBX_SYMPROTOCOL                 0x00000800
+#define PROCEED_TO_CDU                  0x00000400
+#define MBX_LIST_ALL_AGENTIDS		0x00000200
+
+// CDU mailbox notifications emitted by the embedded CDU daemon 
+// upper 16bits are used when constructing CDU_ERR_* return codes (see host_err.h)
+//
+#define CDU_NOT_CDUABLE         0x00800400
+#define CDU_NOT_ATTACHED        0x00810400
+#define CDU_SEG3_NONEXISTENT    0x00820400
+#define CDU_SEG3_OA_FAILURE     0x00830400
+#define CDU_SEG3_MOUNT_FAILURE  0x00840400
+#define CDU_SEG3_UMOUNT_FAILURE 0x00850400
+#define CDU_SEG3_LOCK_FAILURE   0x00860400
+#define CDU_AGENTID_RUNNING     0x00870400
+
+
+#define MCPU_MAILBOX_TEST       0x00009900
+
+#endif
+
diff --git drivers/misc/ibm4767/y_regs.h drivers/misc/ibm4767/y_regs.h
new file mode 100755
index 000000000000..221ce43dd0c5
--- /dev/null
+++ drivers/misc/ibm4767/y_regs.h
@@ -0,0 +1,582 @@
+/*************************************************************************
+ *  Filename:y_regs.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  Hardware register definitions.                                       
+ *                                         
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+#ifndef _YREGISTER_H_
+#define _YREGISTER_H_
+
+//
+// the values below are big-endian unless otherwise specified.  bit 0
+// is the most significant bit.
+//
+
+
+// Host-to-MCPU DMA registers...
+//
+#define  H2M_TCP(pD)            (pD->io_va + 0x0910) // (64) H2M Transfer Command Port
+#define  H2M_CH_STATUS(pD)      (pD->io_va + 0x0900) // (32) Debug: H2M channel status
+#define  H2M_BUFF_PTR(pD)       (pD->io_va + 0x0918) // (64) Debug: H2M next buffer address
+#define  H2M_DT_CTRL_REG(pD)    (pD->io_va + 0x0920) // (64) Debug: H2M DT control status
+#define  H2M_LAST_DT_PTR(pD)    (pD->io_va + 0x0928) // (64) Debug: Address of last fully-processed H2M DT
+#define  H2M_FIFO_DIN(pD)       (pD->io_va + 0x0930) // (64) H2M FIFO write port
+#define  H2M_FIFO_DIN_H(pD)     (H2M_FIFO_DIN(pD))
+#define  H2M_FIFO_DIN_L(pD)     (H2M_FIFO_DIN(pD) + 4)
+
+#define  M2H_CH_STATUS(pD)      (pD->io_va + 0x0D00) // (32) Debug: M2H channel status
+#define  M2H_LAST_HDR(pD)       (pD->io_va + 0x0D10) // (64) Debug: M2H last reply header
+#define  M2H_BUFF_PTR(pD)       (pD->io_va + 0x0D18) // (64) Debug: M2H next buffer address
+#define  M2H_DT_CTRL_REG(pD)    (pD->io_va + 0x0D20) // (64) Debug: M2H DT control status
+#define  M2H_DT_PTR(pD)         (pD->io_va + 0x0D28) // (64) Debug: Address of DT currently being processed
+#define  M2H_FIFO_DOUT(pD)      (pD->io_va + 0x0D30) // (64) M2H FIFO read port
+#define  M2H_FIFO_DOUT_H(pD)    (M2H_FIFO_DOUT(pD))
+#define  M2H_FIFO_DOUT_L(pD)    (M2H_FIFO_DOUT(pD) + 4)
+#define  M2H_FIFO_AUX_DOUT(pD)  (pD->io_va + 0x0D38) // (64) M2H FIFO aux read port
+
+
+// Host-to-SSP DMA registers...
+//
+#define  H2S_TCP(pD)            (pD->io_va + 0x09D0) // (64) H2S Transfer Command Port
+#define  H2S_CH_STATUS(pD)      (pD->io_va + 0x09C0) // (32) Debug: H2S channel status
+#define  H2S_BUFF_PTR(pD)       (pD->io_va + 0x09C8) // (64) Debug: H2S next buffer address
+#define  H2S_DT_CTRL_REG(pD)    (pD->io_va + 0x09E0) // (64) Debug: H2S DT control status
+#define  H2S_LAST_DT_PTR(pD)    (pD->io_va + 0x09E8) // (64) Debug: Address of last fully-processed H2S DT
+#define  H2S_FIFO_DIN(pD)       (pD->io_va + 0x09F0) // (64) H2S FIFO write port
+#define  H2S_FIFO_DIN_H(pD)     (H2S_FIFO_DIN(pD))
+#define  H2S_FIFO_DIN_L(pD)     (H2S_FIFO_DIN(pD) + 4)
+
+#define  S2H_CH_STATUS(pD)      (pD->io_va + 0x0DC0) // (32) Debug: S2H channel status
+#define  S2H_LAST_HDR(pD)       (pD->io_va + 0x0DD0) // (64) Debug: S2H last reply header
+#define  S2H_BUFF_PTR(pD)       (pD->io_va + 0x0DD8) // (64) Debug: S2H next buffer address
+#define  S2H_DT_CTRL_REG(pD)    (pD->io_va + 0x0DE0) // (64) Debug: S2H DT control status
+#define  S2H_DT_PTR(pD)         (pD->io_va + 0x0DE8) // (64) Debug: Address of DT currently being processed
+#define  S2H_FIFO_DOUT(pD)      (pD->io_va + 0x0DF0) // (64) S2H FIFO read port
+#define  S2H_FIFO_DOUT_H(pD)    (S2H_FIFO_DOUT(pD))
+#define  S2H_FIFO_DOUT_L(pD)    (S2H_FIFO_DOUT(pD) + 4)
+#define  S2H_FIFO_AUX_DOUT(pD)  (pD->io_va + 0x0DF8) // (64) S2H FIFO aux read port
+
+
+// Host-to-ModMath DMA registers...
+//
+#define  H2MM_TCP(pD)           (pD->io_va + 0x0810) // (64) H2MM Transfer Command Port
+#define  H2MM_CH_STATUS(pD)     (pD->io_va + 0x0800) // (32) Debug: H2MM channel status
+#define  H2MM_BUFF_PTR(pD)      (pD->io_va + 0x0818) // (64) Debug: H2MM next buffer address
+#define  H2MM_DT_CTRL_REG(pD)   (pD->io_va + 0x0820) // (64) Debug: H2MM DT control status
+#define  H2MM_LAST_DT_PTR(pD)   (pD->io_va + 0x0828) // (64) Debug: Address of last fully-processed H2MM DT
+#define  H2MM_FIFO_DIN(pD)      (pD->io_va + 0x0830) // (64) H2MM FIFO write port
+
+#define  MM2H_CH_STATUS(pD)     (pD->io_va + 0x0C00) // (32) Debug: MM2H channel status
+#define  MM2H_LAST_HDR(pD)      (pD->io_va + 0x0C10) // (64) Debug: MM2H last reply header
+#define  MM2H_BUFF_PTR(pD)      (pD->io_va + 0x0C18) // (64) Debug: MM2H next buffer address
+#define  MM2H_DT_CTRL_REG(pD)   (pD->io_va + 0x0C20) // (64) Debug: MM2H DT control status
+#define  MM2H_DT_PTR(pD)        (pD->io_va + 0x0C28) // (64) Debug: Address DT currently being processed
+#define  MM2H_FIFO_DOUT(pD)     (pD->io_va + 0x0C30) // (64) MM2H FIFO read port
+#define  MM2H_FIFO_AUX_DOUT(pD) (pD->io_va + 0x0C38) // (64) MM2H FIFO aux read port
+
+
+// Host-to-SKCH:A DMA registers...
+//
+#define  H2SK_TCP(pD)            (pD->io_va + 0x0890) // (64) ports A and B share a common TCP register
+
+#define  H2SKA_TCP(pD)           H2SK_TCP(pD)         // (64) ports A and B share a common TCP register
+#define  H2SKA_CH_STATUS(pD)     (pD->io_va + 0x0880) // (32) Debug: H2SKA channel status
+#define  H2SKA_BUFF_PTR(pD)      (pD->io_va + 0x0898) // (64) Debug: H2SKA next buffer address
+#define  H2SKA_DT_CTRL_REG(pD)   (pD->io_va + 0x08A0) // (64) Debug: H2SKA DT control status
+#define  H2SKA_LAST_DT_PTR(pD)   (pD->io_va + 0x08A8) // (64) Debug: Address of last fully-processed H2SKA DT
+#define  H2SKA_FIFO_DIN(pD)      (pD->io_va + 0x08B0) // (64) Host to SKCH:A FIFO
+
+#define  SKA2H_CH_STATUS(pD)     (pD->io_va + 0x0C80) // (32) Debug: SKA2H channel status
+#define  SKA2H_LAST_HDR(pD)      (pD->io_va + 0x0C90) // (64) Debug: SKA2H last reply header
+#define  SKA2H_BUFF_PTR(pD)      (pD->io_va + 0x0C98) // (64) Debug: SKA2H next buffer address
+#define  SKA2H_DT_CTRL_REG(pD)   (pD->io_va + 0x0CA0) // (64) Debug: SKA2H DT control status
+#define  SKA2H_DT_PTR(pD)        (pD->io_va + 0x0CA8) // (64) Debug: Address of DT currently being processed
+#define  SKA2H_FIFO_DOUT(pD)     (pD->io_va + 0x0CB0) // (64) SKA2H FIFO read port
+#define  SKA2H_FIFO_AUX_DOUT(pD) (pD->io_va + 0x0CB8) // (64) SKA2H FIFO aux read port
+
+
+// Host-to-SKCH:B DMA registers...
+//
+#define  H2SKB_TCP(pD)           H2SK_TCP(pD)         // (64) ports A and B share a common TCP register
+#define  H2SKB_CH_STATUS(pD)     (pD->io_va + 0x08C0) // (32) Debug: H2SKB channel status
+#define  H2SKB_BUFF_PTR(pD)      (pD->io_va + 0x08D8) // (64) Debug: H2SKB next buffer address
+#define  H2SKB_DT_CTRL_REG(pD)   (pD->io_va + 0x08E0) // (64) Debug: H2SKB DT control status
+#define  H2SKB_LAST_DT_PTR(pD)   (pD->io_va + 0x08E8) // (64) Debug: Address of last fully-processed H2SKB DT
+#define  H2SKB_FIFO_DIN(pD)      (pD->io_va + 0x08F0) // (64) Host to SKCH:B FIFO
+
+#define  SKB2H_CH_STATUS(pD)     (pD->io_va + 0x0CC0) // (32) Debug: SKB2H channel status
+#define  SKB2H_LAST_HDR(pD)      (pD->io_va + 0x0CD0) // (64) Debug: SKB2H last reply header
+#define  SKB2H_BUFF_PTR(pD)      (pD->io_va + 0x0CD8) // (64) Debug: SKB2H next buffer address
+#define  SKB2H_DT_CTRL_REG(pD)   (pD->io_va + 0x0CE0) // (64) Debug: SKB2H DT control status
+#define  SKB2H_DT_PTR(pD)        (pD->io_va + 0x0CE8) // (64) Debug: Address of DT currently being processed
+#define  SKB2H_FIFO_DOUT(pD)     (pD->io_va + 0x0CF0) // (64) SKB2H FIFO read port
+#define  SKB2H_FIFO_AUX_DOUT(pD) (pD->io_va + 0x0CF8) // (64) SKB2H FIFO aux read port
+
+
+// Host-to-FPGA:A DMA registers...
+//
+#define  H2FA_TCP(pD)           (pD->io_va + 0x0950) // (64) Host to FPGA:A Transfer Command Port
+#define  H2FA_CH_STATUS(pD)     (pD->io_va + 0x0940) // (32) Debug: H2FA channel status
+#define  H2FA_BUFF_PTR(pD)      (pD->io_va + 0x0958) // (64) Debug: H2FA next buffer address
+#define  H2FA_DT_CTRL_REG(pD)   (pD->io_va + 0x0960) // (64) Debug: H2FA DT control status
+#define  H2FA_LAST_DT_PTR(pD)   (pD->io_va + 0x0968) // (64) Debug: Address of last fully-processed H2FA DT
+#define  H2FA_FIFO_DIN(pD)      (pD->io_va + 0x0970) // (64) Host to FPGA:A FIFO
+
+#define  FA2H_CH_STATUS(pD)     (pD->io_va + 0x0D40) // (32) Debug: FA2H channel status
+#define  FA2H_LAST_HDR(pD)      (pD->io_va + 0x0D50) // (64) Debug: FA2H last reply header
+#define  FA2H_BUFF_PTR(pD)      (pD->io_va + 0x0D58) // (64) Debug: FA2H next buffer address
+#define  FA2H_DT_CTRL_REG(pD)   (pD->io_va + 0x0D60) // (64) Debug: FA2H DT control status
+#define  FA2H_DT_PTR(pD)        (pD->io_va + 0x0D68) // (64) Debug: Address DT currently being processed
+#define  FA2H_FIFO_DOUT(pD)     (pD->io_va + 0x0D70) // (64) FA2H FIFO read port
+#define  FA2H_FIFO_AUX_DOUT(pD) (pD->io_va + 0x0D78) // (64) FA2H FIFO aux read port
+
+
+// Host-to-FPGA:B DMA registers...
+//
+#define  H2FB_TCP(pD)           (pD->io_va + 0x0990) // (64) Host to FPGA:B Transfer Command Port
+#define  H2FB_CH_STATUS(pD)     (pD->io_va + 0x0980) // (32) Debug: channel status
+#define  H2FB_BUFF_PTR(pD)      (pD->io_va + 0x0998) // (64) Debug: next buffer address
+#define  H2FB_DT_CTRL_REG(pD)   (pD->io_va + 0x09A0) // (64) Debug: DT control status
+#define  H2FB_LAST_DT_PTR(pD)   (pD->io_va + 0x09A8) // (64) Debug: Address of last fully-processed DT
+#define  H2FB_FIFO_DIN(pD)      (pD->io_va + 0x09B0) // (64) Host to FPGA:B FIFO
+
+#define  FB2H_CH_STATUS(pD)     (pD->io_va + 0x0D80) // (32) Debug: FB2H channel status
+#define  FB2H_LAST_HDR(pD)      (pD->io_va + 0x0D90) // (64) Debug: FB2H last reply header
+#define  FB2H_BUFF_PTR(pD)      (pD->io_va + 0x0D98) // (64) Debug: FB2H next buffer address
+#define  FB2H_DT_CTRL_REG(pD)   (pD->io_va + 0x0DA0) // (64) Debug: FB2H DT control status
+#define  FB2H_DT_PTR(pD)        (pD->io_va + 0x0DA8) // (64) Debug: Address DT currently being processed
+#define  FB2H_FIFO_DOUT(pD)     (pD->io_va + 0x0DB0) // (64) FB2H FIFO read port
+#define  FB2H_FIFO_AUX_DOUT(pD) (pD->io_va + 0x0DB8) // (64) FB2H FIFO aux read port
+
+
+// other DMA registers
+//
+#define  HTB_WA(pD)             (pD->io_va + 0x0BC0) // (64) HTB Address Pointer
+#define  HTB_TC(pD)             (pD->io_va + 0x0BC8) // (64) HTB Transfer Count
+#define  HBMCR(pD)              (pD->io_va + 0x0B80) // (32) Host Bus Master Control
+#define  HBMSR(pD)              (pD->io_va + 0x0B88) // (64) Host Bus Master Status
+#define  HCR(pD)                (pD->io_va + 0x0B90) // (32) Host Control Register
+#define  H_CPU_DMAERR(pD)       (pD->io_va + 0x0BA0) // (32) Debug: Host DMA error register
+
+#define  VF_MAX_HRB_LEN(pD)     (pD->io_va + 0x0A00) // (32) VF max HRB length
+#define  VF_MAX_HRB_LEN_MASK    0x0FFFFF00L
+#define  VF_DMA_MASTER_EN(pD)   (pD->io_va + 0x0A08) // (64) VF DMA Master Enable
+
+
+// Interrupt Registers
+//
+#define  HISR(pD)         (pD->io_va + 0x00A0) // (32) Host Interrupt Status
+#define  HIER(pD)         (pD->io_va + 0x00A8) // (32) Host Interrupt Enable
+#define  HRCSR(pD)        (pD->io_va + 0x0608) // (32) Host Reset Control Status 
+#define  HCSR(pD)         (pD->io_va + 0x0610) // (64) Host Control Status
+
+#define  HMVMC(pD)        (pD->io_va + 0x00B0) // (32) Host MSI-X Vector Mapping
+
+#define  HTS_AP(pD)       (pD->io_va + 0x0210) // (32) Host Tamper Subsystem Address Port
+#define  HTS_AP_ADDR(pD)  (HTS_AP(pD) +  3)    // ( 8) 7-bit target address 
+#define  HTS_DP(pD)       (pD->io_va + 0x0218) // (32) Host Tamper Subsystem Data Port
+
+// Mailboxes...
+//
+#define  H2M_MBX(pD)      (pD->io_va + 0x0000) // (64) Host to MCPU Mailbox
+#define  H2M_MBX_H(pD)    (H2M_MBX(pD)       ) // (32) Host to MCPU Mailbox MSW
+#define  H2M_MBX_L(pD)    (H2M_MBX(pD) +    4) // (32) Host to MCPU Mailbox LSW
+
+#define  M2H_MBX(pD)      (pD->io_va + 0x0008) // (64) MCPU to Host Mailbox
+#define  M2H_MBX_H(pD)    (M2H_MBX(pD)       ) // (32) MCPU to Host Mailbox MSW
+#define  M2H_MBX_L(pD)    (M2H_MBX(pD) +    4) // (32) MCPU to Host Mailbox LSW
+
+#define  H2S_MBX(pD)      (pD->io_va + 0x0010) // (64) Host to SSP Mailbox
+#define  H2S_MBX_H(pD)    (H2S_MBX(pD)       ) // (32) Host to SSP Mailbox MSW
+#define  H2S_MBX_L(pD)    (H2S_MBX(pD) +    4) // (32) Host to SSP Mailbox LSW
+
+#define  S2H_MBX(pD)      (pD->io_va + 0x0018) // (64) SSP to Host Mailbox
+#define  S2H_MBX_H(pD)    (S2H_MBX(pD)       ) // (32) SSP to Host Mailbox MSW
+#define  S2H_MBX_L(pD)    (S2H_MBX(pD) +    4) // (32) SSP to Host Mailbox LSW
+
+#define  PF_VF_MBX(pD)    (pD->io_va + 0x0AF0) // (64) PF<-->VF mailbox
+
+
+// Misc registers
+//
+#define PCIE_TRAINING_CTRL(pD)  (pD->io_va + 0x9B40)  // nihad's request
+
+
+
+//--------------------------------------------------------------------
+// register bitmaps
+// only the bits that we might be interested in are listed here.  see 
+// function spec document(s) for complete register descriptions
+//--------------------------------------------------------------------
+
+
+// HBMCR bitmap
+//    first  16 bits are implemented only for the PF
+//    second 16 bits are implemented for PF and all VFs
+#define  HBMCR_MAX_BURST      0xC0000000L  // set PCIe max burst        (bits 0:1)
+#define  HBMCR_ARB_ONE_STEP   0x10000000L  // arbiter single step mode  (bit 3 )
+#define  HBMCR_ARB_COUNT      0x0F000000L  //                           (bits 4:7)
+#define  HBMCR_H2MM_SRIOV_ON  0x00800000L  // enable PKA VF arbiter     (bit 8 )
+#define  HBMCR_H2SK_SRIOV_ON  0x00400000L  // enable SKCH VF arbiter    (bit 9 )
+//                            0x0020000L   // reserved                  (bit 10)
+#define  HBMCR_H2M_SRIOV_ON   0x00100000L  // enable MCPU VF arbiter    (bit 11)
+#define  HBMCR_H2FA_SRIOV_ON  0x00080000L  // enable FPGA-A VF arbiter  (bit 12)
+#define  HBMCR_H2FB_SRIOV_ON  0x00040000L  // enable FPGA-B VF arbiter  (bit 13)
+//                            0x00020000L  // reserved                  (bit 14)
+#define  HBMCR_RD_CH_INT_EN   0x00010000L  // enable VF interrupts      (bit 15)
+//                            0x00008000L  // reserved                  (bit 16)
+//                            0x00004000L  // reserved                  (bit 17)
+//                            0x00002000L  // reserved                  (bit 18)
+#define  HBMCR_FA2H_BMEN      0x00001000L  // FPGA:A --> Host           (bit 19)
+#define  HBMCR_H2FA_BMEN      0x00000800L  // Host --> FPGA:A           (bit 20)
+#define  HBMCR_FB2H_BMEN      0x00000400L  // FPGA:B --> Host           (bit 21)
+#define  HBMCR_H2FB_BMEN      0x00000200L  // Host --> FPGA:B           (bit 22)
+#define  HBMCR_HTB_BMEN       0x00000100L  // HTB channel enable        (bit 23)
+#define  HBMCR_S2H_BMEN       0x00000080L  // SSP  --> Host             (bit 24)
+#define  HBMCR_H2S_BMEN       0x00000040L  // Host --> SSP              (bit 25)
+#define  HBMCR_M2H_BMEN       0x00000020L  // MCPU --> Host             (bit 26)
+#define  HBMCR_H2M_BMEN       0x00000010L  // Host --> MCPU             (bit 27)
+#define  HBMCR_SK2H_BMEN      0x00000008L  // SKCH --> Host             (bit 28)
+#define  HBMCR_H2SK_BMEN      0x00000004L  // Host --> SKCH             (bit 29)
+#define  HBMCR_MM2H_BMEN      0x00000002L  // Math --> Host             (bit 30)
+#define  HBMCR_H2MM_BMEN      0x00000001L  // Host --> Math             (bit 31)
+#define  HBMCR_BMEN_ALL       0x00001FFFL  // enable all channels 
+#define  HBMCR_BMEN_SSP   (HBMCR_S2H_BMEN  | HBMCR_H2S_BMEN)
+#define  HBMCR_BMEN_MCPU  (HBMCR_M2H_BMEN  | HBMCR_H2M_BMEN)
+#define  HBMCR_BMEN_SKCH  (HBMCR_SK2H_BMEN | HBMCR_H2SK_BMEN)
+#define  HBMCR_BMEN_MM    (HBMCR_MM2H_BMEN | HBMCR_H2MM_BMEN)
+#define  HBMCR_BMEN_FPGAA (HBMCR_FA2H_BMEN | HBMCR_H2FA_BMEN)
+#define  HBMCR_BMEN_FPGAB (HBMCR_FB2H_BMEN | HBMCR_H2FB_BMEN)
+#define  HBMCR_BMEN_FP    (HBMCR_BMEN_SKCH | HBMCR_BMEN_MM | HBMCR_BMEN_FPGAA | HBMCR_BMEN_FPGAB)
+
+
+// HBMSR bitmap
+//    first  32 bits are implemented for PF and all VFs
+//    second 32 bits are implemented only for the PF
+#define  HBMSR_H2MM_DE         0x8000000000000000LL // H->MM disabled              (bit  0)
+#define  HBMSR_MM2H_DE         0x4000000000000000LL // MM->H disabled              (bit  1)
+#define  HBMSR_H2SK_DE         0x2000000000000000LL // H->SKCH disabled            (bit  2)
+#define  HBMSR_SK2H_DE         0x1000000000000000LL // SKCH->H disabled            (bit  3)
+#define  HBMSR_H2M_DE          0x0800000000000000LL // H->MCPU disabled            (bit  4)
+#define  HBMSR_M2H_DE          0x0400000000000000LL // MCPU->H disabled            (bit  5)
+#define  HBMSR_H2S_DE          0x0200000000000000LL // H->SSP disabled             (bit  6)
+#define  HBMSR_S2H_DE          0x0100000000000000LL // SSP->H disabled             (bit  7)
+#define  HBMSR_H2FA_DE         0x0080000000000000LL // H->FPGA:A disabled          (bit  8)
+#define  HBMSR_FA2H_DE         0x0040000000000000LL // FPGA:A->H disabled          (bit  9)
+#define  HBMSR_H2FB_DE         0x0020000000000000LL // H->FPGA:B disabled          (bit  10)
+#define  HBMSR_FB2H_DE         0x0010000000000000LL // FPGA:B->H disabled          (bit  11)
+#define  HBMSR_H2MM_MASTER_EN  0x0008000000000000LL // H2MM enabled by MCPU?       (bit  12)
+#define  HBMSR_H2SK_MASTER_EN  0x0004000000000000LL // H2SK enabled by MCPU?       (bit  13)
+#define  HBMSR_H2FA_MASTER_EN  0x0002000000000000LL // H2FPGA:A enabled by MCPU?   (bit  14)
+#define  HBMSR_H2FB_MASTER_EN  0x0001000000000000LL // H2FPGA:B enabled by MCPU?   (bit  15)
+#define  HBMSR_H2MM_TCPR       0x0000800000000000LL // H->MM TCP ready             (bit  16)
+#define  HBMSR_H2SK_TCPR       0x0000400000000000LL // H->SKCH TCP ready           (bit  17)
+#define  HBMSR_H2M_TCPR        0x0000200000000000LL // H->MCPU TCP ready           (bit  18)
+#define  HBMSR_H2S_TCPR        0x0000100000000000LL // H->SSP TCP ready            (bit  19)
+#define  HBMSR_H2FA_TCPR       0x0000080000000000LL // H->FPGA:A TCP ready         (bit  20)
+#define  HBMSR_H2FB_TCPR       0x0000040000000000LL // H->FPGA:B TCP ready         (bit  21)
+//                             0x0000020000000000LL // reserved                    (bit  22)
+#define  HBMSR_HTB_FULL        0x0000010000000000LL // HTB full                    (bit  23) 
+#define  HBMSR_H2MM_BUSY       0x0000008000000000LL // H->MM channel busy          (bit  24)
+#define  HBMSR_H2SK_BUSY       0x0000004000000000LL // H->SKCH channel busy        (bit  25)
+#define  HBMSR_H2M_BUSY        0x0000002000000000LL // H->MCPU channel busy        (bit  26)
+#define  HBMSR_H2S_BUSY        0x0000001000000000LL // H->SSP channel busy         (bit  27)
+#define  HBMSR_H2FA_BUSY       0x0000000800000000LL // H->FPGA:A busy              (bit  28)
+#define  HBMSR_H2FB_BUSY       0x0000000400000000LL // H->FPGA:B busy              (bit  29)
+//                             0x0000000200000000LL // reserved                    (bit  30)
+//                             0x0000000100000000LL // reserved                    (bit  31)
+//                             0x0000000080000000LL // reserved                    (bit  32)
+#define  HBMSR_DMA_ARB_SM      0x0000000070000000LL // DCA state                   (bits 33-35)
+#define  HBMSR_DMA_ARB_WR      0x0000000008000000LL // DCA curr op is data write   (bit  36)
+#define  HBMSR_DMA_ARB_IV      0x0000000004000000LL // DCA curr op is IV write     (bit  37)
+#define  HBMSR_DMA_ARB_RD      0x0000000002000000LL // DCA curr op is data read    (bit  38)
+#define  HBMSR_DMA_ARB_DT      0x0000000001000000LL // DCA curr op is DT fetch     (bit  39)
+#define  HBMSR_DMA_LAST_BM     0x0000000000F00000LL // Last active DMA ch          (bits 40-43)
+#define  HBMSR_DMA_LAST_WR     0x0000000000080000LL // Last BM was data wr to host (bit  44)
+#define  HBMSR_DMA_LAST_IV     0x0000000000040000LL // Last BM was write to HTB    (bit  45)
+#define  HBMSR_DMA_LAST_RD     0x0000000000020000LL // Last BM was data fetch      (bit  46) 
+#define  HBMSR_DMA_LAST_DT     0x0000000000010000LL // Last BM was DT fetch        (bit  47) 
+//                             0x0000000000008000LL // reserved                    (bit  48)
+//                             0x0000000000004000LL // reserved                    (bit  49)
+#define  HBMSR_H2MM_FF         0x0000000000002000LL // H->MM FIFO full             (bit  50)
+#define  HBMSR_MM2H_FE         0x0000000000001000LL // MM->H FIFO empty            (bit  51)
+#define  HBMSR_H2SKA_FF        0x0000000000000800LL // H->SKCH:A FIFO full         (bit  52)
+#define  HBMSR_SKA2H_FE        0x0000000000000400LL // SKCH:A->H FIFO empty        (bit  53)
+#define  HBMSR_H2SKB_FF        0x0000000000000200LL // H->SKCH:B FIFO full         (bit  54)
+#define  HBMSR_SKB2H_FE        0x0000000000000100LL // SKCH:B->H FIFO empty        (bit  55)
+#define  HBMSR_H2M_FF          0x0000000000000080LL // H->MCPU FIFO full           (bit  56)
+#define  HBMSR_M2H_FE          0x0000000000000040LL // MCPU->H FIFO empty          (bit  57)
+#define  HBMSR_H2S_FF          0x0000000000000020LL // H->SSP FIFO full            (bit  58)
+#define  HBMSR_S2H_FE          0x0000000000000010LL // SSP->H FIFO empty           (bit  59)
+#define  HBMSR_H2FA_FF         0x0000000000000008LL // H->FPGA:A FIFO full         (bit  60)
+#define  HBMSR_FA2H_FE         0x0000000000000004LL // FPGA:A->H FIFO empty        (bit  61)
+#define  HBMSR_H2FB_FF         0x0000000000000002LL // H->FPGA:B FIFO full         (bit  62)
+#define  HBMSR_FB2H_FE         0x0000000000000001LL // FPGA:B->H FIFO empty        (bit  63)
+
+
+// HCR bitmap
+//                         0xFF000000L  // reserved              (bits 0:7)
+#define  HCR_RESUME_SRM    0x00800000L  // resume next SRM xfer  (bit  8)
+#define  HCR_RESUME_MRM    0x00400000L  // resume next MRM xfer  (bit  9)
+//                         0x00200000L  // reserved              (bit 10)
+//                         0x00100000L  // reserved              (bit 11)
+//                         0x000FF000L  // reserved              (bits 12:19)
+#define  HCR_REFETCH_H2FA  0x00000800L  // refetch FPGA:A        (bit 20)
+#define  HCR_FETCH_H2FA    0x00000400L  // fetch FPGA:A          (bit 21)
+#define  HCR_REFETCH_H2FB  0x00000200L  // refetch FPGA:B        (bit 22)
+#define  HCR_FETCH_H2FB    0x00000100L  // fetch FPGA:B          (bit 23)
+#define  HCR_REFETCH_H2S   0x00000080L  // refetch SSP           (bit 24)
+#define  HCR_FETCH_H2S     0x00000040L  // fetch   SSP           (bit 25)
+#define  HCR_REFETCH_H2M   0x00000020L  // refetch H2M           (bit 26)
+#define  HCR_FETCH_H2M     0x00000010L  // fetch   H2M           (bit 27)
+#define  HCR_REFETCH_SK    0x00000008L  // refetch SKCH          (bit 28)
+#define  HCR_FETCH_SK      0x00000004L  // fetch   SKCH          (bit 29)
+#define  HCR_REFETCH_MM    0x00000002L  // refetch MM            (bit 30)
+#define  HCR_FETCH_MM      0x00000001L  // fetch   MM            (bit 31)
+
+
+// H_CPU_DMAERR bitmap
+// see AIB shim layer secion in function spec for channel IDs
+#define  DMAERR_RD_CH_ERR_ID_MASK  0xF0000000    // Read Ch ID                  (bits 0:3)
+//                                 0x0F000000    // reserved                    (bits 4:7)
+#define  DMAERR_RD_CH_DT_SIG_ERR   0x00800000    // DT sig or reserved field    (bit  8)
+#define  DMAERR_RD_CH_DT_HRB_ERR   0x00400000    // HRB len 0 or not mult of 8B (bit  9)
+#define  DMAERR_RD_CH_DT_MAX_ERR   0x00200000    // HRB len exceeds max         (bit 10)
+#define  DMAERR_RD_CH_DT_BC_ERR    0x00100000    // Bad DT byte count           (bit 11)
+#define  DMAERR_RD_CH_DT_ALIGN_ERR 0x00080000    // TCP or next DT not aligned  (bit 12)
+#define  DMAERR_RD_CH_DT_512_ERR   0x00040000    // DT spans 512B boundary      (bit 13)
+#define  DMAERR_RD_CH_DT_EOC_ERR   0x00020000    // Saw EOC too soon            (bit 14)
+#define  DMAERR_RD_CH_HRB_SIG_ERR  0x00010000    // Signature error             (bit 15)
+#define  DMAERR_WR_CH_ERR_ID_MASK  0x0000F000    // Write Ch ID                 (bits 16:19)
+//                                 0x00000F00    // reserved                    (bits 20:23)
+#define  DMAERR_WR_CH_DT_SIG_ERR   0x00000080    // DT sig or reserved field    (bit 24)
+//                                 0x00000040    // reserved                    (bit 25)
+//                                 0x00000020    // reserved                    (bit 26)
+#define  DMAERR_WR_CH_DT_BC_ERR    0x00000010    // Bad DT byte count           (bit 27)
+#define  DMAERR_WR_CH_DT_ALIGN_ERR 0x00000008    // TCP or next DT not aligned  (bit 28)
+#define  DMAERR_WR_CH_DT_512_ERR   0x00000004    // DT spans 512B boundary      (bit 29)
+#define  DMAERR_WR_CH_DT_EOC_ERR   0x00000002    // Saw EOC too soon            (bit 30)
+#define  DMAERR_WR_CH_HRB_SIG_ERR  0x00000001    // Signature error             (bit 31)
+
+
+// VF_DMA_MASTER_EN bitmap
+#define  VF_DMA_MASTER_H2MM_MASK  0xFFFF000000000000LL   // H->MM     VF DMA enables (bits  0-15)
+#define  VF_DMA_MASTER_H2SK_MASK  0x0000FFFF00000000LL   // H->SKCH   VF DMA enables (bits 16-31)
+#define  VF_DMA_MASTER_H2FA_MASK  0x00000000FFFF0000LL   // H->FPGA:A VF DMA enables (bits 32-47)
+#define  VF_DMA_MASTER_H2FB_MASK  0x000000000000FFFFLL   // H->FPGA:B VF DMA enables (bits 48-63)
+
+
+// HISR bitmap
+#define  HISR_HW_ERR      0x80000000L  // MSI 0 hardware error             (bit  0)
+//                        0x40000000L  // reserved                         (bit  1)
+//                        0x20000000L  // reserved                         (bit  2)
+#define  HISR_ACCESS_ERR  0x10000000L  // MSI 0 access error               (bit  3)
+#define  HISR_RECOV_DMA   0x08000000L  // MSI 0 recoverable DMA error      (bit  4)
+#define  HISR_UNRECOV_DMA 0x04000000L  // MSI 0 unrecoverable DMA error    (bit  5)
+#define  HISR_SOFT_TAMPER 0x02000000L  // MSI 0 unrecoverable DMA error    (bit  6)
+#define  HISR_HARD_TAMPER 0x01000000L  // MSI 0 unrecoverable DMA error    (bit  7)
+#define  HISR_H2M_I       0x00800000L  // MSI 1 mailbox read by MCPU       (bit  8)
+#define  HISR_M2H_I       0x00400000L  // MSI 1 mailbox write by MCPU      (bit  9)
+#define  HISR_H2S_I       0x00200000L  // MSI 1 mailbox read by SSP        (bit  10)
+#define  HISR_S2H_I       0x00100000L  // MSI 1 mailbox write by SSP       (bit  11)
+//                        0x000FFF00L  // reserved                         (bits 12:23)
+#define  HISR_TEMPERATURE 0x00000080L  // MSI 1 high temp warning          (bit  24)
+//                        0x00000040L  // reserved                         (bit  25)
+#define  HISR_BATTERY     0x00000020L  // MSI 1 low battery                (bit  26)
+#define  HISR_PFVF_MBX    0x00000010L  // MSI 1 PF-VF mailbox full         (bit  27)
+#define  HISR_SRM_I       0x00000008L  // MSI 1 SRM attention              (bit  28)
+#define  HISR_MRM_I       0x00000004L  // MSI 1 MRM attention              (bit  29)
+#define  HISR_HTB_BF      0x00000002L  // MSI 1 HTB full                   (bit  30)
+#define  HISR_HTB         0x00000001L  // MSI 1 transfer complete          (bit  31)
+
+// technically we can enable HISR_HTB_BF but that means we'll see two bits
+// in HISR (or two MSI-X ints if multiple ints are enabled) when a BF condition
+// exists.  instead, we can just keep track of HTB_WA to detect BF...
+//
+#define HISR_RESERVED_BITS  (~(HISR_HW_ERR | \
+			       HISR_ACCESS_ERR  | \
+			       HISR_RECOV_DMA   | \
+			       HISR_UNRECOV_DMA | \
+			       HISR_SOFT_TAMPER | \
+			       HISR_HARD_TAMPER | \
+			       HISR_H2M_I       | \
+			       HISR_M2H_I       | \
+			       HISR_H2S_I       | \
+			       HISR_S2H_I       | \
+			       HISR_TEMPERATURE | \
+			       HISR_BATTERY     | \
+			       HISR_PFVF_MBX    | \
+			       HISR_SRM_I       | \
+			       HISR_MRM_I       | \
+			       HISR_HTB))
+
+#define HISR_INT_BITS ((HISR_HW_ERR      | \
+			HISR_ACCESS_ERR  | \
+			HISR_RECOV_DMA   | \
+			HISR_UNRECOV_DMA | \
+			HISR_SOFT_TAMPER | \
+			HISR_HARD_TAMPER | \
+			HISR_H2M_I       | \
+			HISR_M2H_I       | \
+			HISR_H2S_I       | \
+			HISR_S2H_I       | \
+			HISR_TEMPERATURE | \
+			HISR_BATTERY     | \
+			HISR_PFVF_MBX    | \
+			HISR_SRM_I       | \
+			HISR_MRM_I       | \
+			HISR_HTB))
+
+#define  link_lost32(reg)               ((reg) == ~(0L))
+#define  link_lost64(reg)               ((reg) == ~(0LL))
+
+#define  hardware_error(hisr)          (hisr & HISR_HW_ERR)
+#define  access_error(hisr)            (hisr & HISR_ACCESS_ERR)
+#define  recoverable_dma_error(hisr)   (hisr & HISR_RECOV_DMA)
+#define  unrecoverable_dma_error(hisr) (hisr & HISR_UNRECOV_DMA)
+#define  is_tamper(hisr)               (hisr & (HISR_SOFT_TAMPER | HISR_HARD_TAMPER))
+#define  is_soft_tamper(hisr)          (hisr & HISR_SOFT_TAMPER)
+#define  is_hard_tamper(hisr)          (hisr & HISR_HARD_TAMPER)
+#define  h2m_empty(hisr)               (hisr & HISR_H2M_I)
+#define  m2h_full(hisr)                (hisr & HISR_M2H_I)
+#define  h2s_empty(hisr)               (hisr & HISR_H2S_I)
+#define  s2h_full(hisr)                (hisr & HISR_S2H_I)
+#define  high_temp_warning(hisr)       (hisr & HISR_TEMPERATURE)
+#define  is_low_batt(hisr)             (hisr & HISR_BATTERY)
+#define  mrm_attn(hisr)                (hisr & HISR_MRM_I)
+#define  srm_attn(hisr)                (hisr & HISR_SRM_I)
+#define  xfer_buffer_full(hisr)        (hisr & HISR_HTB_BF)
+#define  xfer_complete(hisr)           (hisr & HISR_HTB)
+
+
+// HIER bitmap: see HISR
+#define HIER_INITIALIZE      (HISR_INT_BITS)
+#define HIER_TEMPERATURE     (HISR_TEMPERATURE)
+
+
+// HMVMC bits:  see HMVMC in functino spec (p 127, version 1.32)
+//
+#define  HMVMC_1_VEC	  0x80000000L  //             (bit  0)
+#define  HMVMC_2_VEC      0x40000000L  //             (bit  1)
+//                        0x20000000L  // reserved    (bit  2)
+#define  HMVMC_4_VEC      0x10000000L  //             (bit  3)
+//                        0x08000000L  // reserved    (bit  4)
+//                        0x04000000L  // reserved    (bit  5)
+//                        0x02000000L  // reserved    (bit  6)
+#define  HMVMC_8_VEC      0x01000000L  //             (bit  7)
+//                        0x00800000L  // reserved    (bit  8)
+//                        0x00400000L  // reserved    (bit  9)
+//                        0x00200000L  // reserved    (bit 10)
+//                        0x00100000L  // reserved    (bit 11)
+//                        0x00080000L  // reserved    (bit 12)
+//                        0x00040000L  // reserved    (bit 13)
+//                        0x00020000L  // reserved    (bit 14)
+#define  HMVMC_16_VEC     0x00010000L  //             (bit 15)
+//                        0x0000FFFFL  // reserved    (bits 16:31)
+
+
+// HTS_AP bitmap
+#define  HTS_DONE       0x80000000L    //             (bit 0)
+#define  HTS_ERROR      0x40000000L    //             (bit 1)
+//                      0x20000000L    // reserved    (bit 2)
+//                      0x10000000L    // reserved    (bit 3)
+//                      0x0FFFFF00L    // reserved    (bits 4:24)
+#define  HTS_ADDR_MASK  0x000000FFL    //             (bits 25:31)
+
+
+// HRCSR bitmap
+#define  HRCSR_RRAT         0x80000000L  // Rst reqd Tmpr          (bit  0)
+#define  HRCSR_RRAST        0x40000000L  // Rst reqd Soft Tmpr     (bit  1)
+#define  HRCSR_1STRAT       0x20000000L  // 1st RAT                (bit  2)
+#define  HRCSR_1STRAST      0x10000000L  // 1st RAST               (bit  3)
+#define  HRCSR_PCT          0x08000000L  // Permanent Card Tamper  (bit  4)
+#define  HRCSR_SOFT_VOLT    0x04000000L  // Soft Tamper Volts      (bit  5)
+#define  HRCSR_SOFT_TEMP    0x02000000L  // Soft Tamper Temp       (bit  6)
+#define  HRCSR_SOFT_INJ     0x01000000L  // Soft Tamper Inj        (bit  7)
+#define  HRCSR_TRST         0x00800000L  // Tamper Reset           (bit  8)
+#define  HRCSR_STRST        0x00400000L  // Soft Tamper Reset      (bit  9)
+#define  HRCSR_MWRMBOOT     0x00200000L  // MCPU Warm Boot indic   (bit 10)
+#define  HRCSR_SWRMBOOT     0x00100000L  // SSP Warm Boot indic    (bit 11)
+#define  HRCSR_POST2_CMPL   0x00080000L  // POST 2 Complete        (bit 12)
+#define  HRCSR_SEG2_START   0x00040000L  // Seg2 Started           (bit 13)
+#define  HRCSR_SEG3_START   0x00020000L  // Seg3 Started           (bit 14)
+#define  HRCSR_SEG3_CMPL    0x00010000L  // Seg3 Completed         (bit 15)
+#define  HRCSR_HOST_RESET   0x00008000L  // Main Reset             (bit 16)
+#define  HRCSR_S2M_RstS     0x00004000L  // S2M reset status       (bit 17)
+#define  HRCSR_MCPU_RstS    0x00002000L  // MCPU Reset Status      (bit 18)
+//                          0x00001000L  // reserved               (bit 19)
+//                          0x00000800L  // reserved               (bit 20)
+#define  HRCSR_MCPU_WUE     0x00000400L  // MCPU wake-up event     (bit 21)
+#define  HRCSR_ERR_WUE      0x00000200L  // Error wake-up event    (bit 22)
+#define  HRCSR_TMPR_WUE     0x00000100L  // Tamper wake-up event   (bit 23)
+#define  HRCSR_H2S_WUE      0x000000F0L  // H2S wake-up event      (bits 24:27)
+#define  HRCSR_SSP_RstS     0x00000008L  // SSP Reset Status       (bit 28)
+#define  HRCSR_HPRst        0x00000004L  // High-Pri reset         (bit 29)
+#define  HRCSR_H2S_WURqstS  0x00000002L  // Host W-U Req Status    (bit 30)
+#define  HRCSR_H2S_WURqst   0x00000001L  // Host SSP W-U Req       (bit 31)
+
+
+// HCSR bitmap
+// unlike HISR, this register can be polled without side effects 
+//                        0xF000000000000000LL    // reserved                          (bits 0:3)
+#define  HCSR_H2S_EMPTY   0x0800000000000000LL    // H2S mailbox empty                 (bit  4)
+#define  HCSR_S2H_FULL    0x0400000000000000LL    // S2H mailbox full                  (bit  5)
+#define  HCSR_H2M_EMPTY   0x0200000000000000LL    // H2M mailbox empty                 (bit  6)
+#define  HCSR_M2H_FULL    0x0100000000000000LL    // M2H mailbox full                  (bit  7)
+//                        0x0080000000000000LL    // reserved                          (bit  8)
+#define  HCSR_BATTERY     0x0040000000000000LL    // Low battery status                (bit  9)
+//                        0x0020000000000000LL    // reserved                          (bit 10)
+//                        0x0010000000000000LL    // reserved                          (bit 11)
+//                        0x000F000000000000LL    // reserved                          (bits 12:15)
+#define  HCSR_CURR_TEMP   0x0000FF0000000000LL    // Temp (C) reported by PM unit      (bits 16:23)
+#define  HCSR_TAMPER_TEMP 0x000000FF00000000LL    // Temp (C) that caused soft tamper  (bits 24:31)
+#define  HCSR_FPGA_REV    0x00000000FFFF0000LL    // FPGA revision ID                  (bits 32:47)
+#define  HCSR_ASIC_REV    0x000000000000FF00LL    // ASIC revision ID                  (bits 48-55)
+#define  HCSR_CARD_REV    0x00000000000000FFLL    // Card revision ID                  (bits 56-63)
+
+
+// see MAXIM DS3645 docs for the register addresses
+// note that the chip is basically the same as the one used for 4765 but
+// the I2C glue around it has changed...so these values are different
+//
+#define INDIRECT_DS3645_TMPR_TIMESTAMP_A   0x04
+#define INDIRECT_DS3645_TMPR_TIMESTAMP_B   0x05
+#define INDIRECT_DS3645_TMPR_TIMESTAMP_C   0x06
+#define INDIRECT_DS3645_TMPR_TIMESTAMP_D   0x07
+#define INDIRECT_DS3645_TMPR_CTRL          0x14
+#define INDIRECT_DS3645_TMPR_LATCH_A       0x15
+#define INDIRECT_DS3645_TMPR_LATCH_B       0x16
+#define INDIRECT_DS3645_TMPR_LATCH_C       0x17
+#define INDIRECT_DS3645_ADC_VCCI           0x28
+#define INDIRECT_DS3645_ADC_VBAT           0x29
+#define INDIRECT_DS3645_ADC_VREF           0x2A
+#define INDIRECT_DS3645_ADC_TEMP           0x2B
+#define INDIRECT_DS3645_LAST_VISIBLE_REG   0x37
+
+
+#endif 
+
diff --git drivers/misc/ibm4767/y_slowops.h drivers/misc/ibm4767/y_slowops.h
new file mode 100755
index 000000000000..0b3072fc9b99
--- /dev/null
+++ drivers/misc/ibm4767/y_slowops.h
@@ -0,0 +1,125 @@
+/*************************************************************************
+ *  Filename:y_slowops.h
+ *
+ *  IBM 4767 xSeries Host device driver
+ *
+ *  Authors: Mayfield, Jimmie  IBM Lexington <jimmiem@us.ibm.com>
+ *           Andruzzi,Joe      IBM Charlotte <jandruzz@us.ibm.com>
+ *
+ * (C) Copyright 2009,2020 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * Function:  Delay time-out for these operations                                          
+ *                                         
+ *                                                                      
+ * Change History:                                                      
+**************************************************************************/
+// $Date$
+
+// some operations can take so long that they'll be mistaken for timeouts.
+// the most flexible solution would be to allow the app to specify the timeout
+// in the xcRB_t structure since the app has a better idea about a request does
+// than the driver (which should treat requests opaquely).  but that requires all
+// developers to think about things they might not want to think about.
+//
+// so we compromise:  for CCA, we have a list of command IDs that are known to
+// take a long time to execute.  if we see one of those, we assign it to the
+// "slow" MRB and set its timeout accordingly.  for non-CCA apps, the target MRB
+// is determined by xcRB_t::PriorityWindow value.
+//
+
+#ifndef _HOST_DRIVER_FUNC_CODES_
+#define _HOST_DRIVER_FUNC_CODES_
+
+#define  SSAACI_ID          0x4149     /* 'AI', Access Controm Init.          */
+#define  SSAACI_ID_S        "AI"
+
+#define  SSAACM_ID          0x414D     /* 'AM', Access Controm Maint          */
+#define  SSAACM_ID_S        "AM"
+
+#define  SSDPIC_ID          0x4343     /* 'CC', Public Infrastructure Cert    */
+#define  SSDPIC_ID_S        "CC"
+
+#define  SSEDH_ID           0x4448     /* 'DH', EC_Diffie-Hellman             */
+#define  SSEDH_ID_S         "DH"
+
+#define  SSCDE_ID           0x4458     /* 'DX', Crypto Data Extract           */
+#define  SSCDE_ID_S         "DX"
+
+#define  SSCFC_ID           0x4643     /* 'FC', Crypto Facility Control       */
+#define  SSCFC_ID_S         "FC"
+
+#define  SSGLD_ID           0x474C     /* 'GL', Generate HMAC low priority    */
+#define  SSGLD_ID_S         "GL"
+
+#define  SSDHKP_ID          0x4850     /* 'HP', Diffie-Hellman Key Load       */
+#define  SSDHKP_ID_S        "HP"
+
+#define  SSAMKD_ID          0x4B44     /* 'KD', Master Key Distribution       */
+#define  SSAMKD_ID_S        "KD"
+
+#define  SSALCT_ID          0x4C47     /* 'LG', Access Logon Control          */
+#define  SSALCT_ID_S        "LG"
+
+#define  SSSLGQ_ID          0x4C51     /* 'LQ', Log Query                     */
+#define  SSSLGQ_ID_S        "LQ"
+
+#define  SSDHMK_ID          0x4D48     /* 'HM', Diffie-Hellman MK Process     */
+#define  SSDHMK_ID_S        "HM"
+
+#define  SSMKP_ID           0x4D50     /* 'MP', Master key Process            */
+#define  SSMKP_ID_S         "MP"
+
+#define  SSPKG_ID           0x5047     /* 'PG', PKA Key Generate              */
+#define  SSPKG_ID_S         "PG"
+
+#define  SSPKE_ID           0x504B     /* 'PK', PKA Clr Key Export            */
+#define  SSPKE_ID_S         "PK"
+
+#define  SSPKD_ID           0x5044     /* 'PD', PKA Clr Key Import            */
+#define  SSPKD_ID_S         "PD"
+
+#define  SSPIM_ID           0x504D     /* 'PM', Public Infrastructure Manage  */
+#define  SSPIM_ID_S         "PM"
+
+#define  SST34B_ID          0x5242     /* 'RB', TR-34 Bind-Begin              */
+#define  SST34B_ID_S        "RB"
+
+#define  SST34C_ID          0x5243     /* 'RC', TR-34 Bind-Complete           */
+#define  SST34C_ID_S        "RC"
+
+#define  SST34D_ID          0x5244     /* 'RD', TR-34 Key Distribute          */
+#define  SST34D_ID_S        "RD"
+
+#define  SST34R_ID          0x5252     /* 'RR', TR-34 Key Receive             */
+#define  SST34R_ID_S        "RR"
+
+#define  SSDSV_ID           0x5356     /* 'SV', Digital Signature Verify      */
+#define  SSDSV_ID_S         "SV"
+
+#define  SSDSG_ID           0x5347     /* 'SG', Digital signature generate    */
+#define  SSDSG_ID_S         "SG"
+
+#define  SSSYX_ID           0x5358     /* 'SX', Symmetric key export          */
+#define  SSSYX_ID_S         "SX"
+
+#define  SSCTI_ID           0x5449     /* 'TI', Crypto Target Inject          */
+#define  SSCTI_ID_S         "TI"
+
+#define  SSVLD_ID           0x564C     /* 'VL', Verify HMAC low priority      */
+#define  SSVLD_ID_S         "VL"
+
+#endif
